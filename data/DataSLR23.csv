id;year;submissiontype;venue;venue_acc;journalorconf;country;region;title;abstract;url;link;include;name;name_txt;type;type_txt;auton;auton_txt;CA_child;CA_child_txt;tts_child;tts_child_txt;asr_child;asr_child_txt;dof;dof_recode;dof_num;dof_txt;domain;domain_txt;role;role_hier;role_txt;setting;setting_recoded;setting_txt;CA_purpose;CA_purpose_txt;age;age_group;age_range;age_bn;age_txt;typical;typical_txt;number;numbergroup;number_txt;research_purpose;research_purpose_txt;indep;indep_txt;dep;dep_txt;conclusion;significance;sig_txt;trust;trust_txt;language;language_txt;language_English;comments;comm2
5;2020;article;Computers in Human Behavior;hci;journal;Switzerland;Europe;Communicative and social consequences of interactions with voice assistants;The growing prevalence of artificial intelligence and digital media in children's lives provides them with the opportunity to interact with novel non-human agents such as robots and voice assistants. Previous studies show that children eagerly adopt and interact with these technologies, but we have only limited evidence of children's distinction between artificial intelligence and humans. In this study, the communication patterns and prosocial outcomes of interactions with voice assistants were investigated. Children between 5 and 6 years (N = 72) of age solved a treasure hunt in either a human or voice assistant condition. During the treasure hunt, the interaction partner supplied information either about their knowledge of or experience with the objects. Afterwards, children were administered a sharing task and a helping task. Results revealed that children provided voice assistants with less information than humans and that only the type of information given by a human interaction partner was related to children's information selection. Sharing was influenced by an interaction between type of information and interaction partner, showing that the type of information shared influenced children's decisions more when interacting with a human, but less when interacting with a voice assistant. Children in all conditions enjoyed the treasure hunt with the interaction partner. Overall, these results suggest that children do not impose the same expectations on voice assistants as they do on humans. Consequently, cooperation between humans and cooperation between humans and computers differ. © 2020 The Authors;https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087199244&doi=10.1016%2fj.chb.2020.106466&partnerID=40&md5=190d9734f35f77cf3f2b5634a5a6ed45;5.Auschlimann.pdf;Yes;Sila;introduced E2, called “Sila”,;Voice Assistant;Sila was talking as an audible voice via loudspeakers.;Wizarded;While human experimenter 1 (E1) interacted with the child in the study room during the familiarisation for the treasure hunt, human experimenter 2 (E2) was only present via her voice in all conditions.;ns;;Yes;If children were too shy to ask, E1 asked the questions;Wizarded;;ns;ns;ns;;Other;;companion;equal;During the treasure hunt, the interaction partner supplied information either about their knowledge of or experience with the objects;lab;lab;"While human
experimenter 1 (E1) interacted with the child in the study room during
the familiarisation for the treasure hunt, human experimenter 2 (E2)
was only present via her voice in all conditions.";Help with task, share information, share experiences; Afterwards, children were administered a sharing task and a helping task.;5, 6;young;2;narrow;Children between 5 and 6 years;TD;"This sample was extracted by creating similar background environments for the children in the four conditions based on important covariates: age of the children at test time (we included children between 5 and 6 years of age, which is very broad), parental education (parental education is associated with children’s cognition), media use of the child and the parents (the behaviour of the children in the treasure hunt task depends on the experiences of the children with digital media in general and smart home gadgets in particular), the social dimension of the Strengths and Difficulties Questionnaire (Goodman, 1997; Klasen et al., 2003) and a baseline need-of-help recognition task (Brielmann & Stolarova, 2014) (these two & latter scores were included to have a baseline of social behaviour in dependent of the manipulation during the treasure hunt, see how they were assessed below).";72;71-80;N=72;Effect with CA on xyz (could have been any CA);Results revealed that children provided voice assistants with less information than humans and that only the type of information given by a human interaction partner was related to children’s information selection.;Voice assistant vs. human; in either a human or voice assistant condition.;Prosocial behavior; communication patterns and prosocial outcomes of interactions;;;;No;;ns;;ns;;
8;2015;in proceedings;HRI;robot/agent;conference;USA;North America;Design and Architecture of a Robot-Child Speech-Controlled Game;We describe the conceptual design, architecture, and implementation of a multimodal, robot-child dialogue system in a fast-paced, speech-controlled collaborative game. In Mole Madness, two players (a user and an anthropomorphic robot) work together to move an animated mole character through its environment via speech commands. Using a combination of speech recognition systems and a microphone array, the system can accommodate children's natural behavior in real time. We also briefly present the details of a recent data collection with children, ages 5 to 9, and some of the challenging behaviors the system elicited that we intend to explore.;https://doi.org/10.1145/2701973.2702041;8.AlMoubayed.pdf;Yes;Sammy J;A back-projected anthropomorphic robot head (“Sammy J,”;Robot;anthropomorphic robot head;Fully autonomous;"autonomous,
robotic play companion";Yes;found in a child-child pilot study;No;tutorial style chitchat;Yes;found in a child-child pilot study;limited commands;limited;2;"To accomplish this we currently use
simple voice activity detection in conjunction with the mean
duration of “go” and “jump” found in a child-child pilot study
[Lehman and Al Moubayed, 2015]. When the child’s utterance is
the approximate length of an average single “go” or “jump” then
Task ASR hypothesizes a command, otherwise the decision is left
for the large vocabulary speech recognizer, explained below.";Edutainment;"a two-player multimodal interactive game, Mole Madness
provides a platform for studying collaborative language behavior";companion;equal;A robotic play companion;ns;ns;;Help with task;"task language processing occur
in real-time";5, 6, 7, 8, 9;broad;5;broad;ages 5 to 9.;ns;;40;31-40;played with 40 children;Effect with CA on xyz (could have been any CA);"when children play Mole
Madness with each other, they co-adopt a style of interaction that
both keeps the game moving and feeds their excitement and
enthusiasm";Game events;game events;Style of gameplay, style of interaction;preferred style of gameplay;;;;No;;ns;;ns;Small part is user study, not very wel l described;
14;2014;in proceedings;IDC;CCI;conference;New Zealand;Oceania;A Study of Auti: A Socially Assistive Robotic Toy;This paper presents an evaluation of the effectiveness of a new sociallyassistive robot, Auti, in encouraging physical and verbal interactions in children with autism. It aims to encourage positive play behaviors such as gentle speaking and touching, with positive reinforcement through movement responses, and to discourage challenging behaviors, such as screaming or hitting through the removal of the reinforcing movements. This study evaluates the design by comparing a fullyinteractive Auti to an activeonly version, which does the same movements but does not respond to the child. Results from 18 participants indicate that the Interactive Auti does encourage positive behaviors more than the Activeonly version. However, further design is needed around addressing problematic behaviors.;https://doi.org/10.1145/2593968.2610463;14.Andreae.pdf;Yes;Auti;"a new
socially-assistive robot, Auti,";Robot;"a new
socially-assistive robot, Auti,";Wizarded;"During the experiments, the sound detection was
done manually through the remote, since reliable automatic
detection of someone talking quietly close-up versus someone
shouting from a distance is extremely difficult.";Yes;Auti was designed from the start for children with autis;Not relevant;;Wizarded;;ns;ns;ns;;Healthcare;"encouraging physical and verbal
interactions in children with autism.";toy;lower;before the toys were introduced;home;home;"The trials took place in the child’s home to ensure
that the children were in an environment where they felt
comfortable";Induce playful exploring;"robotic toy
designed to help encourage positive play behaviours and discourage
problematic behaviours (";4, 5, 6, 7, 8;broad;5;broad;"18 children with an autism diagnosis
between the ages of 4 years 6 months and 8 years 2 months.";Autistic;"All participants had a formal autism
diagnosis from a paediatrician and this diagnosis was confirmed
with the Gilliam Autism Rating Scale Second Edition (GARS-2).";18;Nov/20;"18 children with an autism diagnosis
between the ages of 4 years 6 months and 8 years 2 months.";Effect of CA on xyz;"evaluates the
design by comparing a fully-interactive Auti to an active-only
version";Interactivity;fully-interactive Auti to an active-only version;Child's behavior;positive behaviors [and] problematic behaviors;;;;No;;ns;;ns;Pre interview;
15;2013;in proceedings;IDC;CCI;conference;USA;North America;Fun and Fair: Influencing Turn-Taking in a Multi-Party Game with a Virtual Agent;Language-based interfaces for children hold great promise in education, therapy, and entertainment. An important subset of these interfaces includes those with a virtual agent that mediates the interaction. When participants are groups of children, the agent will need to exert a certain amount of turn-taking control to ensure that all group members participate and benefit from the experience, but must do so without being so overtly directive as to undermine the children's enjoyment of and engagement in the task. We present a hierarchy of nonverbal and verbal behaviors that a virtual agent can employ flexibly when passing the conversational turn. When used effectively, these behaviors can equalize participation, and potentially decrease the amount of overlapping speech among participants, improving automatic speech recognition in turn. We evaluated the behaviors by having children play a language-based game twice, once with a flexible host and once with an inflexible host that did not have access to the behaviors. Post-game opinion cards revealed no difference between the conditions with respect to fun or likability of the host, despite the flexible agent eliciting more evenly distributed play.;https://doi.org/10.1145/2485760.2485800;15.Andrist.pdf;Yes;Edith;"The host
of the game, Edith, is an animated character responsible for
making the changes and mediating the interaction.";Virtual Agent;"We present a hierarchy of nonverbal and verbal behaviors
that a virtual agent";Wizarded;Edith is controlled in a Wizard-of-Oz manner.;Yes;"We have contextualized this work in an interactive languagebased
game called Robo Fashion World. In this game, groups of
up to four children change the appearance of a model by calling
out the names of items on the game board";ns;;Wizarded;;ns;ns;ns;;Entertainment;"We have contextualized this work in an interactive languagebased
game called Robo Fashion World. In this game, groups of
up to four children change the appearance of a model by calling
out the names of items on the game board";host;higher;"The host
of the game, Edith, is an animated character responsible for
making the changes and mediating the interaction.";lab;lab;"Following informed consent, the children were brought into the
study room and briefly introduced to the two agents they would be
interacting with, Edith and Charlotte";Perform actions of children in the game;"children can either ask Edith to dress up the model with one of the
items currently displayed on the board, or ask her to take a picture
of the model as it is";4, 5, 6, 7, 8, 9, 10;broad;7;wide;"Ages ranged from
4 to 10 (M = 7 years, SD = 2 years)";ns;ns;33;31-40;Thirty-three children (17 females and 16 males);Effect with CA on xyz (could have been any CA);"hierarchy of nonverbal and verbal behaviors
that a virtual agent can employ flexibly when passing the
conversational turn.";(In)Flexible behavior;"once with a flexible
host and once with an inflexible host";Fun. Likability, Distributed play;"fun or likability of the host,
despite the flexible agent eliciting more evenly distributed play";;;;No;;ns;;ns;;
22;2003;conference;Eurospeech;speech;conference;Sweden;Europe;Child and adult speaker adaptation during error resolution in a publicly available spoken dialogue system;This paper describes how speakers adapt their language during error resolution when interacting with the animated agent Pixie. A corpus of spontaneous human-computer interaction was collected at the Telecommunication museum in Stockholm, Sweden. Adult and children speakers were compared with respect to user behavior and strategies during error resolution. In this study, 16 adults and 16 children speakers were randomly selected from a corpus from almost 3.000 speakers. This sub-corpus was then analyzed in greater detail. Results indicate that adults and children use partly different strategies when their interactions with Pixie become problematic. Children tend to repeat the same utterance verbatim, altering certain phonetic features. Adults, on the other hand, often modify other aspects of their utterances such as lexicon and syntax. Results from the present study will be useful for constructing future spoken dialogue systems with improved error handling for adults as well as children.;https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009188327&partnerID=40&md5=c8084855898639b762633496fda29a28;22.Bell.pdf;Yes;Pixie;the animated agent Pixie;Virtual Agent;the animated agent Pixie;Fully autonomous;"With all twelve screens
potentially being used at the same time, and people
simultaneously speaking to each other, the acoustic environment
is very challenging. The visitors are asked to either help Pixie
perform certain tasks in the apartment or encouraged to ask the
agent general questions about herself or the exhibition.";No;"A
corpus of spontaneous human-computer interaction was collected
at the Telecommunication museum in Stockholm, Sweden. Adult
and children speakers were compared";No;"A
corpus of spontaneous human-computer interaction was collected
at the Telecommunication museum in Stockholm, Sweden. Adult
and children speakers were compared";No;"A
corpus of spontaneous human-computer interaction was collected
at the Telecommunication museum in Stockholm, Sweden. Adult
and children speakers were compared";free speech;free speech;;"about 35.000
utterances of spontaneous computer-directed Swedish have been
collected.";Entertainment;"The Pixie system is placed in the permanent exhibition ‘Tänk Om’
(‘What If’), where visitors can experience a full-size apartment of
the year 2010. The animated agent Pixie (see Figure 1) with whom
the users interact in spoken Swedish is supposed to visualize an
embodied speech interface to both information services and home
control in this apartment.";Service;lower;"The Pixie system is placed in the permanent exhibition ‘Tänk Om’
(‘What If’), where visitors can experience a full-size apartment of
the year 2010. The animated agent Pixie (see Figure 1) with whom
the users interact in spoken Swedish is supposed to visualize an
embodied speech interface to both information services and home
control in this apartment.";museum;public space;"A
corpus of spontaneous human-computer interaction was collected
at the Telecommunication museum in Stockholm, Sweden. Adult
and children speakers were compared";Information service and home control;"The Pixie system is placed in the permanent exhibition ‘Tänk Om’
(‘What If’), where visitors can experience a full-size apartment of
the year 2010. The animated agent Pixie (see Figure 1) with whom
the users interact in spoken Swedish is supposed to visualize an
embodied speech interface to both information services and home
control in this apartment.";9, 10, 11, 12;broad;4;broad;"The children in
this subsection of the corpus were between the ages of 9 and 12";ns;;16;Nov/20;16 children speakers;Effect with CA on xyz (could have been any CA);"Adult
and children speakers were compared with respect to user
behavior and strategies during error resolution.";Error resolution;"Adult
and children speakers were compared with respect to user
behavior and strategies during error resolution.";error handling strategies;"Adult
and children speakers were compared with respect to user
behavior and strategies during error resolution.";;;;No;;Swedish;"about 35.000
utterances of spontaneous computer-directed Swedish have been
collected.";Other;;
30;2017;article;Autonomous robots;robot/agent;journal;USA;North America;A low-cost socially assistive robot and robot-assisted intervention for children with autism spectrum disorder: field trials and lessons learned;Recent research has employed socially assistive robots as catalysts for social interaction and improved communication in young children with autism spectrum disorder (ASD). Studies describe observed therapeutic outcomes such as increased speech, social interaction, joint and directed attention, but few detail a robot-inclusive protocol which evaluates a set of robot tasks using widely-accepted, clinical assessments to evaluate the efficacy of the approach. In this study, we employed a low-cost, toy-like robot prototype with safety features such as a snap-off head and two snap-off arms, a camera for face, hand detection and session recording, two autonomous games and a teleoperated mode. We then developed and tested a new, robot-assisted intervention. Eight study participants and three controls diagnosed with ASD and a speech deficiency were recruited. The study group received pre-, post-intervention measures with the Vineland Adaptive Behavioral Scale II (VABS-II), mean length spontaneous utterance determination (MLSUD), motor imitation scale (MIS), unstructured imitation assessment (UIA) and Expressive Vocabulary Test 2 (EVT-2) and participated in twelve 30 min interventions. To explore the efficacy of the robot and new robot-assisted intervention we (1) measured improvements in spontaneous speech, communication and social skills using standard measures of performance, (2) compared improvements observed with a study group receiving the robot-assisted intervention with a control group receiving speech therapy but no robot-assisted intervention and, (3) validated a set of robot behaviors that may inform an integrated, cross-platform, approach for incorporating an autonomous, robot-assisted ASD intervention within a clinical methodology. Paired-samples t test results indicate significantly improved adaptive functioning in the VABS-II socialization and communication domains, MLSUD, UIA Social Interaction, UIA Requesting, and UIA Joint Attention domains. Between-group analyses also suggest significant improvement in VABS-II Play and Leisure, Receptive Language subdomains and trends in VABS-II Coping Skills and Interpersonal Scale subdomains. © 2016, Springer Science+Business Media New York.;https://www.scopus.com/inward/record.uri?eid=2-s2.0-84959135696&doi=10.1007%2fs10514-016-9554-4&partnerID=40&md5=cdac19a97af209945a1a07c86d9d51fd;30.Boccanfuso.pdf;Yes;Charlie;CHARLIE the robot.;Robot;CHARLIE the robot.;Wizarded;"Before developing and implementing a newset of autonomous
robot tasks, particularly due to the exploratory nature of the
novel interaction design, we opted to first use a teleoperated
mode to control the robot’s actions";Yes;"Second, given the critical role of free play during a child’s
early social development, we sought to maximize opportunities
for children to freely explore and manipulate the robot
without excessive concern for the physical integrity of the
robot. To this end, we integrated snap-off arms and head on
the robot and included a base that could be secured to a table.
Finally, the outward appearance of the robot was designed
to be toy-like to invite the attention of young children with
ASD and to avoid being intimidating to the greatest extent
possible.";Yes;"Once it has detected the pose, the robot indicates that it has
learned the pose by saying “O.k., I got it. Now let me try”,
turns to the second player (seated to the left of the robot), asks
the child to imitate and then assumes the same pose learned
from the first player. If the second player successfully imitates
the pose, the robot responds by saying “You got it!”, claps its
hands and giggles. If the player does not immediately imitate
the correct pose, the robot will ask the child to try again. If
the child does not correctly assume the pose after three tries,
the robot asks the current player to initiate a new pose and
the game continues, this time with the second player initially
“passing” the pose to the robot";Wizarded;;ns;ns;ns;;Healthcare;"therapeutic outcomes such
as increased speech, social interaction, joint and directed
attention, but few detail a robot-inclusive protocol which
evaluates a set of robot tasks using widely-accepted, clinical
assessments to evaluate the efficacy of the approach";companion;equal;"child-led communication
and socialization using a set of new, interactive
games with a robot prototype";therapy;therapy;"Children in the control group received speech therapy for the
entire 6-week participation in the study but did not receive
the robot-assisted intervention whereas most of the children
in the study group received the robot-enabled intervention in
addition to speech therapy";Improve social capabilities;"catalysts for social interaction and improved communication
in young children with autism spectrum disorder
(ASD).";3, 4, 5, 6;broad;4;broad;A group of eight children, between 3 and 6 years of age,;Autistic, speech deficiency;"Eight study participants and three controls diagnosed with
ASDand a speech deficiencywere recruited";8;01/Oct;"Eight study participants and three controls diagnosed with
ASDand a speech deficiencywere recruited";Effect of CA on xyz;"“Effectiveness of
CHARLIE the Robot for Improving Verbal and Nonverbal
Skills in Children with Autism";robot intervention or not;"compared improvements observed with a study group
receiving the robot-assisted interventionwith a control group
receiving speech therapy but no robot-assisted intervention";speech and socialization;"To explore the efficacy of
the robot and new robot-assisted intervention we (1) measured
improvements in spontaneous speech, communication
This is one of several papers published in Autonomous Robots
comprising the “Special Issue on Assistive and Rehabilitation
Robotics”.
Electronic supplementary material The online version of this
article (doi:10.1007/s10514-016-9554-4) contains supplementary
material, which is available to authorized users.
B Laura Boccanfuso
laura.boccanfuso@yale.edu
1 University of South Carolina, Columbia, USA
2 Yale University, New Haven, USA
and social skills using standard measures of performance,";;;;No;;ns;;ns;;
35;2005;in proceedings;CHI;hci;conference;France;Europe;Children's and Adults' Multimodal Interaction with 2D Conversational Agents;"Few systems combine both Embodied Conversational Agents (ECAs) and multimodal input. This research aims at modeling the behavior of adults and children during their multimodal interaction with ECAs. A Wizard-of-Oz setup was used and users were video-recorded while interacting with 2D ECAs in a game scenario with speech and pen as input modes. We found that frequent social cues and natural Human-Human syntax condition the verbal interaction of both groups with ECAs. Multimodality accounted for 21% of inputs: it was used for integrating conversational and social aspects (by speech) into task-oriented actions (by pen). We closely examined temporal and semantic integration of modalities: most of the time, speech and gesture overlapped and produced complementary or redundant messages; children also tended to produce concurrent multimodal inputs, as a way of doing several things at the same time. Design implications of our results for multimodal bidirectional ECAs and game systems are discussed.";https://doi.org/10.1145/1056808.1056886;35.Buisine.pdf;Yes;ns;;Virtual Agent;"interacting
with 2D ECAs in a game scenario";Wizarded;A Wizard-of-Oz setup;No;"Our system is being designed for a wide
range of users, i.e. children from about 9 years old to young
adults.";No;"Our system is being designed for a wide
range of users, i.e. children from about 9 years old to young
adults.";No;"Our system is being designed for a wide
range of users, i.e. children from about 9 years old to young
adults.";free speech;free speech;;"We did not give users any indications or instructions
about how to interact with the Agents.";Entertainment;"game scenario with speech and pen as
input modes.";game character;unclear;"Agents and ask them their
wish: it basically consisted in bringing them an object
missing in their room (e.g. bring a lamp or a book to the
Agent in the library). Then the user had to visit the rooms to
find the right objects, take them and bring them back to the
Agents.";ns;ns;;Agent needs something from user;"Agents and ask them their
wish: it basically consisted in bringing them an object
missing in their room (e.g. bring a lamp or a book to the
Agent in the library). Then the user had to visit the rooms to
find the right objects, take them and bring them back to the
Agents.";9, 10, 11, 12, 13, 14, 15;broad;7;wide;10 children (7 male and 3 female users, age range 9 – 15).;ns;;10;01/Oct;10 children (7 male and 3 female users, age range 9 – 15).;Effect with CA on xyz (could have been any CA);"modeling the behavior of adults and children during their
multimodal interaction with ECAs";multimodal behavior;"collect behavioral data for
guiding the implementation of a functional multimodal game
with ECAs";user behavior;"collect behavioral data for
guiding the implementation of a functional multimodal game
with ECAs";;;;No;;French;French-speaking users;Other;;
39;2020;in proceedings;CUI;CUI;conference;Italy;Europe;"What is the Best Action for Children to ""Wake Up"" and ""Put to Sleep"" a Conversational Agent? A Multi-Criteria Decision Analysis Approach";"Popular conversational assistants like Amazon Alexa, Google Assistant, and Apple Siri are activated by pronouncing their wake-up word (""Alexa"", ""OK Google"", and ""Hey Siri"", respectively), and they stop listening when they recognize a pause. We explore whether this paradigm is appropriate for children and whether there are alternative, more effective ways for children to ""wake-up"" and ""put to sleep"" a conversational system. We consider seven possibilities: wake word, free-form speech, physical button, digital button, mouse, gaze, and buzzer. We propose a methodological framework of analysis based on the Multi-Attribute Value Theory (MAVT). As part of the decision-making process, we ran a study involving 42 children aged 9 to 10 who experienced the seven approaches. Our results suggest that the physical button is the most appropriate solution for this target group, which opens up new directions in the design of interaction affordances of conversational agents for children.";https://doi.org/10.1145/3405755.3406129;39.Catania.pdf;Yes;ns;;Virtual Agent;"The computer welcomed the subject, it
put her/him at ease, and presented itself as a game";Wizarded;"The Wizard: A trained person, who spoke with the children
following a rigid script and simulating being a computer
recognizing the wake and sleep actions by the users";Yes;"Decision Maker
(DM: a children-expert User Experience - UX - designer) taking the
partial and the final decisions.";Yes;"Decision Maker
(DM: a children-expert User Experience - UX - designer) taking the
partial and the final decisions.";No;;limited commands;limited;7;"The participant
was provided with instructions about the wake action to use; then,
she/he was asked to repeat two simple sentences interacting with
the system by using the wake/sleep action to prove she/he understood
how to use it. We did not let the child decide what to say,
because we wanted to make participants’ experiences comparable";Other;general domain, not really specified in text;Not clear;unclear;general domain, not really specified in text;school;school;"The study was conducted during a summer campus in a primary
school.";not clear;general domain, not really specified in text;9, 10, 11;10/Dec;3;narrow;42 Italian speaking children aged 9 to 10;ns;;42;41-50;42 Italian speaking children aged 9 to 10;Effect with CA on xyz (could have been any CA);"We explore whether
this paradigm is appropriate for children and whether there are
alternative, more effective ways for children to “wake-up” and “put
to sleep” a conversational system";wake up strategy;"We consider seven possibilities:
wakeword, free-form speech, physical button, digital button, mouse,
gaze, and buzzer.";attributes;"attributes are meant as measurable quantitative or
qualitative characteristics to quantify the level of achievement of
the objectives.";;;;No;;Italian;42 Italian speaking children aged 9 to 10;Other;;
41;2016;conference;Interspeech;speech;conference;USA;North America;An acoustic analysis of child-child and child-robot interactions for understanding engagement during speech-controlled computer games;Engagement is an essential factor towards successful game design and effective human-computer interaction. We analyze the prosodic patterns of child-child and child-robot pairs playing a language-based computer game. Acoustic features include speech loudness and fundamental frequency. We use a linear mixed-effects model to capture the coordination of acoustic patterns between interactors as well as its relation to annotated engagement levels. Our results indicate that the considered acoustic features are related to engagement levels for both the childchild and child-robot interaction. They further suggest significant association of the prosodic patterns during the child-child scenario, which is moderated by the co-occurring engagement. This acoustic coordination is not present in the child-robot interaction, since the robot's behavior was not automatically adjusted to the child. These findings are discussed in relation to automatic robot adaptation and provide a foundation for promoting engagement and enhancing rapport during the considered game-based interactions. Copyright © 2016 ISCA.;https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994222247&doi=10.21437%2fInterspeech.2016-85&partnerID=40&md5=4fa5844fda4f3ed7a0123f8cccb044b4;41.Chaspari.pdf;Yes;Sammy J;"one-on-one with Sammy, a back-projected robot head developed
by Furhat Robotics";Robot;"one-on-one with Sammy, a back-projected robot head developed
by Furhat Robotics";ns;;ns;;ns;;ns;;limited commands;limited;2;"The keywords (“go” and “jump”) were manually segmented
from the audio file. Loudness and fundamental frequency (F0),
were computed by openSMILE [23] over each keyword";Entertainment;"“Mole Madness” (MM) is a language-based, speech-controlled
interactive game played by two children, or a child and a
robot [20]. It is a computer-based game built to explore
language-use, turn-taking and engagement during a fast-paced,
speech-based task.";companion;equal;"“Mole Madness” (MM) is a language-based, speech-controlled
interactive game played by two children, or a child and a
robot [20]. It is a computer-based game built to explore
language-use, turn-taking and engagement during a fast-paced,
speech-based task.";ns;ns;;Help with task;"“Mole Madness” (MM) is a language-based, speech-controlled
interactive game played by two children, or a child and a
robot [20]. It is a computer-based game built to explore
language-use, turn-taking and engagement during a fast-paced,
speech-based task.";5, 6, 7, 8, 9, 10;broad;6;wide;Their ages ranged between 5-10 years old;ns;;62;61-70;Our data contain 62 children;Effect with CA on xyz (could have been any CA);"We explore the association between engagement and the
behavioral coordination of two children as well as a child and
a robot during a game-based interaction.";acoustic patterns;"In this paper we modeled the interaction between acoustic
patterns of children while playing a language-based computer
game and its relation to the annotated engagement levels";engagement;"In this paper we modeled the interaction between acoustic
patterns of children while playing a language-based computer
game and its relation to the annotated engagement levels";;;;No;;ns;;ns;;
42;2012;conference;ICASSP;speech;conference;USA;North America;An acoustic analysis of shared enjoyment in ECA interactions of children with autism;"The quality of shared enjoyment in interactions is a key aspect related to Autism Spectrum Disorders (ASD). This paper discusses two types of enjoyment: the first refers to humorous events and is associated with one's positive affective state and the second is used to facilitate social interactions between people. These types of shared enjoyment are objectively specified by their proximity to a voiced and unvoiced laughter instance, respectively. The goal of this work is to study the acoustic differences of areas surrounding the two kinds of shared enjoyment instances, called ""social zones"", using data collected from children with autism, and their parents, interacting with an Embodied Conversational Agent (ECA). A classification task was performed to predict whether a ""social zone"" surrounds a voiced or an unvoiced laughter instance. Our results indicate that humorous events are more easily recognized than events acting as social facilitators and that related speech patterns vary more across children compared to other interlocutors. © 2012 IEEE.";https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867602874&doi=10.1109%2fICASSP.2012.6288916&partnerID=40&md5=5c8919800c6d399cced914b4bb5862ef;42.Chaspari.pdf;Yes;Rachel;"It consists of four sessions in which the
child interacts with Rachel and his/her parent.";Virtual Agent;"The Rachel
system, developed at the University of Southern California [6, 7], is
a platform for the collection of emotional, social and communicative
behavior observations between a child, the ECA agent, and the
child’s parent (Section 2).";Wizarded;"The ECA was
controlled using the Wizard of Oz paradigm";Yes;"The Rachel
system, developed at the University of Southern California [6, 7], is
a platform for the collection of emotional, social and communicative
behavior observations between a child, the ECA agent, and the
child’s parent (Section 2).";Yes;"The Rachel
system, developed at the University of Southern California [6, 7], is
a platform for the collection of emotional, social and communicative
behavior observations between a child, the ECA agent, and the
child’s parent (Section 2).";Wizarded;;ns;ns;ns;;Healthcare;"This study provides a novel vocal analysis of the indicators of shared
enjoyment in ECA interactions of children with autism";companion;equal;"The Rachel
system, developed at the University of Southern California [6, 7], is
a platform for the collection of emotional, social and communicative
behavior observations between a child, the ECA agent, and the
child’s parent (Section 2).";lab;lab;"Each session was
recorded with a smart-room setup consisting of three Sony High-
Definition cameras and two shot-gun microphones.";Interact socially;"The Rachel
system, developed at the University of Southern California [6, 7], is
a platform for the collection of emotional, social and communicative
behavior observations between a child, the ECA agent, and the
child’s parent (Section 2).";6, 12;broad;7;wide;This paper includes data from the pilot studies conducted for two children. The first child was a 12-year old boy with an expressive language score of 6 years, 7 months. The second child was a 6-year-old boy with an expressive language score of 2 years, 9 months.;Autistic;"This study provides a novel vocal analysis of the indicators of shared
enjoyment in ECA interactions of children with autism";2;01/Oct;This paper includes data from the pilot studies conducted for two children. The first child was a 12-year old boy with an expressive language score of 6 years, 7 months. The second child was a 6-year-old boy with an expressive language score of 2 years, 9 months.;Effect with CA on xyz (could have been any CA);"The goal of this work is
to study the acoustic differences of areas surrounding the two kinds
of shared enjoyment instances, called “social zones”, using data collected
from children with autism, and their parents, interacting with
an Embodied Conversational Agent (ECA).";social zones;"The goal of this work is
to study the acoustic differences of areas surrounding the two kinds
of shared enjoyment instances, called “social zones”, using data collected
from children with autism, and their parents, interacting with
an Embodied Conversational Agent (ECA).";acoustics;"The goal of this work is
to study the acoustic differences of areas surrounding the two kinds
of shared enjoyment instances, called “social zones”, using data collected
from children with autism, and their parents, interacting with
an Embodied Conversational Agent (ECA).";;;;No;;ns;;ns;;
56;2010;in proceedings;AFFINE;misc;conference;France;Europe;Use of Nonverbal Speech Cues in Social Interaction between Human and Robot: Emotional and Interactional Markers;We focus on audio cues required for the interaction between a human and a robot. We argue that a multi-level use of different paralinguistic cues is needed to pilot the decisions of the robot. Our challenge is to know how to use them to pilot the human-robot interaction. We offer in this paper a protocol for a study on the way paralinguistic cues can impact the human-robot interaction, by interpreting the low-level cues computed from speech into an emotional and interactional profile of the user. This study will be carried out through a game between two children and a robot.;https://doi.org/10.1145/1877826.1877846;56.Delaborde.pdf;Yes;Nao;Aldebaran's social robot Nao,;Robot;Aldebaran's social robot Nao,;Wizarded;"The robot is
remotely controlled according to a predefined scenario (known by
the game master), without the children being aware of that";No;Aldebaran's social robot Nao,;ns;;ns;;ns;ns;ns;;Entertainment;"The game master explains the rule of the first game, a
question-answer game, and explains that the robot will also be a
player. The game master deals the cards in the course of the game.";companion;equal;In the context of the robot as a game companion;lab;lab;"In each recording session, two children are invited to play
games with the robot at our laboratory.";elicit emotional responses;"These strategies triggered
various reactions from the children:";8, 9, 10, 11, 12, 13;broad;6;wide;"They were
aged from eight to thirteen,";ns;;10;01/Oct;We recorded ten French children;Effect with CA on xyz (could have been any CA);"We offer in this paper a protocol for a study on
the way paralinguistic cues can impact the human-robot
interaction, by interpreting the low-level cues computed from
speech into an emotional and interactional profile of the user";sex, age;"The proportion between positive and negative emotions will allow
us to define if the user is rather optimistic and the presence of
false starts, postponed starts or dysfluences in the speech a notion
of the self-confidence of the user. We also will use the prediction
of the user's sex and age bracket and, when it is available, the
identitication of the speaker [17].";nonverbal behavior;"The proportion between positive and negative emotions will allow
us to define if the user is rather optimistic and the presence of
false starts, postponed starts or dysfluences in the speech a notion
of the self-confidence of the user. We also will use the prediction
of the user's sex and age bracket and, when it is available, the
identitication of the speaker [17].";;;;No;;French;We recorded ten French children;Other;ethical concerns, since robot deliberately falters, anomalies happen to elicit emotional responses, but no considerations are mentioned;
61;2018;conference;IDC;CCI;conference;Turkey, Netherlands;Combination;"""Wow he is talking!"" A study on intrinsic motivations for child-robotic toy interactions";This paper presents a study conducted to observe motivations for playful interaction of children with the prototype version of the robotic toy Ixi-play. Fourteen children from two age groups (4-5 and 8-9 year-olds) participated in the study. The features of the robotic toy that intrinsically motivate children for interaction were identified through qualitative analysis. The types of childrobotic toy interaction were revealed as: physical interaction, facial expressions, verbal communication, and visual engagement. Four factors were identified as affecting children's intrinsic motivation for an engaged interaction with robotic toys: i) evolving needs and abilities of children, ii) ease of bonding, iii) playfulness, and iv) clarity of responses and multiple feedback. © 2018 Association for Computing Machinery.;https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051490737&doi=10.1145%2f3202185.3202756&partnerID=40&md5=7bba2a8e8b4ae3a097cf3a1533210985;61.Dönmez.pdf;Yes;Ixi-play;"This paper presents a study conducted to observe
motivations for playful interaction of children with the
prototype version of the robotic toy Ixi-play";Robot;"This paper presents a study conducted to observe
motivations for playful interaction of children with the
prototype version of the robotic toy Ixi-play";ns;;Yes;"Ixi-play is an owl-like robotic toy developed by WittyWorX
(The Netherlands). It is intended to be a play companion for
young children and aims to eliminate the need for
supervision while offering engaging activities";Yes;"Ixi-play responds to the cards
by imitating the shown emotion card, making the animal
sound or saying ""yes"" or ""no"". The participant can also pet
Ixi-play's head and receive an affectionate response.
The animal sounds game follows a stimulus-response
strategy, and allows self-correction of actions. Ixi-play
makes an animal sound, the child is expected to understand
it and show the right animal card to Ixi-play. Ixi-play
responds in Dutch (""Yes, very good"" or ""No""). If the child
wants Ixi-play to repeat the sound, s/he pets its head. Once
the correct card is shown, Ixi-play makes another animal
sound and the game continues.";Wizarded;;not relevant;not relevant;;;Entertainment;"Ixi-play is an owl-like robotic toy developed by WittyWorX
(The Netherlands). It is intended to be a play companion for
young children and aims to eliminate the need for
supervision while offering engaging activities";companion;equal;"Ixi-play is an owl-like robotic toy developed by WittyWorX
(The Netherlands). It is intended to be a play companion for
young children and aims to eliminate the need for
supervision while offering engaging activities";school;school;"The research setting was an activity room in the school
(Figure 1, top right";play game and respond to cards;"Ixi-play is an owl-like robotic toy developed by WittyWorX
(The Netherlands). It is intended to be a play companion for
young children and aims to eliminate the need for
supervision while offering engaging activities";4, 5, 8, 9;broad;6;wide;"The participants were six children
between the ages of 4 and 5 (1 female, 5 males), and eight
children between the ages of 8 and 9 (4 females and 4
males)";ns;;14;Nov/20;"The observation was conducted with 14 English-speaking
pupils from an international primary school";Effect of CA on xyz;"observe
motivations for playful interaction of children with the
prototype version of the robotic toy Ixi-play";engaged interaction;"Four factors were identified as affecting
children's intrinsic motivation for an engaged interaction
with robotic toys:";motivation;"Four factors were identified as affecting
children's intrinsic motivation for an engaged interaction
with robotic toys:";;;;No;;English;"The observation was conducted with 14 English-speaking
pupils from an international primary school";English;;
69;2010;conference;IDC;CCI;conference;Netherlands;Europe;Let robots do the talking;This paper describes an investigation of different variations of the robotic intervention protocol, a method for obtaining verbalization data from children during evaluation that involves using a social robot as a proxy for a test facilitator to prompt, ask, and help children thus encouraging them to verbalize their thoughts and experiences. A number of variations of this verbalization protocol were implemented and tried out, leading to guidelines for the application of the method and for the implementation of social robots aimed to support it. Copyright 2010 ACM.;https://www.scopus.com/inward/record.uri?eid=2-s2.0-77954922958&doi=10.1145%2f1810543.1810551&partnerID=40&md5=d018ede980d0bb13a7af3d4d8ec9a8c5;69.Fransen.pdf;Yes;iCat;"In a room the iCat (named Sophie in all tests) faces
the chair of the child,";Robot;"The iCat (figure 2), a social robot developed by Philips
research [5], was used to support the robotic intervention
method.";Wizarded;"The speech of the iCat
was produced in real time as the facilitator typed messages
on a desktop computer application, which were converted
to speech using a commercial text to speech generator";No;"The iCat is a stationary (desktop) robot developed
for research purposes.";Partially;"The speech of the iCat
was produced in real time as the facilitator typed messages
on a desktop computer application, which were converted
to speech using a commercial text to speech generator";Wizarded;;free speech;free speech;;"‘I like to help you
with your task, do you have a question? What’s more I like
you to speak a lot during the task.";Other;not clear In text, quite generic;companion;equal;"They are told that the robot is there to help them with some
tasks they have to perform using an interactive application";school;school;"Test
sessions were carried out during school hour in a room
offered by the school";Intervention;"the robotic
intervention technique";6, 7, 8, 9;broad;4;broad;children aged 6-9.;ns;;26;21-30;"For each iteration different part of the
class could participate; eventually all 26 children of the
class took part in one evaluation session.";Effect with CA on xyz (could have been any CA);"The research reported in this paper set out to understand the
potential value of using social robots in an evaluation with
children, and to explore how best to apply this approach in
order to make the evaluation session effective but also
comfortable and enjoyable for the children taking part.";Intervention method;"We compared the improved robotic intervention method to
‘traditional’ active intervention by an adult test facilitator.
This did not show any major gains regarding the amount of
verbalization obtained as we had intended";Verbalization;"We compared the improved robotic intervention method to
‘traditional’ active intervention by an adult test facilitator.
This did not show any major gains regarding the amount of
verbalization obtained as we had intended";;;;No;;Dutch;"N.B. all robot and
children utterances quoted in this paper are translated from
their original formulation in Dutch).";Other;;
79;2006;article;Artificial Life Robotics;robot/agent;journal;Japan;East Asia;Child-robot interaction mediated by building blocks: From field observations in a public space;"This study attempts to describe children's behavior from the viewpoint of microscopic adjustment of actions when they encounter an oddly shaped robot, called ""Muu."" We investigated this through field observations at a workshop in a children's museum. Children of various ages and their parents participated in the workshop together. They were instructed by an experimenter to play with building blocks while talking with Muu. As a result, it was found that the children and the robot could establish rich communications with each other not when the children evaluated Muu's behavior, but when Muu evaluated the children's work. This indicates that the robot could become an ""other"" that might interact with children mediated by the building blocks, whereas many children and parents treated it as a ""toy,"" just as the building blocks were considered merely as ""objects"" during interactions. © ISAROB 2006.";https://www.scopus.com/inward/record.uri?eid=2-s2.0-33746428611&doi=10.1007%2fs10015-005-0375-3&partnerID=40&md5=3078b749e836713dadff1df2c0729e9d;79.Goan.pdf;Yes;Muu;they encounter an oddly shaped robot, called “Muu;Robot;they encounter an oddly shaped robot, called “Muu;Wizarded;"The Wizard of Oz method was used,4
i.e., Muu was controlled to speak appropriate words, selected
from 150 prepared words, according to the interaction
with the subject.";ns;;Partially;"The Wizard of Oz method was used,4
i.e., Muu was controlled to speak appropriate words, selected
from 150 prepared words, according to the interaction
with the subject.";Wizarded;"The Wizard of Oz method was used,4
i.e., Muu was controlled to speak appropriate words, selected
from 150 prepared words, according to the interaction
with the subject.";free speech;free speech;;"However, a few cases
showed rich and natural human–robot communication. This
section focuses on three typical cases of “good” communication.";Entertainment;"This experiment was implemented in the field at a workshop
in a children’s museum, “Kids Plaza OSAKA,”";companion;equal;"This is Muu. It seems he wants to ask
you to build up the toy blocks while speaking with him.
Will you help him to build up the toy blocks?";museum;public space;"This experiment was implemented in the field at a workshop
in a children’s museum, “Kids Plaza OSAKA,”";Evaluate task and give directions when necessary;"The contents of its speech
were divided into different categories. One is concerned
with the toy blocks, for example, “Pile on a red block,
please.” Another involves an evaluation of the child’s work,
for example, “It’s cool, isn’t it?” The other category is for
compliments or chiming in, for instance, “I see.”";6, 7, 8, 9, 10, 11, 12;broad;7;wide;"The
children’s ages varied from 2 to 12 years.";ns;;42;41-50;"The total number of participants
was 69: 30 teams from 42 children and 27 attendants";Effect of CA on xyz;"describe children’s behavior
from the viewpoint of microscopic adjustment of actions
when they encounter an oddly shaped robot, called “Muu.”";Robot;"It was clarified that children and the communication robot
Muu were able to establish rich and natural communication
with each other not when the children evaluated Muu’s
behavior, but when Muu evaluated the children’s work.";speech;"It was clarified that children and the communication robot
Muu were able to establish rich and natural communication
with each other not when the children evaluated Muu’s
behavior, but when Muu evaluated the children’s work.";;;;No;;ns;;ns;;
82;2015;in proceedings;IDC;CCI;conference;USA;North America;Designing a Virtual Assistant for In-Car Child Entertainment;Driving is an attention-demanding task, especially with children in the back seat. While most recommendations prefer to reduce children's screen time in common entertainment systems, e.g. DVD players and tablets, parents often rely on these systems to entertain the children during car trips. These systems often lack key components that are important for modern parents, namely, sociability and educational content. In this contribution we introduce PANDA, a parental affective natural driving assistant. PANDA is a virtual in-car entertainment agent that can migrate around the car to interact with the parent-driver or with children in the back seat. PANDA supports the parent-driver via speech interface, helps to mediate her interaction with children in the back seat, and works to reduce distractions for the driver while also engaging, entertaining and educating children. We present the design of PANDA system and preliminary tests of the prototype system in a car setting.;https://doi.org/10.1145/2771839.2771916;82.Gordon.pdf;Yes;PANDA;PANDA, a parental affective natural driving assistant;Virtual Agent;"PANDA is a virtual in-car entertainment agent that can migrate
around the car to interact with the parent-driver or with children in
the back seat.";Fully autonomous;"we
designed a failsafe mechanism, such that any touching on the
tablet while PANDA appears stops the program. This was
designed to enable the parent to stop the interaction rapidly if it
becomes too distracting or fails to operate. Panda also stops if it
hears the phrases like PANDA stop, PANDA exit, or stop PANDA.";Yes;"PANDA is designed to address the following issues:
• Parental assistance in reducing attentional and task load.
• Parent-supervised entertainment for children in the car.
• Social, educational and relational activities.
To fulfill these goals, PANDA has a face that appears on either
the front- or back- seat tablets, can produce facial expressions,
recognize speech and interact with the driver and passengers in
the car in a socially engaging way.";Yes;"One of the scenarios we added to PANDA, that aims to create a
positive social atmosphere and interaction in the car, is
conversation starters. Conversation starters are questions PANDA
asks either the children or the driver in an attempt to start a
conversation between the family members in the car.
One line of interaction is asking the children to talk about their
day: What was the most fun thing you did today?, What was the
funniest thing that happened today? Who did you play with today?
Another options is to try and engage the driving parent in a low
attentional-load conversation, such as suggesting to the child to
ask: Why don’t you ask your parent what was their most fun part
of the day.
Finally, PANDA can tell silly things it did, thus both reinforcing it
as a social agent with its own experiences, and also introducing
comical conversation starters.";No;"The scenarios are designed to be simple, and
require short phrases from the children or driver, such as: “show
movie”, “yes”, “no” and other phrases selecting from choices
offered by the agent. This solves one of the major issues with the
PANDA system, which is speech recognition for children.";limited commands;limited;ns;"The scenarios are designed to be simple, and
require short phrases from the children or driver, such as: “show
movie”, “yes”, “no” and other phrases selecting from choices
offered by the agent. This solves one of the major issues with the
PANDA system, which is speech recognition for children.";Entertainment;"PANDA is designed to address the following issues:
• Parental assistance in reducing attentional and task load.
• Parent-supervised entertainment for children in the car.
• Social, educational and relational activities.";Buddy for children, help for parents;equal;"PANDA is designed to address the following issues:
• Parental assistance in reducing attentional and task load.
• Parent-supervised entertainment for children in the car.
• Social, educational and relational activities.";car;other;"We had initially tested the system in the researcher car while
driving. The researcher happens to have two small children of
ages 4 and 5.5. We evaluated the system in 2 test drives, each
lasting around 10 minutes, where the first interacted with a four
year old boy, the second with a five and half year old girl.";entertain children;"PANDA is designed to address the following issues:
• Parental assistance in reducing attentional and task load.
• Parent-supervised entertainment for children in the car.
• Social, educational and relational activities.";4, 5;young;2;narrow;"We had initially tested the system in the researcher car while
driving. The researcher happens to have two small children of
ages 4 and 5.5. We evaluated the system in 2 test drives, each
lasting around 10 minutes, where the first interacted with a four
year old boy, the second with a five and half year old girl.";ns;;2;01/Oct;"We had initially tested the system in the researcher car while
driving. The researcher happens to have two small children of
ages 4 and 5.5. We evaluated the system in 2 test drives, each
lasting around 10 minutes, where the first interacted with a four
year old boy, the second with a five and half year old girl.";Evaluating the CA;"The first evaluation tests were functionality of the hardware and
communication in a real field test";CA;"The first evaluation tests were functionality of the hardware and
communication in a real field test";errors, usability, recognition;"The first evaluation tests were functionality of the hardware and
communication in a real field test";;;;No;;ns;;ns;"acknowledge ASR for children is hard: The scenarios are designed to be simple, and
require short phrases from the children or driver, such as: “show
movie”, “yes”, “no” and other phrases selecting from choices
offered by the agent. This solves one of the major issues with the
PANDA system, which is speech recognition for children.";
90;2017;conference;RO-MAN;robot/agent;conference;USA;North America;He can read your mind: Perceptions of a character-guessing robot;"After playing a five to seven minute character guessing game with a Nao robot, children answered questions about their perceptions of the robot's abilities. Responses from interactions with 30 children, ages eight to twelve, showed that when the robot made an attempt at guessing the participant's character, rather than being stumped and unable to guess, the robot was more likely to be perceived as being able to understand the participant's feelings and able to provide advice. Regardless of their game experience, boys were more likely than girls to feel they could have discussions with the robot about things they could not talk to other people about. This article provides details associated with the implementation of a game used to guess a character the children selected; a twelve question verbally-Administered survey that examined their perceptions of the robot; quantitative and qualitative results from the study; and a discussion of the implications, limitations, and future directions of this research. © 2017 IEEE.";https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045766364&doi=10.1109%2fROMAN.2017.8172309&partnerID=40&md5=a4af6c035d10971cb2946148b2d75b65;90.Henkel.pdf;Yes;Nao;a Nao robot;Robot;a Nao robot;Wizarded;"The robot was controlled using a Wizard-of-Oz approach [8],
with a human robot operator acting as the robot’s speech
recognizer, updating the game state and administering the
robot’s speech utterances";ns;;ns;;No;"The robot then posed a series of
increasingly specific questions to the participant, which could
be answered with “yes”, “no”, “I don’t know”, “probably”, or
“probably not”.";limited commands;limited;5;"The robot then posed a series of
increasingly specific questions to the participant, which could
be answered with “yes”, “no”, “I don’t know”, “probably”, or
“probably not”.";Entertainment;"We implemented a character guessing game that had a
duration of five to seven minutes, in which a Nao robot
asked each participant a series of questions in an effort to
guess the identity of a fictional or non-fictional character
the participant selected.";opponent;equal;"We implemented a character guessing game that had a
duration of five to seven minutes, in which a Nao robot
asked each participant a series of questions in an effort to
guess the identity of a fictional or non-fictional character
the participant selected.";ns;ns;;guess the character;"We implemented a character guessing game that had a
duration of five to seven minutes, in which a Nao robot
asked each participant a series of questions in an effort to
guess the identity of a fictional or non-fictional character
the participant selected.";8,9,10,11,12;broad;5;broad;"were
between the ages of eight and twelve";ns;;30;21-30;"33 participants completed the study, yielding
30 sessions of analyzable data";Effect with CA on xyz (could have been any CA);"the development of a thorough
understanding of children’s perceptions and beliefs about
social robots is essential. To gain further insight into this
area, we equipped a robot with character guessing abilities,
an identifiable skill that is beyond that of most humans,
and designed an open-ended verbally-administered survey
to understand participants’ beliefs, feelings, and potential
behaviors related to the robot";robot, interaction;"the development of a thorough
understanding of children’s perceptions and beliefs about
social robots is essential. To gain further insight into this
area, we equipped a robot with character guessing abilities,
an identifiable skill that is beyond that of most humans,
and designed an open-ended verbally-administered survey
to understand participants’ beliefs, feelings, and potential
behaviors related to the robot";perceptions, beliefs;"the development of a thorough
understanding of children’s perceptions and beliefs about
social robots is essential. To gain further insight into this
area, we equipped a robot with character guessing abilities,
an identifiable skill that is beyond that of most humans,
and designed an open-ended verbally-administered survey
to understand participants’ beliefs, feelings, and potential
behaviors related to the robot";;;;no;;ns;;ns;;
91;2013;in proceedings;IDC;CCI;conference;Netherlands, USA;Combination;Engaging Children in Cars through a Robot Companion;Having children as passengers in a car influences the parents' experience of driving. Concern for their safety often supersedes other considerations. When designing in-car solutions to address the special needs of children as passengers, one could aim for assisting the parents with this task. For such systems, it is important that the proposed solution is able to engage the children and keeps them from distracting the driver, while offering the children an interesting and meaningful way to spend their time in the car. We propose and evaluate a conceptual design that involves an interactive, full-speech companion that uses information from the drive to entertain and educate children. Our evaluation reveals that a robot companion is able to engage the children more than a similar system without a physical companion, giving them an entity to direct their interactions to. This finding makes it a worthwhile consideration for designers to add such components to their solutions.;https://doi.org/10.1145/2485760.2485815;91.Hiah.pdf;Yes;ns;;Robot;"Our
evaluation reveals that a robot companion is able to engage the
children more than a similar system without a physical
companion, giving them an entity to direct their interactions to.";Wizarded;"The full-speech interaction, and the robot’s expressions and
movements were all simulated using the Wizard-of-Oz method:
one wizard controlled the robot, while a second wizard’s voice
was transformed through software and functioned as the system’s
speech-interface.";Yes;"The full-speech interaction, and the robot’s expressions and
movements were all simulated using the Wizard-of-Oz method:
one wizard controlled the robot, while a second wizard’s voice
was transformed through software and functioned as the system’s
speech-interface.";Not relevant;;Wizarded;;ns;ns;ns;;Edutainment;"We propose and evaluate a conceptual design that
involves an interactive, full-speech companion that uses
information from the drive to entertain and educate children";companion;equal;"We propose and evaluate a conceptual design that
involves an interactive, full-speech companion that uses
information from the drive to entertain and educate children";lab;lab;"The experiment was based on role-play, and therefore, a setting
was developed that would match the target environment to assist
the children with their imagination.";entertain and educate;"We propose and evaluate a conceptual design that
involves an interactive, full-speech companion that uses
information from the drive to entertain and educate children";5,6,7;young;3;narrow;16 pupils (ages 5-7);ns;;16;Nov/20;16 pupils (ages 5-7);Effect with CA on xyz (could have been any CA);"Our
evaluation reveals that a robot companion is able to engage the
children more than a similar system without a physical
companion, giving them an entity to direct their interactions to.";physical robot or not;"Our
evaluation reveals that a robot companion is able to engage the
children more than a similar system without a physical
companion, giving them an entity to direct their interactions to.";engagement;"Adding a robot companion to
this context greatly increases children’s engagement with the
system, giving parent-drivers a peace of mind concerning safety,
and targeting a younger group of users who cannot reach the
visuals on the window yet, due to their size or their seat’s
restrictions.";;;;no;;ns;;ns;no markings in pdf due to crash;NO MARKINGS
96;2017;conference;HFES;hci;conference;USA;North America;Exploration of human reactions to a humanoid robot in public STEM education;"The service and entertainment industry advocates the possibility of using humanoid robots; however, direct interaction experience is uncommon. To understand humans' interactions with humanoid robots, the present study used a robot capable of face recognition and conversation in a park and a school setting to explore the behavioral patterns of humans, dialog themes, and emotional responses. Results showed that humans' behavioral patterns included looking at the robot, talking to the robot, talking to others about the robot, and adults taking photos. School children showed strong interest to interact with the robot and rich emotional responses. Major dialog themes included greeting, asking about the robot's identity, testing the robot's knowledge and capabilities, asking and replying about preferences and opinions, and correcting the robot's errors. Observed emotional responses included liking, surprise, excitement, fright, frustration, and awkwardness. Humans interacted with the robot similarly to how they would interact with other humans but also differently. The educational value and design implication for humanoid robots are discussed. Copyright 2017 by Human Factors and Ergonomics Society.";https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042464206&doi=10.1177%2f1541931213601796&partnerID=40&md5=275424d9e29c00d4d490af52a564b672;96.Huang.pdf;Yes;Ken;KEN is a humanoid robot made from a mannequin upper body and head, with built-in computers;Robot;KEN is a humanoid robot made from a mannequin upper body and head, with built-in computers;Fully autonomous;"KEN uses voice activity detection and speech recognition to record spoken phrases and translate them to
text. The artificial intelligence system based on the A.L.I.C.E. chatbot processes the text into a response, which is spoken back using the eSpeak text-to-speech synthesizer. KEN has a speaker embedded in its chest. It hears through a microphone on the table or a cell phone receiver. When the background noise is low, the audience can speak directly standing in front of KEN. When there is a certain level of noise, the audience needs to pick up the microphone on the table to speak to KEN. Voice activity detection is done by a naïve sound intensity threshold algorithm. Manual voice activity detection is possible for acoustically challenging environments by using either a cell phone app or manually unmuting and muting the microphone. Speech-to-text conversion is performed by the Google Web Speech API. The computer monitor shows the transcribed text KEN receives from the audience and the machine generated responses that it speaks.";No;"text. The artificial intelligence system based on the A.L.I.C.E.
chatbot processes the text into a response, which is spoken
back using the eSpeak text-to-speech synthesizer. KEN has a
speaker embedded in its chest. It hears through a microphone
on the table or a cell phone receiver. When the background
noise is low, the audience can speak directly standing in front
of KEN. When there is a certain level of noise, the audience
needs to pick up the microphone on the table to speak to KEN.
Voice activity detection is done by a naïve sound intensity
threshold
algorithm.
Manual
voice
activity
detection
is
possible for acoustically challenging environments by using
either a cell phone app or manually unmuting and muting the
microphone. Speech-to-text conversion is performed by the
Google Web Speech API. The computer monitor shows the
transcribed text KEN receives from the audience and the
machine generated responses that it speaks.
KEN uses voice activity detection and speech
recognition to record spoken phrases and translate them to";No;"text. The artificial intelligence system based on the A.L.I.C.E.
chatbot processes the text into a response, which is spoken
back using the eSpeak text-to-speech synthesizer. KEN has a
speaker embedded in its chest. It hears through a microphone
on the table or a cell phone receiver. When the background
noise is low, the audience can speak directly standing in front
of KEN. When there is a certain level of noise, the audience
needs to pick up the microphone on the table to speak to KEN.
Voice activity detection is done by a naïve sound intensity
threshold
algorithm.
Manual
voice
activity
detection
is
possible for acoustically challenging environments by using
either a cell phone app or manually unmuting and muting the
microphone. Speech-to-text conversion is performed by the
Google Web Speech API. The computer monitor shows the
transcribed text KEN receives from the audience and the
machine generated responses that it speaks.
KEN uses voice activity detection and speech
recognition to record spoken phrases and translate them to";No;"text. The artificial intelligence system based on the A.L.I.C.E.
chatbot processes the text into a response, which is spoken
back using the eSpeak text-to-speech synthesizer. KEN has a
speaker embedded in its chest. It hears through a microphone
on the table or a cell phone receiver. When the background
noise is low, the audience can speak directly standing in front
of KEN. When there is a certain level of noise, the audience
needs to pick up the microphone on the table to speak to KEN.
Voice activity detection is done by a naïve sound intensity
threshold
algorithm.
Manual
voice
activity
detection
is
possible for acoustically challenging environments by using
either a cell phone app or manually unmuting and muting the
microphone. Speech-to-text conversion is performed by the
Google Web Speech API. The computer monitor shows the
transcribed text KEN receives from the audience and the
machine generated responses that it speaks.
KEN uses voice activity detection and speech
recognition to record spoken phrases and translate them to";free speech;free speech;;"text. The artificial intelligence system based on the A.L.I.C.E.
chatbot processes the text into a response, which is spoken
back using the eSpeak text-to-speech synthesizer. KEN has a
speaker embedded in its chest. It hears through a microphone
on the table or a cell phone receiver. When the background
noise is low, the audience can speak directly standing in front
of KEN. When there is a certain level of noise, the audience
needs to pick up the microphone on the table to speak to KEN.
Voice activity detection is done by a naïve sound intensity
threshold
algorithm.
Manual
voice
activity
detection
is
possible for acoustically challenging environments by using
either a cell phone app or manually unmuting and muting the
microphone. Speech-to-text conversion is performed by the
Google Web Speech API. The computer monitor shows the
transcribed text KEN receives from the audience and the
machine generated responses that it speaks.
KEN uses voice activity detection and speech
recognition to record spoken phrases and translate them to";Entertainment;"The service and entertainment industry advocates the possibility of using humanoid robots; however, direct interaction experience is uncommon.";companion;equal;Second, the children were asked to turn to face KEN at the west wall and the robot developer went through the following steps: (a) Briefly introducing KEN, (b) demonstrating speaking to KEN, (c) using two American Girl dolls (Emily & Liberty) to demonstrate face recognition and to ask the children the difference between a doll and a robot, (d) asking volunteers to come up to talk to KEN, one person at a time, and (e) answering more questions from the audience.;school;school;OBSERVATION 2 : AT A SCHOOL;entertain;"The service and entertainment industry advocates the possibility of using humanoid robots; however, direct interaction experience is uncommon.";6,7,8,9,10,11,12,13;broad;6;wide;A total of 360 children from kindergarten to 7th grade participated in the event (from age 6 to 13).;ns;;360;>100;A total of 360 children from kindergarten to 7th grade participated in the event (from age 6 to 13).;Effect with CA on xyz (could have been any CA);To understand humans’ interactions with humanoid robots, the present study used a robot capable of face recognition and conversation in a park and a school setting to explore the behavioral patterns of humans, dialog themes, and emotional responses.;humanoid robots;To understand humans’ interactions with humanoid robots, the present study used a robot capable of face recognition and conversation in a park and a school setting to explore the behavioral patterns of humans, dialog themes, and emotional responses.;behavioral patterns, dialog themes, emotional responses;To understand humans’ interactions with humanoid robots, the present study used a robot capable of face recognition and conversation in a park and a school setting to explore the behavioral patterns of humans, dialog themes, and emotional responses.;Children also talked to the humanoid robot and expected it to respond like a human. The majority of the school children were eager to interact with KEN and reluctant to leave. The questions asked by children at three different grade levels about why KEN does not have arms and legs suggested that they anthropomorphized the robot and found it odd that it was incomplete. Children’s questions, behaviors, and emotions showed strong curiocity and excitement toward the robot.;na;na;No;;ns;;ns;;
98;2010;conference;HRI;robot/agent;conference;Korea;East Asia;Relationships between user experiences and children's perceptions of the education robot;"The purpose of this study is to investigate the biological, mental, social, moral, and educational perceptions of young children of the intelligent robot iRobiQ and to explore the effects of user experience on them. The interview was conducted with 111 five-year-old children attending two kindergartens and two childcare centers in which iRobiQ had been purchased and had been in use since March 2009. The young children interacted with the robot for one hour or less everyday over a period of two weeks or less. The robot contents were related to the socio-emotional perceptions of robots and had a high level of human-robot interactions, such as ""Talking with the Robot"" or ""Attendance Check."" Children who experienced the ""voice"" and ""touch screen"" functions of the robot showed higher educational perception. The social and educational perception was higher when the robot was placed in a classroom than when it was placed in the hallway or in the office. The results indicated that robot content focusing on socio-emotional characteristics should be developed for educational purposes and that a robot should be placed in the classroom for individual use. © 2010 IEEE.";https://www.scopus.com/inward/record.uri?eid=2-s2.0-77951527979&doi=10.1145%2f1734454.1734542&partnerID=40&md5=ef158c8d1ea7af52359eb0af3335db65;98.Hyun.pdf;Yes;iRobiQ;the intelligent robot iRobiQ;Robot;the intelligent robot iRobiQ;ns;;Yes;"The education robot iRobiQ is a
small, intelligent robot that has been
developed as an educational tool by
Yujin Robot Co., Ltd. It can move
its head and arms, can wheel itself,
and can express simulated emotions
through the use of face lamps.";ns;;ns;;ns;ns;ns;;Education;"to investigate the
biological, mental, social, moral, and educational perceptions of
young children of the intelligent robot iRobiQ";companion?;equal;"to investigate the
biological, mental, social, moral, and educational perceptions of
young children of the intelligent robot iRobiQ";child care facilities;other;"Interviews were conducted of 111 five-year-olds in two
kindergartens and two childcare centers in which iRobiQ has
been in use since March 2009";educate;"The education robot iRobiQ is a
small, intelligent robot that has been
developed as an educational tool by
Yujin Robot Co., Ltd. It can move
its head and arms, can wheel itself,
and can express simulated emotions
through the use of face lamps.";5;young;1;narrow;"Interviews were conducted of 111 five-year-olds in two
kindergartens and two childcare centers in which iRobiQ has
been in use since March 2009";ns;;111;>100;"Interviews were conducted of 111 five-year-olds in two
kindergartens and two childcare centers in which iRobiQ has
been in use since March 2009";Evaluating the CA;"to investigate the
biological, mental, social, moral, and educational perceptions of
young children of the intelligent robot iRobiQ";robot interaction;"to investigate the
biological, mental, social, moral, and educational perceptions of
young children of the intelligent robot iRobiQ and to explore
the effects of user experience on them.";perceptions, effects of user experience;"to investigate the
biological, mental, social, moral, and educational perceptions of
young children of the intelligent robot iRobiQ and to explore
the effects of user experience on them.";"Children who
experienced the “voice” and “touch screen” functions of the
robot showed higher educational perception. The social and
educational perception was higher when the robot was placed in
a classroom than when it was placed in the hallway or in the
office.";<.05;Table 1;no;;ns;;ns;very badly reported;
99;2015;in proceedings;HRI;robot/agent;conference;Korea;East Asia;The Acoustic-Phonetics Change of English Learners in Robot Assisted Learning;This study is to verify the effectiveness of robot TTS technology in assisting Korean English language learners to acquire a native-like accent by correcting the prosodic errors they commonly make. Child English language learners' F0 range, a prosodic variable, will be measured and analyzed for any changes in accent. We examined whether if robot with the currently available TTS technology appeared to be effective as much as a tele-presence robot with native speaker from the acoustic phonetic viewpoint.;https://doi.org/10.1145/2701973.2702003;99.In.pdf;Yes;ROBOSEM;ROBOSEM;Robot;"We examined whether if robot
with the currently available TTS technology";Combination;"We examined whether if robot
with the currently available TTS technology appeared to be
effective as much as a tele-presence robot --> telepresence meaning WOZ";ns;;Partially;"They were subsequently divided into two
groups of adult TTS and child TTS";ns;;ns;ns;ns;;Education;"This study is to verify the effectiveness of robot TTS
technology in assisting Korean English language learners to
acquire a native-like accent by correcting the prosodic errors
they commonly make";teacher;higher;"This study is to verify the effectiveness of robot TTS
technology in assisting Korean English language learners to
acquire a native-like accent by correcting the prosodic errors
they commonly make";ns;ns;;assist;"This study is to verify the effectiveness of robot TTS
technology in assisting Korean English language learners to
acquire a native-like accent by correcting the prosodic errors
they commonly make";10;10/Dec;1;narrow;"Considering the English capability of children learners in
Korean elementary schools, 16 students (8 boys, 8 girls)
among the 4th grade (10 years old) were selected as the
study participants.";ns;;16;Nov/20;"Considering the English capability of children learners in
Korean elementary schools, 16 students (8 boys, 8 girls)
among the 4th grade (10 years old) were selected as the
study participants.";Effect of CA on xyz;"This study is to verify the effectiveness of robot TTS
technology in assisting Korean English language learners to
acquire a native-like accent by correcting the prosodic errors
they commonly make";robot TTS;"This study examined the potential of the robot TTS as an
effective instructional tool in assisting English language
learners to produce a natural English accent";F0 range;"We aim to measure and to
analysis a prosodic condition of robot TTS with the acoustic
phonetic variable (F0 range) in English language learning
from the acoustic phonetic viewpoint.";;;;ns;;English;"This study is to verify the effectiveness of robot TTS
technology in assisting Korean English language learners to
acquire a native-like accent by correcting the prosodic errors
they commonly make";English;;
104;2014;in proceedings;UbiComp;hci;conference;Korea;East Asia;Robot-Based Augmentative and Alternative Communication for Nonverbal Children with Communication Disorders;"Nonverbal children with communication disorders have difficulties communicating through oral language. To facilitate communication, Augmentative and Alternative Communication (AAC) is commonly used in intervention settingss. Different forms of AAC have been used; however, one key aspect of AAC is that children have different preferences and needs in the intervention process. One particular AAC method does not necessarily work for all children. Although robots have been used in different applications, this is one of the first times that robots have been used for improvement of communication in nonverbal children. In this work, we explore robot-based AAC through humanoid robots that assist therapists in interventions with nonverbal children. Through playing activities, our study assessed changes in gestures, vocalization, speech, and verbal expression in children. Our initial results show that robot-based AAC intervention has a positive impact on the communication skills of nonverbal children.";https://doi.org/10.1145/2632048.2636078;104.Jeon.pdf;Yes;iRobi;Child using iRobi;Robot;"In this work, we explore
robot-based AAC through humanoid robots that assist therapists
in interventions with nonverbal children.";Wizarded;"The robot can be controlled
by smartphone throughWi-fi connectivity; this capability was
used by the therapist during the intervention phase of our
study";ns;;ns;;ns;;ns;ns;ns;;Healthcare;"To facilitate
communication, Augmentative and Alternative Communication
(AAC) is commonly used in intervention settingss.";assistant;lower;"In this work, we explore
robot-based AAC through humanoid robots that assist therapists
in interventions with nonverbal children.";therapy;therapy;"The study was carried out for approximately 6 months between
March 2013 and August 2013 at the Children’s Center
for Developmental Support";improving communication skills;"Although robots have
been used in different applications, this is one of the first
times that robots have been used for improvement of communication
in nonverbal children.";2,4,5,6;broad;5;broad;Table 1;Autism;Table 1;4;01/Oct;Table 1;Effect with CA on xyz (could have been any CA);"Although robots have
been used in different applications, this is one of the first
times that robots have been used for improvement of communication
in nonverbal children.";robot;"Our initial
results show that robot-based AAC intervention has a positive
impact on the communication skills of nonverbal children.";communication skills;"Our initial
results show that robot-based AAC intervention has a positive
impact on the communication skills of nonverbal children.";;;;ns;;Korean;"their language development was delayed by more than 1
year as indicated by the results of the Korean Preschool receptive
and expressive language test";Other;;
106;2004;article;Human-Computer Interaction;hci;journal;Japan;East Asia;Interactive robots as social partners and peer tutors for children : A field trial;"Robots increasingly have the potential to interact with people in daily life. It is believed that, based on this ability, they will play an essential role in human society in the not-so-distant future. This article examined the proposition that robots could form relationships with children and that children might learn from robots as they learn from other children. In this article, this idea is studied in an 18-day field trial held at a Japanese elementary school. Two English-speaking ""Robovie"" robots interacted with first- and sixth-grade pupils at the perimeter of their respective classrooms. Using wireless identification tags and sensors, these robots identified and interacted with children who came near them. The robots gestured and spoke English with the children, using a vocabulary of about 300 sentences for speaking and 50 words for recognition. The children were given a brief picture-word matching English test at the start of the trial, after 1 week and after 2 weeks. Interactions were counted using the tags, and video and audio were recorded. In the majority of cases, a child's friends were present during the interactions. Interaction with the robot was frequent in the1st week, and then it fell off sharply by the 2nd week. Nonetheless, some children continued to interact with the robot. Interaction time during the 2nd week predicted improvements in English skill at the posttest, controlling for pretest scores. Further analyses indicate that the robots may have been more successful in establishing common ground and influence when the children already had some initial proficiency or interest in English. These results suggest that interactive robots should be designed to have something in common with their users, providing a social as well as technical challenge. © 2004, Lawrence Erlbaum Associates, Inc.";https://www.scopus.com/inward/record.uri?eid=2-s2.0-3142702306&doi=10.1207%2fs15327051hci1901%26amp%3b2_4&partnerID=40&md5=bbb62614d9da7bfe5d84aacacf27e3bf;106.Kanda.pdf;Yes;Robovie;“Robovie” robots;Robot;“Robovie” robots;Fully autonomous;"The
robots gestured and spoke English with the children, using a vocabulary of
about 300 sentences for speaking and 50 words for recognition.";ns;;Yes;"The
robots gestured and spoke English with the children, using a vocabulary of
about 300 sentences for speaking and 50 words for recognition.";Yes;"The
robots gestured and spoke English with the children, using a vocabulary of
about 300 sentences for speaking and 50 words for recognition.";limited commands;limited;50;recognize about 50 words;Education;"Two English-speaking
“Robovie” robots interacted with first- and sixth-grade pupils at the perimeter
of their respective classrooms";peer;equal;"This article examined the proposition that
robots could form relationships with children and that children might learn
from robots as they learn from other children";School;school;"Two English-speaking
“Robovie” robots interacted with first- and sixth-grade pupils at the perimeter
of their respective classrooms";interact socially in English;"This article examined the proposition that
robots could form relationships with children and that children might learn
from robots as they learn from other children";6,7,11,12;broad;7;wide;"This particular
elementary school has three classes for each grade and about 40 students in
each class. There were 119 first-grade students (6–7 years old; 59 boys and 60
girls) and 109 sixth-grade students (11–12 years old; 53 boys and 56 girls).";ns;;228;>100;"This particular
elementary school has three classes for each grade and about 40 students in
each class. There were 119 first-grade students (6–7 years old; 59 boys and 60
girls) and 109 sixth-grade students (11–12 years old; 53 boys and 56 girls).";Effect with CA on xyz (could have been any CA);"the effect of the robots on social interaction over time and
learning";robot;"the effect of the robots on social interaction over time and
learning";English learning, social bond;"the effect of the robots on social interaction over time and
learning";"Further analyses indicate
that the robots may have been more successful in establishing common
ground and influence when the children already had some initial proficiency or
interest in English.";;;ns;;English;The robot could only recognize and speak English;English;"These results suggest that interactive robots should be designed
to have something in common with their users, providing a social as well
as technical challenge.";NO MARKINGS
111;2016;in proceedings;HRI;robot/agent;conference;UK;Europe;Social Robot Tutoring for Child Second Language Learning;An increasing amount of research is being conducted to determine how a robot tutor should behave socially in educational interactions with children. Both human-human and human-robot interaction literature predicts an increase in learning with increased social availability of a tutor, where social availability has verbal and nonverbal components. Prior work has shown that greater availability in the nonverbal behaviour of a robot tutor has a positive impact on child learning. This paper presents a study with 67 children to explore how social aspects of a tutor robot's speech influences their perception of the robot and their language learning in an interaction. Children perceive the difference in social behaviour between `low' and `high' verbal availability conditions, and improve significantly between a pre- and a post-test in both conditions. A longer-term retention test taken the following week showed that the children had retained almost all of the information they had learnt. However, learning was not affected by which of the robot behaviours they had been exposed to. It is suggested that in this short-term interaction context, additional effort in developing social aspects of a robot's verbal behaviour may not return the desired positive impact on learning gains.;https://ieeexplore.ieee.org/abstract/document/7451757;111.Kennedy.pdf;Yes;NAO;"An Aldebaran NAO robot acted as a tutor, delivering all
lessons through speech and moving words on a touchscreen
(Fig. 1).";Robot;"An Aldebaran NAO robot acted as a tutor, delivering all
lessons through speech and moving words on a touchscreen
(Fig. 1).";Wizarded;"To compensate for unreliable speech recognition, a Wizardof-
Oz intervention was used in the HIGH condition to let
the robot reply ‘that’s great’ after the children answered a
question from the robot about their understanding of the
material (children always said they had understood the lesson),
and to trigger pre-scripted phrases at the appropriate time for
the discussion about the child’s hobby.";ns;;ns;;No;"To compensate for unreliable speech recognition, a Wizardof-
Oz intervention was used in the HIGH condition to let
the robot reply ‘that’s great’ after the children answered a
question from the robot about their understanding of the
material (children always said they had understood the lesson),
and to trigger pre-scripted phrases at the appropriate time for
the discussion about the child’s hobby.";ns;ns;ns;;Education;"This paper presents
a study with 67 children to explore how social aspects of a
tutor robot’s speech influences their perception of the robot and
their language learning in an interaction";Tutor;higher;"This paper presents
a study with 67 children to explore how social aspects of a
tutor robot’s speech influences their perception of the robot and
their language learning in an interaction";School;school;"The interactions took place on the school premises in a quiet
working space familiar to the children";improve learning gains;"This paper presents
a study with 67 children to explore how social aspects of a
tutor robot’s speech influences their perception of the robot and
their language learning in an interaction";8,9;07/Sep;2;narrow;"All
children were native English speakers and were from the same
year group (with three class teachers) from a primary school
in the U.K. (average age M=8.8, SD=0.4; 30M, 37F).";ns;;67;61-70;"This paper presents
a study with 67 children to explore how social aspects of a
tutor robot’s speech influences their perception of the robot and
their language learning in an interaction";Effect with CA on xyz (could have been any CA);"This paper presents
a study with 67 children to explore how social aspects of a
tutor robot’s speech influences their perception of the robot and
their language learning in an interaction";verbal availability;"Children perceive the
difference in social behaviour between ‘low’ and ‘high’ verbal
availability conditions, and improve significantly between a preand
a post-test in both conditions. A longer-term retention test
taken the following week showed that the children had retained
almost all of the information they had learnt.";perception of robot, language learning;"This paper presents
a study with 67 children to explore how social aspects of a
tutor robot’s speech influences their perception of the robot and
their language learning in an interaction";;;;ns;;English;"All
children were native English speakers and were from the same
year group (with three class teachers) from a primary school
in the U.K. (average age M=8.8, SD=0.4; 30M, 37F).";English;;
114;2009;conference;RO-MAN;robot/agent;conference;Korea;East Asia;Can robotic emotional expressions induce a human to empathize with a robot?;Can humans empathize with robot that displays emotional expressions? If a human can do this, how is it possible to create an emotionally interactive robot that will induce empathy from a human? As interest in emotional interactions with robots is increasing, the answers to these questions are increasingly sought by researchers in HRI. As a starting point to find these answers, three hypotheses are investigated in this paper. An experiment with ten-year old children was performed to analyze the three hypotheses. The results showed that humans can empathize with a robot when the robot expresses emotions through the use of bruise colors and speech. It was also found that the bruised color expression as used here can induce a human to empathize with a robot as much as speech expression. Additionally, no gender difference was noted when a human empathized with an emotional expressing robot. © 2009 IEEE.;https://www.scopus.com/inward/record.uri?eid=2-s2.0-72849126622&doi=10.1109%2fROMAN.2009.5326282&partnerID=40&md5=c25c1d8af3a7f4dd973680d5d788bf60;114.Kim.pdf;Yes;Mung;"The developed robots, Mung as exhibited in the IAEA 51st
Annual General Conference: bruised robots such as having a blue
eye";Robot;"The developed robots, Mung as exhibited in the IAEA 51st
Annual General Conference: bruised robots such as having a blue
eye";ns;;No;"The
robot was developed for emotional human-robot interactions
with passive characteristics and a delicate interaction concept
using the interaction response: “Bruises due to emotional
stimuli.” The robot has a simple design; it is composed of a
body part and two eye parts. The robot can recognize human
emotions from human-robot or human-human verbal
communications";ns;;Not relevant;;ns;ns;ns;;Generic;"Can humans empathize with robot that displays
emotional expressions? If a human can do this, how is it possible
to create an emotionally interactive robot that will induce
empathy from a human?";Companion;equal;"There are twenty
quizzes and only if both the subject and the robot give the
correct answer is the team correct.";lab;lab;"The subjects were invited to the laboratory that had been
specially prepared for the experiment";emotional, passive human-robot interaction;"The
robot was developed for emotional human-robot interactions
with passive characteristics and a delicate interaction concept
using the interaction response: “Bruises due to emotional
stimuli.” The robot has a simple design; it is composed of a
body part and two eye parts. The robot can recognize human
emotions from human-robot or human-human verbal
communications";10;10/Dec;1;narrow;"Sixteen children comprised of ten females and six males
participated in the experiment. The subjects were all ten years
old and they had never experienced the developed robot.";ns;;16;Nov/20;"Sixteen children comprised of ten females and six males
participated in the experiment. The subjects were all ten years
old and they had never experienced the developed robot.";Effect with CA on xyz (could have been any CA);"emotional empathy with a robot when a
robot expresses its emotional state.";emotion expression;"humans can empathize with a robot when the robot expresses
emotions through the use of bruise colors and spe";empathy;"humans can empathize with a robot when the robot expresses
emotions through the use of bruise colors and spe";;;;ns;;ns;;ns;"ethically dubious!! Plus no sound conclusion: tested 16 children --> ""humans seem to do this and that""";
118;2020;in proceedings;LAK;misc;conference;USA;North America;An Exploratory Approach to Measuring Collaborative Engagement in Child Robot Interaction;This study explored data analytic approaches to assessing young children's engagement in robot-mediated collaborative interaction. To develop our analytic models, we took a case-study approach and looked closely into four children's behaviors during three conversational sessions. Grounded in engagement theory, three sources of multimodal behavioral data (utterances, kinesics, and vocie) were coded through human annotation and automatic speech recognition and analysis. Then, information-theoretic methods were used to uncover nonlinear dependencies (called mutual information) among the multimodal behaviors of each child. From this, we derived a model to compute a compound variable of engagement. This computation produced engagement trends of each child, the engagement relationship between two children in a pair, and the engagement relationship with the robot over time. The computed trends corresponded well with the data from human observations. This approach has implications for quantifying engagement from rich and natural multimodal behaviors.;https://doi.org/10.1145/3375462.3375522;118.Kim.pdf;Yes;Skusie;"we designed the robot Skusie as a new
friend from another planet who liked to learn about life on earth.";Robot;"we designed the robot Skusie as a new
friend from another planet who liked to learn about life on earth.";Wizarded;"For Skusie’s behavior, we adopted a wizard-ofoz
method, where a researcher controlled Skusie remotely hidden
behind the scene.";Yes;"we designed the robot Skusie as a new
friend from another planet who liked to learn about life on earth.";ns;;Wizarded;;free speech;free speech;;"Two directional microphones (AT897) were located on each side
of the children to capture a child’s speech separately. Also, since
the interaction sessions were run in a natural setting, there were
constant noises surrounding the setting while children talked to
each other and with the robot.We placed two ambient microphones
(Shure SM94) at two corners of the walls (one in front and the other
behind the children) to capture the background noises.";Edutainment;"It
asked children to work together to help him learn about animals,
birthdays, school, and family.";companion;equal;"For the collaborative triad, we designed the robot Skusie as a new
friend from another planet";School;school;"These triadic interaction sessions were implemented naturally in
a school library during the regular school hours";mediation;"To what degree does the robot’s mediation relate
to the child’s engagement";5,6;young;2;narrow;"A total of eighty English-speaking children (aged five to six) participated
in the robotic sessions.";ns;;80;71-80;"A total of eighty English-speaking children (aged five to six) participated
in the robotic sessions.";Effect with CA on xyz (could have been any CA);"To what degree does the robot’s mediation relate
to the child’s engagement";mediation;"To what degree does the robot’s mediation relate
to the child’s engagement";engagement;"To what degree does the robot’s mediation relate
to the child’s engagement";;;;ns;;English;"A total of eighty English-speaking children (aged five to six) participated
in the robotic sessions.";English;;
122;2014;in proceedings;HAI;robot/agent;conference;Japan;East Asia;Can a Social Robot Help Children's Understanding of Science in Classrooms?;"This study investigates whether a social robot which interacts with children via quiz-style conversations increases their understanding of science classes. We installed a social robot in an elementary school science classroom where children could freely interact with it during their breaks. The robot asks children questions related to their latest science classes to support their understanding of the classes. During interaction, the robot says children's name and distribute its gaze among the group of children by using a face recognition system and a human tracking system. Still, speech recognition is difficult in the noisy elementary school environment; therefore the operator takes over this function during interactions. In this study our result did not show significant effects of the robot for helping children's understanding, but we found several interesting interaction scenes which shows that robot had a certain effect on specific children.";https://doi.org/10.1145/2658861.2658881;122.Komatsubara.pdf;Yes;Robovie;We used a communication robot, Robovie, which is 120-cm tall with two arms (4*2 DOF) and a head (3 DOF) (Figure 3).;Robot;We used a communication robot, Robovie, which is 120-cm tall with two arms (4*2 DOF) and a head (3 DOF) (Figure 3).;Wizarded;For this study, an operator takes over the speech recognition funFor this study, an operator takes over the speech recognition fun;ns;;No;We used speech synthesis software for utterances [16].;Wizarded;;free speech;free speech;;During breaks, children were allowed to freely interact with Robovie in the free space.;Education;Before the quiz the robot says a topic related to the latest class;teacher;higher;When a question was asked, the operator judged its relevance to science classes.;School;school;Robovie was installed in an elementary school science room (Fig. 4).;Asking and answering science questions;"conversations, such as ""I'll give you a question about the size of the human ovum. What is the size of the human ovum? No.1 1.4 mm, No.2 0.14 mm, or No.3 0.014 mm. Pick an answer."" After explaining choices, the robot tells the correct answer of the quiz and gives a simple explanation about it.
Answering science questions";10,11;10/Dec;2;narrow;In this field trial, we targeted four classes of 114 5th grade students at a public elementary school.;ns;;114;>100;In this field trial, we targeted four classes of 114 5th grade students at a public elementary school.;Effect with CA on xyz (could have been any CA);This study investigates whether a social robot which interacts with children via quiz-style conversations increases their understanding of science classes;interaction;This study investigates whether a social robot which interacts with children via quiz-style conversations increases their understanding of science classes;understanding;This study investigates whether a social robot which interacts with children via quiz-style conversations increases their understanding of science classes;;;;ns;;ns;;ns;;
125;2017;article;Frontiers in Human Neuroscience;misc;journal;USA;North America;Flat vs. Expressive storytelling: Young children’s learning and retention of a social robot’s narrative;"Prior research with preschool children has established that dialogic or active book reading is an effective method for expanding young children’s vocabulary. In this exploratory study, we asked whether similar benefits are observed when a robot engages in dialogic reading with preschoolers. Given the established effectiveness of active reading, we also asked whether this effectiveness was critically dependent on the expressive characteristics of the robot. For approximately half the children, the robot’s active reading was expressive; the robot’s voice included a wide range of intonation and emotion (Expressive). For the remaining children, the robot read and conversed with a flat voice, which sounded similar to a classic text-to-speech engine and had little dynamic range (Flat). The robot’s movements were kept constant across conditions. We performed a verification study using Amazon Mechanical Turk (AMT) to confirm that the Expressive robot was viewed as significantly more expressive, more emotional, and less passive than the Flat robot. We invited 45 preschoolers with an average age of 5 years who were either English Language Learners (ELL), bilingual, or native English speakers to engage in the reading task with the robot. The robot narrated a story from a picture book, using active reading techniques and including a set of target vocabulary words in the narration. Children were post-tested on the vocabulary words and were also asked to retell the story to a puppet. A subset of 34 children performed a second story retelling 4–6 weeks later. Children reported liking and learning from the robot a similar amount in the Expressive and Flat conditions. However, as compared to children in the Flat condition, children in the Expressive condition were more concentrated and engaged as indexed by their facial expressions; they emulated the robot’s story more in their story retells; and they told longer stories during their delayed retelling. Furthermore, children who responded to the robot’s active reading questions were more likely to correctly identify the target vocabulary words in the Expressive condition than in the Flat condition. Taken together, these results suggest that children may benefit more from the expressive robot than from the flat robot. © 2017 Kory Westlund, Jeong, Park, Ronfard, Adhikari, Harris, DeSteno and Breazeal.";https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021669850&doi=10.3389%2ffnhum.2017.00295&partnerID=40&md5=1aa14f446756b19dec86db3d021c0f05;125.KoryWestlund.pdf;Yes;Tega;The Tega robot sat on a table facing the child.;Robot;The Tega robot sat on a table facing the child.;Wizarded;"We used a custom teleoperation interface to control the robot
and the digital storybook";Yes;"We used the Tega robot, a squash and stretch robot designed
for educational activities with young children (Kory Westlund
et al., 2016). The";Yes;"A female adult recorded the robot’s speech. These utterances
were shifted into a higher pitch to make them sound child-like.";Wizarded;;free speech;free speech;;"the events in the story, e.g., ‘‘What is the frog doing?’’, ‘‘Why
did the boy and the dog fall?’’, and ‘‘How do you think the
boy feels now?’’ (11 questions total). The robot responded
to children’s answers with encouraging, but non-committal,
phrases such as ‘‘Mmhm’’, ‘‘Good thought’’ and ‘‘You may
be right’’.
For every other page, the
robot asked a dialogic reading comprehension question about";Edutainment;"The robot then told
the story which consisted of a 22-page subset of the wordless
picture book ‘‘Frog, Where Are you?’’ by Mercer Mayer.";Tutor;higher;"The robot then told
the story which consisted of a 22-page subset of the wordless
picture book ‘‘Frog, Where Are you?’’ by Mercer Mayer.";Lab;lab;"Each child was greeted by an experimenter and led into the study
area.";Reading aloud and asking questions;"For every other page, the
robot asked a dialogic reading comprehension question about
the events in the story, e.g., ‘‘What is the frog doing?’’, ‘‘Why
did the boy and the dog fall?’’, and ‘‘How do you think the
boy feels now?’’ (11 questions total). The robot responded
to children’s answers with encouraging, but non-committal,
phrases such as ‘‘Mmhm’’, ‘‘Good thought’’ and ‘‘You may
be right’’.";4,5,6,7;broad;4;broad;"We recruited 50 children aged 4–7 (23 female, 27 male) from
a Boston-area school (36 children) and the general Boston area
(14 children) to participate in the study. Five children were
removed from the analysis because they did not complete the
study. The children in the final sample included 45 children
(22 female, 23 male; 34 from the school and 11 from the general
Boston area) with a mean age of 5.2 years (SD = 0.77).";ns;;45;41-50;"We recruited 50 children aged 4–7 (23 female, 27 male) from
a Boston-area school (36 children) and the general Boston area
(14 children) to participate in the study. Five children were
removed from the analysis because they did not complete the
study. The children in the final sample included 45 children
(22 female, 23 male; 34 from the school and 11 from the general
Boston area) with a mean age of 5.2 years (SD = 0.77).";Effect with CA on xyz (could have been any CA);"Prior research with preschool children has established that dialogic or active book
reading is an effective method for expanding young children’s vocabulary. In this
exploratory study, we asked whether similar benefits are observed when a robot
engages in dialogic reading with preschoolers. Given the established effectiveness of
active reading, we also asked whether this effectiveness was critically dependent on
the expressive characteristics of the robot";expressiveness;"Prior research with preschool children has established that dialogic or active book
reading is an effective method for expanding young children’s vocabulary. In this
exploratory study, we asked whether similar benefits are observed when a robot
engages in dialogic reading with preschoolers. Given the established effectiveness of
active reading, we also asked whether this effectiveness was critically dependent on
the expressive characteristics of the robot";retention;"Prior research with preschool children has established that dialogic or active book
reading is an effective method for expanding young children’s vocabulary. In this
exploratory study, we asked whether similar benefits are observed when a robot
engages in dialogic reading with preschoolers. Given the established effectiveness of
active reading, we also asked whether this effectiveness was critically dependent on
the expressive characteristics of the robot";;;;ns;;English;"Seventeen
children were ELL, eight were bilingual, 18 were native English
speakers, and three were not reported.";English;;
127;2019;article;Frontiers in Robotics and AI;robot/agent;journal;USA;North America;Exploring the Effects of a Social Robot's Speech Entrainment and Backstory on Young Children's Emotion, Rapport, Relationship, and Learning;In positive human-human relationships, people frequently mirror or mimic each other's behavior. This mimicry, also called entrainment, is associated with rapport and smoother social interaction. Because rapport in learning scenarios has been shown to lead to improved learning outcomes, we examined whether enabling a social robotic learning companion to perform rapport-building behaviors could improve children's learning and engagement during a storytelling activity. We enabled the social robot to perform two specific rapport and relationship-building behaviors: speech entrainment and self-disclosure (shared personal information in the form of a backstory about the robot's poor speech and hearing abilities). We recruited 86 children aged 3–8 years to interact with the robot in a 2 _ 2 between-subjects experimental study testing the effects of robot entrainment Entrainment vs. No entrainment and backstory about abilities Backstory vs. No Backstory. The robot engaged the children one-on-one in conversation, told a story embedded with key vocabulary words, and asked children to retell the story. We measured children's recall of the key words and their emotions during the interaction, examined their story retellings, and asked children questions about their relationship with the robot. We found that the robot's entrainment led children to show more positive emotions and fewer negative emotions. Children who heard the robot's backstory were more likely to accept the robot's poor hearing abilities. Entrainment paired with backstory led children to use more of the key words and match more of the robot's phrases in their story retells. Furthermore, these children were more likely to consider the robot more human-like and were more likely to comply with one of the robot's requests. These results suggest that the robot's speech entrainment and backstory increased children's engagement and enjoyment in the interaction, improved their perception of the relationship, and contributed to children's success at retelling the story. © Copyright © 2019 Kory-Westlund and Breazeal.;https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087524332&doi=10.3389%2ffrobt.2019.00054&partnerID=40&md5=2b924c091577015a224bddd1126b111a;127.KoryWestlund.pdf;Yes;Tega;"We used the Tega robot, a colorful, fluffy squash and stretch robot
designed for interactions with young children";Robot;"We used the Tega robot, a colorful, fluffy squash and stretch robot
designed for interactions with young children";Wizarded;"As in the prior study (Kory Westlund et al., 2017b), we
used custom teleoperation software to control the robot and
digital storybook";Yes;"We used the Tega robot, a colorful, fluffy squash and stretch robot
designed for interactions with young children";Yes;"Speech was recorded by a human adult female and shifted
to a higher pitch to sound more child-like. All robot speech
was sent through the automated audio entrainment module and
streamed to the robot.";Wizarded;;free speech;free speech;;"For each child, the interaction with the robot lasted about
20 min, followed by 5–10min for the posttests. The interaction
script, full interaction procedure, and other study materials
are available for download from figshare at: https://doi.org/10.
6084/m9.figshare.7175273; they are available for download as
SupplementaryMaterials.";Edutainment;"explored whether enabling a social
robot to perform rapport-building behaviors, including speech
and behavior entrainment, and giving the robot an appropriate
backstory regarding its abilities, could help establish rapport
and generate positive interactions with children, which we
hypothesized could improve children’s learning and engagement.";peer;equal;"explored whether enabling a social
robot to perform rapport-building behaviors, including speech
and behavior entrainment, and giving the robot an appropriate
backstory regarding its abilities, could help establish rapport
and generate positive interactions with children, which we
hypothesized could improve children’s learning and engagement.";lab;lab;"Five different experimenters (three female adults and two male
adults) ran the study in pairs in a quiet room in the lab.";rapport building;"explored whether enabling a social
robot to perform rapport-building behaviors, including speech
and behavior entrainment, and giving the robot an appropriate
backstory regarding its abilities, could help establish rapport
and generate positive interactions with children, which we
hypothesized could improve children’s learning and engagement.";3,4,5,6,7,8;broad;6;wide;"The children in the final sample
included 86 children aged 3–8 (44 female, 42 male), with a mean
age of 5.31 years (SD = 1.43).Of these, 3 were 3-year-olds, 30 were
4-year-olds, 19 were 5-year-olds, 15 were 6-year-olds, and 9 were
7-year-olds, and 10 were 8-year-olds. Forty-nine children spoke
English only; 37 children were bilingual.";ns;;86;81-90;"The children in the final sample
included 86 children aged 3–8 (44 female, 42 male), with a mean
age of 5.31 years (SD = 1.43).Of these, 3 were 3-year-olds, 30 were
4-year-olds, 19 were 5-year-olds, 15 were 6-year-olds, and 9 were
7-year-olds, and 10 were 8-year-olds. Forty-nine children spoke
English only; 37 children were bilingual.";Effect with CA on xyz (could have been any CA);"explored whether enabling a social
robot to perform rapport-building behaviors, including speech
and behavior entrainment, and giving the robot an appropriate
backstory regarding its abilities, could help establish rapport
and generate positive interactions with children, which we
hypothesized could improve children’s learning and engagement.";entrainment, backstory;"explored whether enabling a social
robot to perform rapport-building behaviors, including speech
and behavior entrainment, and giving the robot an appropriate
backstory regarding its abilities, could help establish rapport
and generate positive interactions with children, which we
hypothesized could improve children’s learning and engagement.";rapport, learning, engagement;"explored whether enabling a social
robot to perform rapport-building behaviors, including speech
and behavior entrainment, and giving the robot an appropriate
backstory regarding its abilities, could help establish rapport
and generate positive interactions with children, which we
hypothesized could improve children’s learning and engagement.";;;;ns;;English;"The children in the final sample
included 86 children aged 3–8 (44 female, 42 male), with a mean
age of 5.31 years (SD = 1.43).Of these, 3 were 3-year-olds, 30 were
4-year-olds, 19 were 5-year-olds, 15 were 6-year-olds, and 9 were
7-year-olds, and 10 were 8-year-olds. Forty-nine children spoke
English only; 37 children were bilingual.";English;;
139;2015;in proceedings;i-CREATe;hci;conference;Korea;East Asia;Effects of an Intelligent Robot on Number of Words and Length of Sentences Uttered by Children with Autism;In this study, we conducted an in-class experiment using an intelligent robot to find answers to the following research questions: (1) Does the robot affect the number of words spoken by children with autism? (2) Does the robot affect the length of sentences spoken by children with autism? Three children with autism aged 9--11 years participated in the experiment. The teacher used the intelligent robot during normal classroom activities. A single subject with multiple baseline design was used. Results and implications are discussed.;https://dl.acm.org/doi/abs/10.5555/2846712.2846733;139.Lee.pdf;Yes;iRobi Home;"The robot used in this study was
“iRobi Home,” manufactured by
Yujin Robot";Robot;"The robot used in this study was
“iRobi Home,” manufactured by
Yujin Robot";Wizarded;"Through the touch-enabled display,
the robot offers various contents including sing-along, interactive
storybook, family album (using camera), games, and puzzles. The
robot also responds to sounds and touches. One of the features of
the robot is the “home-monitoring” function. Using the
microphone and camera, a teacher or a caregiver can log on to the
robot using a remote computer and access the camera feed. The
adult can also type in words to be recited by the robot. This can be
used to communicate with the child user, giving the robot a reallife
personality.";ns;;ns;;Wizarded;;free speech;free speech;;"greetings with the robot; (2) listening to children’s songs or stories,
or playing games provided by the robot; (3) talking with the robot;
and (4) talking about the robot and about what they felt during the
interaction with the robot. One special education teacher
implemented the intervention to all three children individually.";Education;"intelligent robot to find answers to the following research
questions: (1) Does the robot affect the number of words spoken
by children with autism? (2) Does the robot affect the length of
sentences spoken by children with autism? Three children with
autism aged 9–11 years participated in the experiment. The
teacher used the intelligent robot during normal classroom
activities";peer;equal;"The intervention consisted of four main activities: (1) Saying
greetings with the robot; (2) listening to children’s songs or stories,
or playing games provided by the robot; (3) talking with the robot;
and (4) talking about the robot and about what they felt during the
interaction with the robot.";School;school;"The research
was conducted in a special education classroom.";conversing with children;"intelligent robot to find answers to the following research
questions: (1) Does the robot affect the number of words spoken
by children with autism? (2) Does the robot affect the length of
sentences spoken by children with autism? Three children with
autism aged 9–11 years participated in the experiment. The
teacher used the intelligent robot during normal classroom
activities";9,10,11;10/Dec;3;narrow;"intelligent robot to find answers to the following research
questions: (1) Does the robot affect the number of words spoken
by children with autism? (2) Does the robot affect the length of
sentences spoken by children with autism? Three children with
autism aged 9–11 years participated in the experiment. The
teacher used the intelligent robot during normal classroom
activities";autistic;"intelligent robot to find answers to the following research
questions: (1) Does the robot affect the number of words spoken
by children with autism? (2) Does the robot affect the length of
sentences spoken by children with autism? Three children with
autism aged 9–11 years participated in the experiment. The
teacher used the intelligent robot during normal classroom
activities";3;01/Oct;"intelligent robot to find answers to the following research
questions: (1) Does the robot affect the number of words spoken
by children with autism? (2) Does the robot affect the length of
sentences spoken by children with autism? Three children with
autism aged 9–11 years participated in the experiment. The
teacher used the intelligent robot during normal classroom
activities";Effect of CA on xyz;"intelligent robot to find answers to the following research
questions: (1) Does the robot affect the number of words spoken
by children with autism? (2) Does the robot affect the length of
sentences spoken by children with autism? Three children with
autism aged 9–11 years participated in the experiment. The
teacher used the intelligent robot during normal classroom
activities";conversation;"intelligent robot to find answers to the following research
questions: (1) Does the robot affect the number of words spoken
by children with autism? (2) Does the robot affect the length of
sentences spoken by children with autism? Three children with
autism aged 9–11 years participated in the experiment. The
teacher used the intelligent robot during normal classroom
activities";number of words, sentence length;"intelligent robot to find answers to the following research
questions: (1) Does the robot affect the number of words spoken
by children with autism? (2) Does the robot affect the length of
sentences spoken by children with autism? Three children with
autism aged 9–11 years participated in the experiment. The
teacher used the intelligent robot during normal classroom
activities";;;;ns;;ns;;ns;CA enough? Robot does not really listen, speaks typed out text by adults Education  or healthcare domain?;
141;2019;conference;AAMAS;robot/agent;conference;Netherlands;Europe;A child and a robot getting acquainted - Interaction design for eliciting self-disclosure;In order to facilitate a sustainable long-term interaction between a child and a robot they need to get acquainted with one another. In this paper we discuss the foundation, the rationale, and the evaluation (N = 75) of our design for an autonomous robot conversational partner that engages with Dutch children (8-11 y.o.) in a getting acquainted interaction. The main objective of the robot is to elicit children to self-disclose. Firstly, we discuss five interaction design patterns (IDPs) that proved to be successful in autonomously eliciting and processing self-disclosures. Secondly, we compared two robot behavior profiles. The behavior profiles can be relatively considered as being more and less energetic. We manipulated the movement speed, the speech rate and volume, the use of high/low energy language, waiting time before responding, and the order of high/low energy activities. Results show that the less energetic behavior profile significantly leads to more self-disclosure. © 2019 International Foundation for Autonomous Agents and Multiagent Systems (www.ifaamas.org). All rights reserved.;https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076575306&partnerID=40&md5=eb5a4f2c6d1b877bb416c811d66f747a;141.Ligthart.pdf;Yes;NAO;"A standard Nao robot was used with its default speech recognition
software";Robot;"A standard Nao robot was used with its default speech recognition
software";Fully autonomous;"We evaluated a robot that autonomously engaged with children
in a getting acquainted interaction.";No;;Partially;"The behavior profiles can be relatively considered as being more and
less energetic. We manipulated the movement speed, the speech
rate and volume, the use of high/low energy language, waiting
time before responding, and the order of high/low energy activities.
Results show that the less energetic behavior profile significantly
leads to more self-disclosure.";No;"If speech recognition
fails, the touch modality proved to be an effective back-up.";free speech;free speech;;"Open-ended questions have no valid
answer, i.e. accept all answers.";Generic;"We used an 2x2 between-subject study design for RQ2. The two
independent variables are the extraversion of the child (introvert
versus extrovert) and the behavior adaptation of the robot (more
or less energetic). The two dependent variables are the amount
and the intimacy of self-disclosure. The interaction design patterns
were implemented across all conditions. All the interactions that
were included in the experiment were used to evaluate the IDPs
(RQ1).";peer;equal;"an autonomous robot conversational
partner that engages with Dutch children (8-11 y.o.) in a getting
acquainted interaction. The main objective of the robot is to elicit
children to self-disclose";school;school;"(an arts and crafts room in school A and a surplus classroom in
school B) where the participants interacted with the robot. The
second, the interview room, was a small workroom mostly used, in
both schools, for homework guidance or quiet working.";talking;"an autonomous robot conversational
partner that engages with Dutch children (8-11 y.o.) in a getting
acquainted interaction. The main objective of the robot is to elicit
children to self-disclose";8,9,10,11;broad;4;broad;"In
this paper we discuss the foundation, the rationale, and the evaluation
(N = 75) of our design for an autonomous robot conversational
partner that engages with Dutch children (8-11 y.o.) in a getting
acquainted interaction.";ns;;75;71-80;"In
this paper we discuss the foundation, the rationale, and the evaluation
(N = 75) of our design for an autonomous robot conversational
partner that engages with Dutch children (8-11 y.o.) in a getting
acquainted interaction.";Effect with CA on xyz (could have been any CA);"We evaluated a robot that autonomously engaged with children
in a getting acquainted interaction. We designed five structured
dyadic interaction design patterns. Results show that the design
patterns allow the robot to effectively elicit and process children’s
self-disclosures by asking a combination of closed-ended, openended,
and pseudo-open-ended questions.";extraversion child, energy level robot;"We used an 2x2 between-subject study design for RQ2. The two
independent variables are the extraversion of the child (introvert
versus extrovert) and the behavior adaptation of the robot (more
or less energetic). The two dependent variables are the amount
and the intimacy of self-disclosure. The interaction design patterns
were implemented across all conditions. All the interactions that
were included in the experiment were used to evaluate the IDPs
(RQ1).";amount and intimacy self-disclosure;"We used an 2x2 between-subject study design for RQ2. The two
independent variables are the extraversion of the child (introvert
versus extrovert) and the behavior adaptation of the robot (more
or less energetic). The two dependent variables are the amount
and the intimacy of self-disclosure. The interaction design patterns
were implemented across all conditions. All the interactions that
were included in the experiment were used to evaluate the IDPs
(RQ1).";;;;ns;;Dutch;"In
this paper we discuss the foundation, the rationale, and the evaluation
(N = 75) of our design for an autonomous robot conversational
partner that engages with Dutch children (8-11 y.o.) in a getting
acquainted interaction.";Other;;
144;2020;in proceedings;HRI;robot/agent;conference;Netherlands;Europe;Design Patterns for an Interactive Storytelling Robot to Support Children's Engagement and Agency;In this paper we specify and validate three interaction design patterns for an interactive storytelling experience with an autonomous social robot. The patterns enable the child to make decisions about the story by talking with the robot, reenact parts of the story together with the robot, and recording self-made sound effects. The design patterns successfully support children's engagement and agency. A user study (N = 27, 8-10 y.o.) showed that children paid more attention to the robot, enjoyed the storytelling experience more, and could recall more about the story, when the design patterns were employed by the robot during storytelling. All three aspects are important features of engagement. Children felt more autonomous during storytelling with the design patterns and highly appreciated that the design patterns allowed them to express themselves more freely. Both aspects are important features of children's agency. Important lessons we have learned are that reducing points of confusion and giving the children more time to make themselves heard by the robot will improve the patterns efficiency to support engagement and agency. Allowing children to pick and choose from a diverse set of stories and interaction settings would make the storytelling experience more inclusive for a broader range of children.;https://doi.org/10.1145/3319502.3374826;144.Ligthart.pdf;Yes;Nao;"A V6 grey-white Nao robot (see figure 3) was used. Google’s Dialogflow
was used for speech recognition";Robot;"A V6 grey-white Nao robot (see figure 3) was used. Google’s Dialogflow
was used for speech recognition";Fully autonomous;"In this paper we specify and validate three interaction design patterns
for an interactive storytelling experience with an autonomous
social robot.";No;;ns;;No;"A V6 grey-white Nao robot (see figure 3) was used. Google’s Dialogflow
was used for speech recognition";ns;ns;ns;;Entertainment;"A user study (N = 27, 8-10 y.o.) showed that children paid more
attention to the robot, enjoyed the storytelling experience more, and
could recall more about the story, when the design patterns were
employed by the robot during storytelling. All three aspects are
important features of engagement";peer;equal;"A user study (N = 27, 8-10 y.o.) showed that children paid more
attention to the robot, enjoyed the storytelling experience more, and
could recall more about the story, when the design patterns were
employed by the robot during storytelling. All three aspects are
important features of engagement";lab;lab;Participants came to the experiment room one after the other.;enact stories with children;"A user study (N = 27, 8-10 y.o.) showed that children paid more
attention to the robot, enjoyed the storytelling experience more, and
could recall more about the story, when the design patterns were
employed by the robot during storytelling. All three aspects are
important features of engagement";8,9,10;07/Sep;3;narrow;"A user study (N = 27, 8-10 y.o.) showed that children paid more
attention to the robot, enjoyed the storytelling experience more, and
could recall more about the story, when the design patterns were
employed by the robot during storytelling. All three aspects are
important features of engagement";ns;;27;21-30;"A user study (N = 27, 8-10 y.o.) showed that children paid more
attention to the robot, enjoyed the storytelling experience more, and
could recall more about the story, when the design patterns were
employed by the robot during storytelling. All three aspects are
important features of engagement";Effect with CA on xyz (could have been any CA);"We have specified, and successfully validated, three new interaction
design patterns that support children’s engagement and agency
during an interactive storytelling experience with a social robo";interactive design patterns;"This study had a within-subject design. Participants were exposed
to two different stories. One with all the design patterns (interactive)
and one without (plain). Which of the stories contained the patterns,
and whether the interactive story was first or second, were both
counter balanced.";engagement, agency, narrative transportation, efficiency, satisfaction;"As dependent variables engagement, agency, and narrative trans
portation
were measured during both stories. We furthermore mea
sured
the interaction design pattern efficiency and satisfaction and
story satisfaction (see section 4.5).";;;;ns;;Dutch;"We translated the questions to Dutch,
simplified the language to match the vocabulary of the participants,
and adapted it to the context.";Other;;
145;2011;in proceedings;SLPAT;speech;conference;Sweden;Europe;Lekbot: A Talking and Playing Robot for Children with Disabilities;This paper describes an ongoing project where we develop and evaluate a setup involving a communication board and a toy robot, which can communicate with each other via synthesised speech. The purpose is to provide children with communicative disabilities with a toy that is fun and easy to use together with peers, with and without disabilities. When the child selects a symbol on the communication board, the board speaks and the robot responds. This encourages the child to use language and learn to cooperate to reach a common goal. Throughout the project, three children with cerebral palsy and their peers use the robot and provide feedback for further development. The multimodal interaction with the robot is video recorded and analysed together with observational data in activity diaries.;https://www.aclweb.org/anthology/W11-2312.pdf;145.Ljunglöf.pdf;Yes;Lekbot;"The Lekbot robot is also a
physical agent, acting in the world, thus adding
another dimension to the interaction.";Robot;"The Lekbot robot is also a
physical agent, acting in the world, thus adding
another dimension to the interaction.";Wizarded;"Note that it is the communication board com
puter
that controls the robot via the dialogue
system, but the intention is that it should seem
like the robot is autonomous";Yes;"The child has a communication board that can
talk; when the child points at one of the symbols
it is translated to an utterance which the board
expresses via speech synthesis in Swedish. This
is recognised by a robot that moves around in the
room, and performs the commands that the child
expresses through the board. The robot has an
incarnation as a toy animal, currently a bumble
bee.
It has a very basic personality which means
that it can take the initiative, without the child
telling it, refuse actions, or even negotiate with
the child.";Yes;"The Lekbot system uses two different voices,
one for the touch screen, acting as the child’s
voice, and one for the robot. Whereas the touch
screen
voice is a vocalisation of something the
child has already seen on the screen, the utter
ances
of the robot have no visualisations. Hence,
it is particularly important that the robot’s ut
terances
are as clear as possible, and the TTS
voice chosen for the robot is therefore the voice
that was determined to have the best and most
flexible intonation in informal perception tests
at the start of the project.";Wizarded;"An advantage of the Lekbot setup is that we will, in a
sense, have “perfect speech recognition”, since
we are cheating a bit. The robot does not ac
tually
have to listen for the speech generated by
the communication board; since the information
is already electronically encoded, it can instead
be transferred wirelessly. This means that the
robot will never hear “go forward and then stop”
when the communication board actually says “go
forward seven steps”.";ns;ns;ns;;Healthcare;"The purpose
is to provide children with communicative
disabilities with a toy that is fun and easy
to use together with peers, with and with
out
disabilities.";toy;lower;"communication board and a toy
robot, which can communicate with each
other via synthesised speech";preschool;school;"target children with peers and staff, at three
different pre-schools, was recruited.";communicative aid;"The purpose
is to provide children with communicative
disabilities with a toy that is fun and easy
to use together with peers, with and with
out
disabilities.";4,5,6;young;3;n;"The 6 year old Per shows a level of 3: Ef
fective
sender and effective receiver with fa
miliar
partners.
The 5 year old Hans is estimated to level

5: Seldom effective sender and effective re
ceiver
with familiar partners, and
The 4 year old Greta is at level 4: Incon

sistent
sender and/or receiver with familiar
partners.";TD & cerebral palsy;"They have cere
bral
palsy with complex communication needs.";6;01/Oct;"Along with the target children, three typically
developed peers, of the same age, or slightly
younger, were recruited at each pre-school";Evaluating the CA;"This paper describes an ongoing project
where we develop and evaluate a setup in
volving
a communication board and a toy
robot";Robot;"This paper describes an ongoing project
where we develop and evaluate a setup in
volving
a communication board and a toy
robot";Enjoyment;"All target children have enjoyed the Lekbot play
from the beginning. The more commands and
abilities the robot has received the more appre
ciated
has the play become also by the peers";;;;ns;;Swedish;"the dialogue system GoDiS (Larsson, 2002),
using Acapela Multimedia text-to-speech
with Swedish voices";Other;cicrumvented ASR;
147;2016;article;Int. J. of Social Robotics;robot/agent;journal;Netherlands;Europe;Integrating Robot Support Functions into Varied Activities at Returning Hospital Visits: Supporting Child’s Self-Management of Diabetes;Persistent progress in the self-management of their disease is important and challenging for children with diabetes. The European ALIZ-e project developed and tested a set of core functions for a social robot that may help to establish such progress. These functions were studied in different set-ups and with different groups of children (e.g. classmates at a school, or participants of a diabetes camp). This paper takes the lessons learned from these studies to design a general scenario for educational and enjoying child–robot activities during returning hospital visits. The resulting scenario entailed three sessions, each lasting almost one hour, with three educational child–robot activities (quiz, sorting game and video watching), two intervening child–robot interactions (small talk and walking), and specific tests to assess the children and their experiences. Seventeen children (age 6–10) participated in the evaluation of this scenario, which provided new insights of the combined social robot support in the real environment. Overall, the children, but also their parents and formal caregivers, showed positive experiences. Children enjoyed the variety of activities, built a relationship with the robot and had a small knowledge gain. Parents and hospital staff pointed out that the robot had positive effects on child’s mood and openness, which may be helpful for self-management. Based on the evaluation results, we derived five user profiles for further personalization of the robot, and general requirements for mediating the support of parents and caregivers. © 2016, The Author(s).;https://www.scopus.com/inward/record.uri?eid=2-s2.0-84985995803&doi=10.1007%2fs12369-016-0365-8&partnerID=40&md5=ffbb5c173ae76c47d50939d3574d008f;147.Looije.pdf;Yes;Nao;"In order to get a feeling of how diabetic children interacted
with the NAO when different activities are offered and physical
interaction is possible we carried out an experiment";Robot;"In order to get a feeling of how diabetic children interacted
with the NAO when different activities are offered and physical
interaction is possible we carried out an experiment";Wizarded;Wizard Laptop;No;;ns;;Wizarded;;ns;ns;ns;;Healthcare;"This evaluation showed that parents, medical staff and children
enjoyed working with the robot and saw advantages of
the use.";companion;equal;"The resulting scenario
entailed three sessions, each lasting almost one hour,
with three educational child–robot activities (quiz, sorting
gameand videowatching), two intervening child–robot interactions
(small talk and walking), and specific tests to assess
the children and their experiences.";hospital;public space;Every child had three sessions of about an hour in the hospital.;help with self management of  disease;"Persistent progress in the self-management of
their disease is important and challenging for children with
diabetes. The European ALIZ-e project developed and tested
a set of core functions for a social robot that may help to establish
such progress.";6,7,8,9,10;broad;5;broad;17 diabetic children in the age of 6–10 (M=8.24,SD=1.25);diabetes;17 diabetic children in the age of 6–10 (M=8.24,SD=1.25);17;Nov/20;17 diabetic children in the age of 6–10 (M=8.24,SD=1.25);Effect with CA on xyz (could have been any CA);"Overall, the general scenario for educational and enjoying
child–robot activities during returning hospital visits, proved
to capture the lessons learned well.";chri activities;"a general scenario for educational and enjoying child–robot
activities during returning hospital visits. The resulting scenario
entailed three sessions, each lasting almost one hour,
with three educational child–robot activities (quiz, sorting
gameand videowatching), two intervening child–robot interactions
(small talk and walking), and specific tests to assess
the children and their experiences. Seventeen children (age
6–10) participated in the evaluation of this scenario, which
provided new insights of the combined social robot support
in the real environment. Overall, the children, but also their
parents and formal caregivers, showed positive experiences.
Children enjoyed the variety of activities, built a relationship
B Rosemarijn Looije
rosemarijn.looije@tno.nl
Mark A. Neerincx
mark.neerincx@tno.nl
Johanna K. Peters
johannapeters.jkp@gmail.com
Olivier A. Blanson Henkemans
olivier.blansonhenkemans@tno.nl
1 TNO, Postbus 23, 3769 ZG Soesterberg, The Netherlands
2 Interactive Intelligence Group, Delft University of
Technology, Mekelweg 4, 2628 CD Delft, The Netherlands
3 Department of Artificial Intelligence, University of
Groningen, Postbus 407, 9700 AK Groningen,
The Netherlands
with the robot and had a small knowledge gain";enjoyment, relationship, knowledge gain;"a general scenario for educational and enjoying child–robot
activities during returning hospital visits. The resulting scenario
entailed three sessions, each lasting almost one hour,
with three educational child–robot activities (quiz, sorting
gameand videowatching), two intervening child–robot interactions
(small talk and walking), and specific tests to assess
the children and their experiences. Seventeen children (age
6–10) participated in the evaluation of this scenario, which
provided new insights of the combined social robot support
in the real environment. Overall, the children, but also their
parents and formal caregivers, showed positive experiences.
Children enjoyed the variety of activities, built a relationship
B Rosemarijn Looije
rosemarijn.looije@tno.nl
Mark A. Neerincx
mark.neerincx@tno.nl
Johanna K. Peters
johannapeters.jkp@gmail.com
Olivier A. Blanson Henkemans
olivier.blansonhenkemans@tno.nl
1 TNO, Postbus 23, 3769 ZG Soesterberg, The Netherlands
2 Interactive Intelligence Group, Delft University of
Technology, Mekelweg 4, 2628 CD Delft, The Netherlands
3 Department of Artificial Intelligence, University of
Groningen, Postbus 407, 9700 AK Groningen,
The Netherlands
with the robot and had a small knowledge gain";;;;ns;;ns;;ns;;
163;2020;in proceedings;UbiComp-ISWC;hci;conference;USA;North America;Why Doesn't the Conversational Agent Understand Me? A Language Analysis of Children Speech;"Conversation agents have shifted the way we communicate with ubiquitous services by enabling the use of natural language communication and the analysis of acoustic and linguistic language patterns. Speech skills of children are not yet fully developed; therefore, most conversational agents frequently misunderstand them. In this research, we examined if conversational agents can uncover instances of language discrimination in children. We developed Bolita, a conversational agent using a Google Home and a Sphero Robot to encourage children to practice how to tell a joke. The results of a two week study of the use of Bolita by 37 Mexican children showed a conversational agent is more likely to misunderstand children with speech skills below average. Our results indicate that they speak less, use fewer words, and need more time to answer when interacting with a conversational agent, which may explain the challenges with the conversational agent understanding them. We close discussing the potential of conversational agents to uncover digital markers in children with language differences and suggest ways that conversational agents could be built to be more inclusive.";https://doi.org/10.1145/3410530.3414401;163.Monarca.pdf;Yes;Bolita;"We developed Bolita,
a conversational agent using a Google Home and a Sphero Robot
to encourage children to practice how to tell a joke";Robot;"We developed Bolita,
a conversational agent using a Google Home and a Sphero Robot
to encourage children to practice how to tell a joke";Wizarded;"We used Wizard
of Oz to synchronize the animation of the Sphero and the voice
commands of the conversational interface.";Yes;"Bolita runs in a smartphone
with Google Assistant1 and uses the Sphero Bolt Robot2 to provide
visual and auditory feedback as a reward to children.";ns;;Wizarded;;ns;ns;ns;;Edutainment;"Bolita (i.e., ""little ball"" in Spanish), is a bilingual conversational agent
that teaches children how to tell jokes (Figure 1).";tutor;higher;"Bolita (i.e., ""little ball"" in Spanish), is a bilingual conversational agent
that teaches children how to tell jokes (Figure 1).";ns;ns;;facilitate children's speech analysis;"The work described here contributes to empirical evidence showing
the potential of conversational agents in facilitating the acoustic and
linguistic analysis of children’s speech.";8,9,10,11;broad;4;broad;"We enrolled 37 young children (18 boys/19 girls) between 8 and 11
years old (9.76 ± 1.10).";ns;;37;31-40;"We enrolled 37 young children (18 boys/19 girls) between 8 and 11
years old (9.76 ± 1.10).";Evaluating the CA;"At the end of the two sessions with Bolita, children
answered a questionnaire about how much they liked talking with
Bolita. The questionnaire includes 4 questions: (1) How much did
you like talking to Bolita? (2) How much fun was it talking to
Bolita? (3) How much would you like to talk to Bolita again? (4)
How much would you like to have Bolita at home?";robot;"At the end of the two sessions with Bolita, children
answered a questionnaire about how much they liked talking with
Bolita. The questionnaire includes 4 questions: (1) How much did
you like talking to Bolita? (2) How much fun was it talking to
Bolita? (3) How much would you like to talk to Bolita again? (4)
How much would you like to have Bolita at home?";enjoyment;"At the end of the two sessions with Bolita, children
answered a questionnaire about how much they liked talking with
Bolita. The questionnaire includes 4 questions: (1) How much did
you like talking to Bolita? (2) How much fun was it talking to
Bolita? (3) How much would you like to talk to Bolita again? (4)
How much would you like to have Bolita at home?";;;;ns;;Spanish;"All participants
speak Spanish.";Other;;
167;2011;conference;Interspeech;speech;conference;USA;North America;Analyzing the nature of ECA interactions in children with autism;Embodied conversational agents (ECA) offer platforms for the collection of structured interaction and communication data. This paper discusses the data collected from the Rachel system, an ECA developed at the University of Southern California, for interactions with children with autism. Two dyads each composed of a child with autism and his parent participated in an experiment with two modes: interactions with and without the ECA present. The goal of this work is to assess the naturalness of the data recorded in the ECA interaction. This analysis was carried out using a classification framework with a prediction variable of the presence or absence of the ECA in the interaction. The results demonstrate that it is possible to estimate whether or not a parent is interacting with the ECA using their speech data. However, it is not generally possible to do so for the child suggesting that the Rachel system is eliciting communication data that is similar to that elicited through interactions between the child and his parent. Copyright © 2011 ISCA.;https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865721912&partnerID=40&md5=397d6a82efc6fca580740bd55db893bf;167.Mower.pdf;Yes;Rachel;"This paper discusses the data collected from the Rachel system,
an ECA developed at the University of Southern California, for
interactions with children with autism.";Virtual Agent;"This paper discusses the data collected from the Rachel system,
an ECA developed at the University of Southern California, for
interactions with children with autism.";Wizarded;"Each session was recorded using a suite of audio-visual sensors.
The behavior of the ECA was logged for post-hoc analysis.
The ECA was controlled using the Wizard of Oz (WoZ)
paradigm in which a hidden experimenter controls the output of
the ECA. Additional information can be found in [10].";Yes;"The Rachel system is an ECA environment created at the
University of Southern California to collect communication
data from children with autism and their parents";ns;;Wizarded;;ns;ns;ns;;Healthcare;"activities designed to assess the children’s
emotional reasoning ability";parent;higher;"The parent plays the same role
as Rachel in the story telling task, initiating the story telling.";lab;lab;"Each session follows the same protocol:
1) the child and parent enter the experimental room";elicit communicative behaviour;"The use of an ECA to collect social
communicative behavior assumes that the elicited behavior
is representative of the child’s communication abilities.";6,12;broad;7;wide;"The subjects met the inclusion criteria: of
a diagnosis of autism using the Autism Diagnostic Observation
Scale (ADOS), that the child and the parent both speak English,
that the child is from 5-13 years of age, and that the child has received
a score on the Expressive Communication subtest of the
Vineland Adaptive Behavior scales of at least an age equivalent
of 2 years, 6 months.";autistic;"The subjects met the inclusion criteria: of
a diagnosis of autism using the Autism Diagnostic Observation
Scale (ADOS), that the child and the parent both speak English,
that the child is from 5-13 years of age, and that the child has received
a score on the Expressive Communication subtest of the
Vineland Adaptive Behavior scales of at least an age equivalent
of 2 years, 6 months.";2;01/Oct;"The first child was a 12-year-old boy with
an expressive language score of 6 years, 7 months (Vineland).
He was accompanied by his mother during the first session and
his father in the remaining sessions. His younger brother also
attended the final session. The interactions between this child,
his parent, and Rachel will be referred to as the “subject one
experiments.” The second child was a 6-year-old boy with an
expressive language score of 2 years, 9 months (Vineland). He
was accompanied by his mother in all four sessions.";Effect with CA on xyz (could have been any CA);"The purpose of the studies presented in this paper is to understand
how the data collected from the Rachel-moderated interactions
differ from that of the parent-moderated interactions
(without Rachel).";interaction with or without CA;"interactions with and without the
ECA present.";Naturalness;"The goal of this work is to assess the naturalness
of the data recorded in the ECA interaction";;;;ns;;English;"The subjects met the inclusion criteria: of
a diagnosis of autism using the Autism Diagnostic Observation
Scale (ADOS), that the child and the parent both speak English,
that the child is from 5-13 years of age, and that the child has re
ceived
a score on the Expressive Communication subtest of the
Vineland Adaptive Behavior scales of at least an age equivalent
of 2 years, 6 months.";English;;
170;2012;conference;RO-MAN;robot/agent;conference;Italy, Germany, UK;Europe;Children's adaptation in multi-session interaction with a humanoid robot;This work presents preliminary observations from a study of children (N=19, age 5-12) interacting in multiple sessions with a humanoid robot in a scenario involving game activities. The main purpose of the study was to see how their perception of the robot, their engagement, and their enjoyment of the robot as a companion evolve across multiple interactions, separated by one-two weeks. However, an interesting phenomenon was observed during the experiment: most of the children soon adapted to the behaviors of the robot, in terms of speech timing, speed and tone, verbal input formulation, nodding, gestures, etc. We describe the experimental setup and the system, and our observations and preliminary analysis results, which open interesting questions for further research. © 2012 IEEE.;https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870799283&doi=10.1109%2fROMAN.2012.6343778&partnerID=40&md5=c621e6b23a06ec83138c36e8fb3ebbdd;170.Nalin.pdf;Yes;NAO;"The robot selected for this study was Nao, a humanoid
robot from Aldebaran Robotics5";Robot;"The robot selected for this study was Nao, a humanoid
robot from Aldebaran Robotics5";Wizarded;"The system, described more
fully in section IV, produces verbal and nonverbal output
automatically, but requires the intervention of a human Wizard
to interpret the child’s speech.";No;"The Nao is approximately
58cm tall, weighs 5.2kg and has a cartoon-like appearance
which was considered especially suitable for use with
children";No;Acapela and MARY for speech synthesis,;Wizarded;;free speech;free speech;;"The system, described more
fully in section IV, produces verbal and nonverbal output
automatically, but requires the intervention of a human Wiz
ard
to interpret the child’s speech.";Entertainment;"in multiple
sessions with a humanoid robot in a scenario involving game
activities.";companion;equal;"The main purpose of the study was to see how their
perception of the robot, their engagement, and their enjoyment
of the robot as a companion evolve across multiple interac
tions,
separated by one–two weeks";hospital;public space;"The tests were
run at San Raffaele Hospital in Milan from March to May
2012.";play;"evaluate how longterm
interaction affects the behavior of children playing with
a humanoid robot, and how they perceive it, an experiment
with multiple game sessions was organized. The purpose
of the experiment was to study how children changed their
opinion of the robot and their way of interacting with it, once
novelty effects disappear.";5,6,7,8,9,10,11,12;broad;8;wide;"19 children (11 male, 8 female) were recruited for this
study, ranging in age from 5 to 12 years old. However
only 13 of them were actually able to participate in all
three interactions specified by the protocol.";diabetes;"The invitation to the experiments was sent through a
diabetic patients association, however most of children were
healthy, and received the invitation through diabetic relatives
or classmates/friends.";13;Nov/20;"19 children (11 male, 8 female) were recruited for this
study, ranging in age from 5 to 12 years old. However
only 13 of them were actually able to participate in all
three interactions specified by the protocol.";Effect with CA on xyz (could have been any CA);"The main purpose of the study was to see how their
perception of the robot, their engagement, and their enjoyment
of the robot as a companion evolve across multiple interactions,
separated by one–two weeks";time;"The main purpose of the study was to see how their
perception of the robot, their engagement, and their enjoyment
of the robot as a companion evolve across multiple interac
tions,
separated by one–two weeks";perception, enjoyment, engagement;"The main purpose of the study was to see how their
perception of the robot, their engagement, and their enjoyment
of the robot as a companion evolve across multiple interac
tions,
separated by one–two weeks";;;;ns;;Italian;Automatic production of verbal output in Italian;Other;;
171;2002;article;Transactions on Speech and Audio Processing;speech;journal;USA;North America;Creating conversational interfaces for children;Creating conversational interfaces for children is challenging in several respects. These include acoustic modeling for automatic speech recognition (ASR), language and dialog modeling, and multimodal-multimedia user interface design. First, issues in ASR of children speech are introduced by an analysis of developmental changes in the spectral and temporal characteristics of the speech signal using data obtained from 456 children, ages five to 18 years. Acoustic modeling adaptation and vocal tract normalization algorithms that yielded state-of-the-art ASR performance on children speech are described. Second, an experiment designed to better understand how children interact with machines using spoken language is described. Realistic conversational multimedia interaction data were obtained from 160 children who played a voice-activated computer game in a Wizard of Oz (WoZ) scenario. Results of using these data in developing novel language and dialog models as well as in a unified maximum likelihood framework for acoustic decoding in ASR and semantic classification for spoken language understanding are described. Leveraging the lessons learned from the WoZ study and a con-current user experience evaluation, a multimedia personal agent prototype for children was designed. Details of the architecture and application details are described. Informal evaluation by children was found positive especially for the animated agent and the speech interface.;https://www.scopus.com/inward/record.uri?eid=2-s2.0-0036475971&doi=10.1109%2f89.985544&partnerID=40&md5=11d98bfcd073bfe9ecd1432d189642bf;171.Narayanan.pdf;Yes;WITUICS;"popular computer
game “Where in the U.S.A. is Carmen Sandiego?” (WITUICS)
by Brøderbund Software. WITUICS is an interactive detective
game for children ages eight years and older";Virtual Agent;"popular computer
game “Where in the U.S.A. is Carmen Sandiego?” (WITUICS)
by Brøderbund Software. WITUICS is an interactive detective
game for children ages eight years and older";Wizarded;"To investigate how children converse with interactive systems
and to collect speech data, dialog interaction and user experience
data in a realistic spoken language application environment,
a Wizard of Oz (WoZ) experiment was designed.";Yes;"popular computer
game “Where in the U.S.A. is Carmen Sandiego?” (WITUICS)
by Brøderbund Software. WITUICS is an interactive detective
game for children ages eight years and older";Yes;"popular computer
game “Where in the U.S.A. is Carmen Sandiego?” (WITUICS)
by Brøderbund Software. WITUICS is an interactive detective
game for children ages eight years and older";Wizarded;;free speech;free speech;;"Overall, the game was rich in dialog subtasks including navigation
and multiple queries, database entry, and database search.
Further, the fact that (during a substantial part of the game) the
child conversed with cartoon characters on the screen made the
dialog more natural and human-like. As a result spontaneous
speech could be elicited.1";Entertainment;"popular computer
game “Where in the U.S.A. is Carmen Sandiego?” (WITUICS)
by Brøderbund Software. WITUICS is an interactive detective
game for children ages eight years and older";characters;unclear;"The player could
talk to various characters appearing on the game screen seeking
clues about the suspect’s trail and physical appearance.";lab;lab;"The player sat in front of a slave monitor wearing
headphones, i.e., watching and listening to the audio-visual
output piped from the wizard’s computer. In the observation
room, the wizard controlled the experiment by providing the
appropriate output in response to the user’s input.";give clues;"The player could
talk to various characters appearing on the game screen seeking
clues about the suspect’s trail and physical appearance.";8,9,10,11,12,13,14;broad;7;wide;"About 160 children, ages eight to 14 years, participated in
the study by playing an interactive computer game using voice
commands, or keyboard and mouse control [24].";ns;;160;>100;"About 160 children, ages eight to 14 years, participated in
the study by playing an interactive computer game using voice
commands, or keyboard and mouse control [24].";Effect with CA on xyz (could have been any CA);"To investigate how children converse with interactive systems
and to collect speech data, dialog interaction and user experience
data in a realistic spoken language application environment,
a Wizard of Oz (WoZ) experiment was designed.";interactive system;"To investigate how children converse with interactive systems
and to collect speech data, dialog interaction and user experience
data in a realistic spoken language application environment,
a Wizard of Oz (WoZ) experiment was designed.";user experience;"To investigate how children converse with interactive systems
and to collect speech data, dialog interaction and user experience
data in a realistic spoken language application environment,
a Wizard of Oz (WoZ) experiment was designed.";;;;ns;;ns;;ns;;
176;2015;in proceedings;HRI;robot/agent;conference;Japan;East Asia;Why Do Children Abuse Robots?;We found that children sometimes abuse a social robot in a hallway of a shopping mall. They spoke bad words, repeatedly obstructed the robot's path, and sometimes even kicked and punched the robot. To investigate why they abused it, we conducted a field study, in which we let visiting children freely interact with the robot, and interviewed when they engaged in a serious abusive behavior including physical contacts. In total, we obtained valid interviews from twenty-three children over 13 days of observations. They are aged between five and nine. Adults and older children were rarely involved. We interviewed them to know whether they perceived the robot as human-like others, why they abused it, and whether they thought that the robot would suffer from their abusive behavior. We found that 1) the majority of the children abused because they were curious about the robot's reactions or enjoyed abusing it while considering it as human-like, and 2) about half of the children believed in the capability of the robot to perceive their abusive behaviors.;https://doi.org/10.1145/2701973.2701977;176.Nomura.pdf;Yes;ns;The study was conducted in a shopping mall in Japan, using a human-sized humanoid robot.;Robot;The study was conducted in a shopping mall in Japan, using a human-sized humanoid robot.;ns;;ns;;ns;;ns;;free speech;free speech;;"Figure 1: Children abusing a robot
When children started to interact with the robot, the interviewer
who observed the interaction judged whether they conducted
serious abusive behaviors toward the robot. If the behaviors were";Other;"When children started to interact with the robot, the interviewer who observed the interaction judged whether they conducted serious abusive behaviors toward the robot. If the behaviors were
regarded as such serious abusive, after the children finished interaction with the robot, the interviewer identified the parents of the children and asked for the parents to allow the conduction of interview for the children.";peer;equal;"When children started to interact with the robot, the interviewer who observed the interaction judged whether they conducted serious abusive behaviors toward the robot. If the behaviors were
regarded as such serious abusive, after the children finished interaction with the robot, the interviewer identified the parents of the children and asked for the parents to allow the conduction of interview for the children.";shopping mall;public space;The study was conducted in a shopping mall in Japan, using a human-sized humanoid robot.;respond to negative behavior;"When
children’s
aggressive
behaviors escalate, we observed that they engage in aggressive
actions with physical contacts with the robot. For them, we made
the robot to provide explicit negative reactions that the robot
recognizes their actions and want the children to stop such actions,
imitating what a person would say in such a situation.";5,6,7,8,9;broad;5;broad;"A total of twenty-eight children were interviewed (male: 20, female: 3; 5 years old: 3, 6 years old: 3, 7 years old: 6, 8 years old; 6, 9 years old: 5).";ns;;28;21-30;"A total of twenty-eight children were interviewed (male: 20, female: 3; 5 years old: 3, 6 years old: 3, 7 years old: 6, 8 years old; 6, 9 years old: 5).";Effect with CA on xyz (could have been any CA);We wonder whether these causes may or may not be applied to abusive behaviors against robots. Here, the question is whether children perceive robots as a kind of human-like (or at least living) entity or not;humanlikeness;From this finding, we speculate that, although one might consider that human-likeness might help moderating the abuse, human-likeness is probably not that powerful way to moderate robot abuse.;abuse moderation;From this finding, we speculate that, although one might consider that human-likeness might help moderating the abuse, human-likeness is probably not that powerful way to moderate robot abuse.;;;;ns;;Japanese;"The study was conducted in a shopping mall in Japan, using a
human-sized
humanoid
robot.";Other;;
184;2004;article;HRI;robot/agent;conference;USA;North America;Toward adaptive conversational interfaces: Modeling speech convergence with animated personas;The design of robust interfaces that process conversational speech is a challenging research direction largely because users' spoken language is so variable. This research explored a new dimension of speaker stylistic variation by examining whether users' speech converges systematically with the text-to-speech (TTS) heard from a software partner. To pursue this question, a study was conducted in which twenty-four 7 to 10-year-old children conversed with animated partners that embodied different TTS voices. An analysis of children's amplitude, durational features, and dialogue response latencies confirmed that they spontaneously adapt several basic acoustic-prosodic features of their speech 10-50%, with the-largest adaptations involving utterance pause structure and amplitude. Children's speech adaptations were relatively rapid, bidirectional, and dynamically readaptable when introduced to new partners, and generalized across different types of users and TTS voices. Adaptations also occurred consistently, with 70-95% of children converging with their partner's TTS, although individual differences in magnitude of adaptation were evident. In the design of future conversational systems, users' spontaneous convergence could be exploited to guide their speech within system processing bounds, thereby enhancing robustness. Adaptive system processing could yield further significant performance gains. The long-term goal of this research is the development of predictive models of human-computer communication to guide the design of new conversational interfaces.;https://www.scopus.com/inward/record.uri?eid=2-s2.0-5444241431&doi=10.1145%2f1017494.1017498&partnerID=40&md5=ef4296cc3f3d0845fe5f7d990fa717e6;184.Oviatt.pdf;Yes;Spin;"When children stopped the videotape, the animal displayed became
animated and available as a conversational partner who could answer
questions about itself using TTS output. In addition, an animated Spin the
dolphin software character, shown in the lower right corner of Figure 1, was
co-present on the screen. Spin could also converse with the child, answering
questions and assisting as needed with the interactive activity (e.g., starting
and stopping videotapes, spelling, offering praise, telling jokes).";Virtual Agent;"When children stopped the videotape, the animal displayed became
animated and available as a conversational partner who could answer
questions about itself using TTS output. In addition, an animated Spin the
dolphin software character, shown in the lower right corner of Figure 1, was
co-present on the screen. Spin could also converse with the child, answering
questions and assisting as needed with the interactive activity (e.g., starting
and stopping videotapes, spelling, offering praise, telling jokes).";Wizarded;"As such, children’s input was received by an informed
assistant who interpreted their queries and provided system responses";Yes;"which is an application designed to teach
children about basic marine biology,";No;"Text-to-speech voices from Lernout and Hauspie’s TTS 3000 were used to convey
the animated characters’ spoken output, including an American English
male and female voice. These voices then were tailored for general intelligibility
of pronunciation. They were further tailored to represent opposite ends of
the introvert-extrovert personality spectrum, as indicated by the speech signal
literature [Scherer 1979; Smith et al. 1995]. In total, four TTS voices were
used in this study: (1) Male extrovert (ME), (2) Male introvert (MI), (3) Female
extrovert (FE), and (4) Female introvert (FI)";Wizarded;;free speech;free speech;;"However, the child also was instructed
to ask each animal one question for Spin the dolphin";Edutainment;"During each encounter with a new animal, the child was encouraged to collect
information about it, to ask any questions he or she wished, and to have fun
learning new things about the animals.";tutor;higher;"However, the child also was instructed
to ask each animal one question for Spin the dolphin";School;school;was conducted at an elementary school field site.;give information;"During each encounter with a new animal, the child was encouraged to collect
information about it, to ask any questions he or she wished, and to have fun
learning new things about the animals.";8,9;07/Sep;2;narrow;"Twenty-four elementary-school children participated in this study as paid volunteers.
The participants were evenly divided into two age groups, younger
children (mean age 8 yrs, 2 mos), and older ones (mean age 9 yrs, 7 mos), with
each age group gender balanced. Based on parental and teacher pre-screening
information, all participants were native English speakers without known behavioral
or linguistic impairments, and not on medication that affects speech
patterns (e.g., for ADHD).";TD;"Twenty-four elementary-school children participated in this study as paid volunteers.
The participants were evenly divided into two age groups, younger
children (mean age 8 yrs, 2 mos), and older ones (mean age 9 yrs, 7 mos), with
each age group gender balanced. Based on parental and teacher pre-screening
information, all participants were native English speakers without known behavioral
or linguistic impairments, and not on medication that affects speech
patterns (e.g., for ADHD).";24;21-30;"Twenty-four elementary-school children participated in this study as paid volunteers.
The participants were evenly divided into two age groups, younger
children (mean age 8 yrs, 2 mos), and older ones (mean age 9 yrs, 7 mos), with
each age group gender balanced. Based on parental and teacher pre-screening
information, all participants were native English speakers without known behavioral
or linguistic impairments, and not on medication that affects speech
patterns (e.g., for ADHD).";Effect with CA on xyz (could have been any CA);"explore whether the basic acousticprosodic
features of users’ speech are influenced by the TTS they hear when interacting
with a computer partner.";TTS;"explore whether the basic acousticprosodic
features of users’ speech are influenced by the TTS they hear when interacting
with a computer partner.";acoustic prosodic features;"explore whether the basic acousticprosodic
features of users’ speech are influenced by the TTS they hear when interacting
with a computer partner.";;;;ns;;English;"Twenty-four elementary-school children participated in this study as paid volunteers.
The participants were evenly divided into two age groups, younger
children (mean age 8 yrs, 2 mos), and older ones (mean age 9 yrs, 7 mos), with
each age group gender balanced. Based on parental and teacher pre-screening
information, all participants were native English speakers without known behavioral
or linguistic impairments, and not on medication that affects speech
patterns (e.g., for ADHD).";English;;
188;2017;in proceedings;HRI;robot/agent;conference;USA;North America;Telling Stories to Robots: The Effect of Backchanneling on a Child's Storytelling;While there has been a growing body of work in child-robot interaction, we still have very little knowledge regarding young children's speaking and listening dynamics and how a robot companion should decode these behaviors and encode its own in a way children can understand. In developing a backchannel prediction model based on observed nonverbal behaviors of 4-6 year-old children, we investigate the effects of an attentive listening robot on a child's storytelling. We provide an extensive analysis of young children's nonverbal behavior with respect to how they encode and decode listener responses and speaker cues. Through a collected video corpus of peer-to-peer storytelling interactions, we identify attention-related listener behaviors as well as speaker cues that prompt opportunities for listener backchannels. Based on our findings, we developed a backchannel opportunity prediction (BOP) model that detects four main speaker cue events based on prosodic features in a child's speech. This rule-based model is capable of accurately predicting backchanneling opportunities in our corpora. We further evaluate this model in a human-subjects experiment where children told stories to an audience of two robots, each with a different backchanneling strategy. We find that our BOP model produces contingent backchannel responses that conveys an increased perception of an attentive listener, and children prefer telling stories to the BOP model robot.;https://doi.org/10.1145/2909824.3020245;188.Park.pdf;Yes;Tega;"Tega is an expressive social robot designed for long-term
deployment in homes and schools to support children's early
education [36].";Robot;"Tega is an expressive social robot designed for long-term
deployment in homes and schools to support children's early
education [36].";Fully autonomous;"For our human-subject experiment, we developed a sys-
tem architecture to support a fully autonomous human-robot
interaction (see Figure 4).";Yes;"Tega is an expressive social robot designed for long-term
deployment in homes and schools to support children's early
education [36].";Yes;"we pioneer the development of a computational backchannel-
ing model based on children voices and behaviors.";ns;;free speech;free speech;;"Using a high-quality microphone,
we extracted speakers' prosodic features in realtime using
openSMILE (pf) to determine speaking binaries(sb) and
predict backchannel (bc) opportunities with our BOP model.";Entertainment;"Participants were brought to the study room with two
sleeping Tegas on a table. The child was asked to sit on a
chair in the center of the table, and the parents were invited
to observe the session from a chair three feet behind the
child. Children initiated the interaction by either gently
rubbing, greeting, or telling Tegas that they were here to
tell stories. Tegas \woke up"" yawning at random intervals
and started backchanneling as the participant told his/her
story. When the child indicated he/she was done, the robots
fell back asleep before the post survey began in order to
prevent the child from feeling bad about making comments
on Tegas' behaviors.";listener;lower;"Participants were brought to the study room with two
sleeping Tegas on a table. The child was asked to sit on a
chair in the center of the table, and the parents were invited
to observe the session from a chair three feet behind the
child. Children initiated the interaction by either gently
rubbing, greeting, or telling Tegas that they were here to
tell stories. Tegas \woke up"" yawning at random intervals
and started backchanneling as the participant told his/her
story. When the child indicated he/she was done, the robots
fell back asleep before the post survey began in order to
prevent the child from feeling bad about making comments
on Tegas' behaviors.";lab;lab;"After a short introduction to the robots, participants were
brought to the experimental room and was asked to tell sto-
ries to the two robots (Figure 5).";listening;"Participants were brought to the study room with two
sleeping Tegas on a table. The child was asked to sit on a
chair in the center of the table, and the parents were invited
to observe the session from a chair three feet behind the
child. Children initiated the interaction by either gently
rubbing, greeting, or telling Tegas that they were here to
tell stories. Tegas \woke up"" yawning at random intervals
and started backchanneling as the participant told his/her
story. When the child indicated he/she was done, the robots
fell back asleep before the post survey began in order to
prevent the child from feeling bad about making comments
on Tegas' behaviors.";4,5,6,7;broad;4;broad;"Twenty-three children (age M = 6:13, SD = 1:36; 43.5%
female) were recruited through a local parents' mailing-list.";ns;;23;21-30;"Twenty-three children (age M = 6:13, SD = 1:36; 43.5%
female) were recruited through a local parents' mailing-list.";Effect with CA on xyz (could have been any CA);"we in-
vestigate the eects of an attentive listening robot on a
child's storytelling.";atentiveness;"we in-
vestigate the eects of an attentive listening robot on a
child's storytelling.";storytelling;"we in-
vestigate the eects of an attentive listening robot on a
child's storytelling.";;;;ns;;ns;;ns;more sound recognition than speech recognition;
192;2020;article;Computer Speech and Language;speech;journal;Mexico;North America;IESC-Child: An Interactive Emotional Children's Speech Corpus;In this paper, we describe the process that we used to create a new corpus of children's emotional speech. We used a Wizard of Oz (WoZ) setting to induce different emotional reactions in children during speech-based interactions with two robots. We recorded the speech spoken in Mexican Spanish by 174 children (both sexes) between 6 and 11 years of age. The recordings were manually segmented and transcribed. The segments were then labeled with two types of emotional-related paralinguistic information: emotion and attitude. The corpus contained 2093 min of audio recordings (34.88 h) divided into 19,793 speech segments. The Interactive Emotional Children's Speech Corpus (IESC-Child) can be a valuable resource for researchers studying affective reactions in speech communication during child-computer interactions in Spanish and for creating models to recognize acoustic paralinguistic information. IESC-Child is available to the research community upon request. © 2019 Elsevier Ltd;https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067598143&doi=10.1016%2fj.csl.2019.06.006&partnerID=40&md5=649d1f3f36317481ff4c4602de0559b7;192.Perez-Espinosa.pdf;Yes;EV3;"At the beginning of each test session, the facilitator showed the children two physically identical Lego Mindstorms
EV3 robots";Robot;"At the beginning of each test session, the facilitator showed the children two physically identical Lego Mindstorms
EV3 robots";Wizarded;"We used a Wizard of
Oz (WoZ) setting to induce different emotional reactions in children during speech-based interactions with two robots";ns;;No;"We used the Mexican Spanish voices included in the TTS engine of OS X to produce
the speech utterances";Wizarded;;free speech;free speech;;"The technician only had to choose a phrase
and play it using a keyboard shortcut. This manner of generating speech prompts facilitated the production of fluid
dialogues during the interaction.";Entertainment;"We designed this experiment based on the idea that entertaining and enjoyable games evoke a heightened level of
emotional experience during play (Hazlett, 2006). We expected that the child would engage in the activity and react
emotionally to positive and negative events such as winning or losing candies, avoiding or knocking over obstacles,
and being congratulated or reproached by the robots.";pupil;lower;"If the child
was able to guide the robot into a station, the child would obtain the number of candies indicated by its color.";school;school;"In the case that an affirmative answer was obtained, the parents took
the children to the research center for the activity. We recorded these subjects in the facilities of CICESE-UT3, in
Tepic, Nayarit, Mexico. In the second recruitment task, a primary school in the town of Ruiz, Nayarit, Mexico,
allowed us to perform the experiment within its facilities and record the sessions with their students during school
hours.";elicit emotional behavior;"We designed this experiment based on the idea that entertaining and enjoyable games evoke a heightened level of
emotional experience during play (Hazlett, 2006). We expected that the child would engage in the activity and react
emotionally to positive and negative events such as winning or losing candies, avoiding or knocking over obstacles,
and being congratulated or reproached by the robots.";6,7,8,9,10,11;broad;6;wide;"We
recorded the speech spoken in Mexican Spanish by 174 children (both sexes) between 6 and 11 years of age";ns;;174;>100;"We
recorded the speech spoken in Mexican Spanish by 174 children (both sexes) between 6 and 11 years of age";Effect with CA on xyz (could have been any CA);We designed a WoZ experiment to induce affective reaction in children;collaborativeness;"To produce different affective reactions in the participants,
we used a Wizard of Oz (WoZ) scenario where children participated in interactive sessions with two Lego
Mindstorms behaving either collaboratively or noncollaboratively";affective reactions;"To produce different affective reactions in the participants,
we used a Wizard of Oz (WoZ) scenario where children participated in interactive sessions with two Lego
Mindstorms behaving either collaboratively or noncollaboratively";;;;ns;;Spanish;"We
recorded the speech spoken in Mexican Spanish by 174 children (both sexes) between 6 and 11 years of age";Other;;
213;2017;in proceedings;HRI;robot/agent;conference;USA;North America;Creating Prosodic Synchrony for a Robot Co-Player in a Speech-Controlled Game for Children;Synchrony is an essential aspect of human-human interactions. In previous work, we have seen how synchrony manifests in low-level acoustic phenomena like fundamental frequency, loudness, and the duration of keywords during the play of child-child pairs in a fast-paced, cooperative, language-based game. The correlation between the increase in such low-level synchrony and increase in enjoyment of the game suggests that a similar dynamic between child and robot co-players might also improve the child's experience. We report an approach to creating on-line acoustic synchrony by using a dynamic Bayesian network learned from prior recordings of child-child play to select from a predefined space of robot speech in response to real-time measurement of the child's prosodic features. Data were collected from 40 new children, each playing the game with both a synchronizing and non-synchronizing version of the robot. Results show a significant order effect: although all children grew to enjoy the game more over time, those that began with the synchronous robot maintained their own synchrony to it and achieved higher engagement compared with those that did not.;https://doi.org/10.1145/2909824.3020244;213.Sadoughi.pdf;Yes;Sammy J;"Sammy (SC), a back-projected robot head designed by
Furhat Robotics [1] that has been set in a cardboard body to
sit next to the child in a more peer-like way";Robot;"Sammy (SC), a back-projected robot head designed by
Furhat Robotics [1] that has been set in a cardboard body to
sit next to the child in a more peer-like way";Fully autonomous;"An overall architecture controls the multiple parallel processes
for the game, the robot, and the custom word spotter
that performs keyword recognition. The game is programmed
in Unity3D, and Sammy plays by accessing an
A search algorithm that returns a go/jump decision based
on the next move along an optimal path. Sammy's vocal
space of utterances consists mainly of a set of pre-recorded
keyword les that vary with respect to prosodic features,
durations, and frequency, although the robot also has some
social speech that can be deployed at various points in the
environment when gameplay allows";ns;"An overall architecture controls the multiple parallel processes
for the game, the robot, and the custom word spotter
that performs keyword recognition. The game is programmed
in Unity3D, and Sammy plays by accessing an
A search algorithm that returns a go/jump decision based
on the next move along an optimal path. Sammy's vocal
space of utterances consists mainly of a set of pre-recorded
keyword les that vary with respect to prosodic features,
durations, and frequency, although the robot also has some
social speech that can be deployed at various points in the
environment when gameplay allows";ns;"An overall architecture controls the multiple parallel processes
for the game, the robot, and the custom word spotter
that performs keyword recognition. The game is programmed
in Unity3D, and Sammy plays by accessing an
A search algorithm that returns a go/jump decision based
on the next move along an optimal path. Sammy's vocal
space of utterances consists mainly of a set of pre-recorded
keyword les that vary with respect to prosodic features,
durations, and frequency, although the robot also has some
social speech that can be deployed at various points in the
environment when gameplay allows";ns;"An overall architecture controls the multiple parallel processes
for the game, the robot, and the custom word spotter
that performs keyword recognition. The game is programmed
in Unity3D, and Sammy plays by accessing an
A search algorithm that returns a go/jump decision based
on the next move along an optimal path. Sammy's vocal
space of utterances consists mainly of a set of pre-recorded
keyword les that vary with respect to prosodic features,
durations, and frequency, although the robot also has some
social speech that can be deployed at various points in the
environment when gameplay allows";limited commands;limited;2;"Eective play requires coordinated use of the keywords \go""
and \jump,"" which control horizontal and vertical motion,
respectively (see Figure 1(a)). Each participant is responsi
ble
for one keyword/direction at a time, but switches roles
between levels.";Entertainment;"Mole Madness (MM) is a speech-controlled, interactive
92
side-scroller in which two players move a mole through its
environment, avoiding obstacles and gaining rewards";peer;equal;"Sammy (SC), a back-projected robot head designed by
Furhat Robotics [1] that has been set in a cardboard body to
sit next to the child in a more peer-like way";ns;ns;;teamplayer;"Eective play requires coordinated use of the keywords \go""
and \jump,"" which control horizontal and vertical motion,
respectively (see Figure 1(a)). Each participant is responsible
for one keyword/direction at a time, but switches roles
between levels.";4,5,6,7,8,9,10;broad;7;wide;"We recruited 40 new children (50% girls) via postings in
physical and online community boards. The children's ages
ranged from 4 to 10 years old (M = 6:73 years, SD = 1:72).";ns;;40;31-40;"Data were collected from 40 new children,
each playing the game with both a synchronizing and
non-synchronizing version of the robot. Results show a signi
cant order eect: although all children grew to enjoy
the game more over time, those that began with the synchronous
robot maintained their own synchrony to it and
achieved higher engagement compared with those that did
not.";Effect with CA on xyz (could have been any CA);"Data were collected from 40 new children,
each playing the game with both a synchronizing and
non-synchronizing version of the robot. Results show a signi
cant order eect: although all children grew to enjoy
the game more over time, those that began with the synchronous
robot maintained their own synchrony to it and
achieved higher engagement compared with those that did
not.";sonchrony;"Data were collected from 40 new children,
each playing the game with both a synchronizing and
non-synchronizing version of the robot. Results show a signi
cant order eect: although all children grew to enjoy
the game more over time, those that began with the synchronous
robot maintained their own synchrony to it and
achieved higher engagement compared with those that did
not.";enjoyment, engagment;"Data were collected from 40 new children,
each playing the game with both a synchronizing and
non-synchronizing version of the robot. Results show a signi
cant order eect: although all children grew to enjoy
the game more over time, those that began with the synchronous
robot maintained their own synchrony to it and
achieved higher engagement compared with those that did
not.Data were collected from 40 new children,
each playing the game with both a synchronizing and
non-synchronizing version of the robot. Results show a signi
cant order eect: although all children grew to enjoy
the game more over time, those that began with the synchronous
robot maintained their own synchrony to it and
achieved higher engagement compared with those that did
not.";;;;ns;;ns;;ns;;
215;2011;conference;International Conference on Robotics and Biomimetics;robot/agent;conference;France;Europe;Evaluation of Emi interaction with non-disabled children in nursery school using wizard of Oz technique;Research in the field of emotional interaction is discussed here, for the EmotiRob project, to maintain interaction with children in the 4-to-8 year old age range. The objective of this project is to give comfort to vulnerable children and/or those undergoing long-term hospitalisation through the help of an emotional robot companion. The studies carried out on perception and emotional synthesis have allowed us to develop an experimental stuffed robot Emi, using an emotional model, iGrace, allowing for emotional reaction based on the speech of the user. This paper briefly presents the EmotiRob project and how emotion has been used for Emi. The last experiment done with children to evaluate their interaction with Emi is then described. © 2011 IEEE.;https://www.scopus.com/inward/record.uri?eid=2-s2.0-84860741391&doi=10.1109%2fROBIO.2011.6181442&partnerID=40&md5=6ff93071e631f092ccae3aab39e20936;215.Saint-Aime.pdf;Yes;Emi;"The studies carried out on
perception and emotional synthesis have allowed us to develop
an experimental stuffed robot Emi, using an emotional model,
iGrace, allowing for emotional reaction based on the speech
of the user.";Robot;"The studies carried out on
perception and emotional synthesis have allowed us to develop
an experimental stuffed robot Emi, using an emotional model,
iGrace, allowing for emotional reaction based on the speech
of the user.";Wizarded;the wizard of Oz device;Yes;"the objective of the EmotiRob project is
to design an autonomous stuffed robot with expressiveness,
which may bring some comfort to vulnerable children (eg,
children enduring long hospital stays).";Not relevant;;Wizarded;;free speech;free speech;;"he can understand what you
say, move his head (by yes or no) and express emotions";Healthcare;"The objective
of this project is to give comfort to vulnerable children and/or
those undergoing long-term hospitalisation through the help
of an emotional robot companion.";companion;equal;"In conclusion, in this experimental situation, Emi doesn’t
seem achieve its goal of robot companion.";School;school;"The experimentation took place in an
isolated room in the nursery school";give comfort;"The objective
of this project is to give comfort to vulnerable children and/or
those undergoing long-term hospitalisation through the help
of an emotional robot companion.";3,4,5;young;3;narrow;"Participants were 13 French children with typical development
(7 girls and 5 boys) between 3 and 5 years old (mean
± SD: 4.41 ± 0.60 years)";TD;"Participants were 13 French children with typical development
(7 girls and 5 boys) between 3 and 5 years old (mean
± SD: 4.41 ± 0.60 years)";13;Nov/20;"Participants were 13 French children with typical development
(7 girls and 5 boys) between 3 and 5 years old (mean
± SD: 4.41 ± 0.60 years)";Evaluating the CA;"This paper briefly presents the EmotiRob project
and how emotion has been used for Emi. The last experiment
done with children to evaluate their interaction with Emi is
then described.";robot;"In conclusion, in this experimental situation, Emi doesn’t
seem achieve its goal of robot companion.";comfort;"In conclusion, in this experimental situation, Emi doesn’t
seem achieve its goal of robot companion.";;;;ns;;French;"Participants were 13 French children with typical development
(7 girls and 5 boys) between 3 and 5 years old (mean
± SD: 4.41 ± 0.60 years)";Other;;
216;2011;conference;HRI;robot/agent;conference;France;Europe;Children recognize emotions of EmI companion robot;This article presents the evaluation of the emotional expressiveness of EmI companion robot in the EmotiRob project. We describe iGrace emotional computational model of emotion to generate an emotional response based on the speech of the interlocutor, the mechanical design and implementation of EmI, and experimentation to evaluate the expressiveness of EmI with 52 school children aged 7 to 9 years. © 2011 IEEE.;https://www.scopus.com/inward/record.uri?eid=2-s2.0-84860746587&doi=10.1109%2fROBIO.2011.6181443&partnerID=40&md5=cb9026e839014225d0b90e2fec38785f;216.Saint-Aime.pdf;Yes;Emi;"This article presents the evaluation of the emotional
expressiveness of EmI companion robot in the EmotiRob
project.";Robot;"This article presents the evaluation of the emotional
expressiveness of EmI companion robot in the EmotiRob
project.";Wizarded;"For this experiment, we use the Wizard
of Oz technique to simulate EmI behaviour";Yes;"In this context, the MAPH project objective is to design
an autonomous stuffed robot with expressivness, which may
bring some comfort to vulnerable children (eg, children
in long hospital stay)";Not relevant;"We will not forget to specify that EmI can
not speak but can move the body, head, lips and eyebrows";Wizarded;;limited phrases;limited;7;"The seven sentences previously defined (see Table I) are
distributed randomly.";Healthcare;"In this context, the MAPH project objective is to design
an autonomous stuffed robot with expressivness, which may
bring some comfort to vulnerable children (eg, children
in long hospital stay)";companion;equal;"This article presents the evaluation of the emotional
expressiveness of EmI companion robot in the EmotiRob
project.";School;school;"The experiment takes place over two half days in a class
room including EmI";give comfort;"In this context, the MAPH project objective is to design
an autonomous stuffed robot with expressivness, which may
bring some comfort to vulnerable children (eg, children
in long hospital stay)";7,8,9;07/Sep;3;narrow;"experimentation to evaluate the expressiveness of
EmI with 52 school children aged 7 to 9 years";ns;;52;51-60;"experimentation to evaluate the expressiveness of
EmI with 52 school children aged 7 to 9 years";Evaluating the CA;"This article presents the evaluation of the emotional
expressiveness of EmI companion robot in the EmotiRob
project.";robot;"This article has presented expressiveness evaluation for
EmI companion robot. To do this, an experiment was conducted
with 52 children. It aimed to evaluate its physical aspect
and emotional state expressed with an architecture using a
mechanical cable system.";expressiveness;"This article has presented expressiveness evaluation for
EmI companion robot. To do this, an experiment was conducted
with 52 children. It aimed to evaluate its physical aspect
and emotional state expressed with an architecture using a
mechanical cable system.";;;;ns;;ns;;ns;;
220;2015;in proceedings;IDC;CCI;conference;Korea;East Asia;Design and Evaluation of Socially Assistive Robotics Providing Assistance for Children through Machine Learning of Languages;"This study aims to design socially assistive robotics (SAR) that provide assistance to children aged 4-6 years; this is achieved through machine learning of languages, and by evaluating changes in children's reactions according to language learning stages of a robot. To this end, an assistive toy for language learning, named ARA, was developed and taught the language spoken by children who played with Furby and the responses and reactions of children during the different language learning degrees of ARA were video-recorded and analyzed. The recorded videos were quantitatively analyzed from a behavioral perspective (i.e. talking to Furby, touching or stroking Furby, and talking to his/her mother) and emotional perspective (i.e. positive expression, neutral expression, and negative expression). The analytical results were compared according to the language learning stages of ARA. Furthermore, on the basis of the analysis, it was evident that if robots can naturally communicate with children through gradual language learning based on conversations with the children, the children can improve their conversational ability as well as their communication skills by playing with robots. In addition, the children can interact with a communication target more positively compared with their reaction to simple movements or sound of a robot, and this can help achieve more positive emotional development in children.";https://doi.org/10.1145/2771839.2771887;220.Sanghoo.pdf;Yes;ARA;To this end, an assistive toy for language learning, named ARA, was developed and taught the language spoken by children who played with Furby and the responses and reactions of children during the different language learning degrees of ARA were video-recorded and analyzed;Robot;To this end, an assistive toy for language learning, named ARA, was developed and taught the language spoken by children who played with Furby and the responses and reactions of children during the different language learning degrees of ARA were video-recorded and analyzed;Fully autonomous;"In terms of software, ARA is operated by an Artificial Intelligence application named Sunny developed by Samsung I-Tech. This application can be downloaded from the Google Play store for free and is programmed to have basic conversations. Moreover, it learns new words and expressions through direct conversation with users and uses them appropriately according to situations that were experienced. Using this function, we recorded and analyzed the scene of children playing with Furby and taught their conversation to ARA based on the data analyzed in order to examine the children's reactions according to language learning degrees. In general, ARA resembles a book in terms of hardware (Figure 2(a)); further, it consists of an internal area in which a smartphone that runs the Sunny is to be placed, and a speaker that delivers external sound to the smartphone (Figure 2(b)). When ARA, in which the smartphone operating Sunny is placed, is sealed and put besides Furby; thus, children think that they are communicating with Furby although it is the ARA placed behind Furby that is communicating with them on the basis of the learned language.";Yes;"In terms of software, ARA is operated by an Artificial Intelligence application named Sunny developed by Samsung I-Tech. This application can be downloaded from the Google Play store for free and is programmed to have basic conversations. Moreover, it learns new words and expressions through direct conversation with users and uses them appropriately according to situations that were experienced. Using this function, we recorded and analyzed the scene of children playing with Furby and taught their conversation to ARA based on the data analyzed in order to examine the children's reactions according to language learning degrees. In general, ARA resembles a book in terms of hardware (Figure 2(a)); further, it consists of an internal area in which a smartphone that runs the Sunny is to be placed, and a speaker that delivers external sound to the smartphone (Figure 2(b)). When ARA, in which the smartphone operating Sunny is placed, is sealed and put besides Furby; thus, children think that they are communicating with Furby although it is the ARA placed behind Furby that is communicating with them on the basis of the learned language.";Yes;"In terms of software, ARA is operated by an Artificial Intelligence application named Sunny developed by Samsung I-Tech. This application can be downloaded from the Google Play store for free and is programmed to have basic conversations. Moreover, it learns new words and expressions through direct conversation with users and uses them appropriately according to situations that were experienced. Using this function, we recorded and analyzed the scene of children playing with Furby and taught their conversation to ARA based on the data analyzed in order to examine the children's reactions according to language learning degrees. In general, ARA resembles a book in terms of hardware (Figure 2(a)); further, it consists of an internal area in which a smartphone that runs the Sunny is to be placed, and a speaker that delivers external sound to the smartphone (Figure 2(b)). When ARA, in which the smartphone operating Sunny is placed, is sealed and put besides Furby; thus, children think that they are communicating with Furby although it is the ARA placed behind Furby that is communicating with them on the basis of the learned language.";Yes;"In terms of software, ARA is operated by an Artificial Intelligence application named Sunny developed by Samsung I-Tech. This application can be downloaded from the Google Play store for free and is programmed to have basic conversations. Moreover, it learns new words and expressions through direct conversation with users and uses them appropriately according to situations that were experienced. Using this function, we recorded and analyzed the scene of children playing with Furby and taught their conversation to ARA based on the data analyzed in order to examine the children's reactions according to language learning degrees. In general, ARA resembles a book in terms of hardware (Figure 2(a)); further, it consists of an internal area in which a smartphone that runs the Sunny is to be placed, and a speaker that delivers external sound to the smartphone (Figure 2(b)). When ARA, in which the smartphone operating Sunny is placed, is sealed and put besides Furby; thus, children think that they are communicating with Furby although it is the ARA placed behind Furby that is communicating with them on the basis of the learned language.";free speech;free speech;;"In terms of software, ARA is operated by an Artificial Intelligence application named Sunny developed by Samsung I-Tech. This application can be downloaded from the Google Play store for free and is programmed to have basic conversations. Moreover, it learns new words and expressions through direct conversation with users and uses them appropriately according to situations that were experienced. Using this function, we recorded and analyzed the scene of children playing with Furby and taught their conversation to ARA based on the data analyzed in order to examine the children's reactions according to language learning degrees. In general, ARA resembles a book in terms of hardware (Figure 2(a)); further, it consists of an internal area in which a smartphone that runs the Sunny is to be placed, and a speaker that delivers external sound to the smartphone (Figure 2(b)). When ARA, in which the smartphone operating Sunny is placed, is sealed and put besides Furby; thus, children think that they are communicating with Furby although it is the ARA placed behind Furby that is communicating with them on the basis of the learned language.";Education;"To this end, an assistive toy for language
learning, named ARA, was developed and taught the language
spoken by children who played with Furby and the responses and
reactions
of children
during the different
language learning
degrees of ARA were video-recorded and analyzed";assistant;lower;"To this end, an assistive toy for language
learning, named ARA, was developed and taught the language
spoken by children who played with Furby and the responses and
reactions
of children
during the different
language learning
degrees of ARA were video-recorded and analyzed";ns;ns;;teach language;"To this end, an assistive toy for language
learning, named ARA, was developed and taught the language
spoken by children who played with Furby and the responses and
reactions
of children
during the different
language learning
degrees of ARA were video-recorded and analyzed";4,5,6;young;3;narrow;This study aims to design an SAR that provides assistance to children aged 4–6 years through machine learning of languages and to evaluate changes in children’s reactions according to the language learning stages of the robot;ns;;5;01/Oct;A child and his or her mother formed a group, and five such groups were tested each week (Figure 3).;Effect with CA on xyz (could have been any CA);"This study aims to design socially assistive robotics (SAR) that provide assistance to children aged 4–6 years; this is achieved through machine learning of languages, and by evaluating changes in children’s reactions according to language learning stages of a robot.";CA;an assistive toy for language learning, named ARA, was manufactured and taught the language spoken by children who played with Furby along with appropriate responses, and reactions of children according to language learning degrees of ARA were video-recorded and analyzed.;behavior;an assistive toy for language learning, named ARA, was manufactured and taught the language spoken by children who played with Furby along with appropriate responses, and reactions of children according to language learning degrees of ARA were video-recorded and analyzed.;;;;ns;;Korean;"the
children
in
kindergarten
class
of
Kangnam Presbyterian Church located in Seoul.";Other;;
232;2018;conference;IDC;CCI;conference;Australia;Oceania;Talk to Me: The Role of Human-Robot Interaction in Improving Verbal Communication Skills in Students with Autism or Intellectual Disability;Autism is a developmental condition that can cause significant social, communication, and behavioral challenges. Children on the autism spectrum may have difficulties developing verbal communication skills, understanding what others say, or communicating through non-verbal cues. Similar difficulties are experienced by children with developmental delay. A recent trend in robotics is the design and implementation of robots to assist during therapy and education of children with learning difficulties. Although encouraging results suggests that robots can be beneficial, there has been limited work on the long-term impact of these tools on the verbal communication skills of children with autism or developmental delay. This paper explores the impact of robots on the verbal communication skills of secondary aged students with moderate to severe intellectual disabilities and autism. A qualitative study was carried out, via focus groups and interviews with parents, carers and staff members, 24 months after the introduction of two humanoid robots into the disability unit of a public secondary school. Results show that humanoid robots can provide benefits in articulation, verbal participation and spontaneous conversation in these young adults. Three exemplars are presented. © 2018 IEEE.;https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058100845&doi=10.1109%2fROMAN.2018.8525698&partnerID=40&md5=0d0346413a58a762347bb10cb33049b2;232.Silvera-Tawil.pdf;Yes;NAO;"NAO is a small (58 cm height, 4.3 kg weight), autonomous,
programmable humanoid robot developed by";Robot;"NAO is a small (58 cm height, 4.3 kg weight), autonomous,
programmable humanoid robot developed by";Wizarded;"the robot’s conversation was remotely controlled
by a staff member";No;;ns;;Wizarded;;free speech;free speech;;"individual session would run for approximately 30-45 min.
The robots were scripted to respond to the students’
speech or touch according to the requirements of the lesson.";Healthcare;"This paper explores the impact of robots on
the verbal communication skills of secondary aged students
with moderate to severe intellectual disabilities and autism";instructor;higher;"Delivery of robot-assisted lessons were generally done
with groups of five to ten students, where the robots acted as
an instructor or a social mediator,";school (DU);school;"SARs at the disability unit (DU) from a public secondary
school.";improve verbal communication skills;"This paper explores the impact of robots on
the verbal communication skills of secondary aged students
with moderate to severe intellectual disabilities and autism";ns;ns;ns;ns;"SARs at the disability unit (DU) from a public secondary
school.";autistic;"This paper explores the impact of robots on
the verbal communication skills of secondary aged students
with moderate to severe intellectual disabilities and autism";28;21-30;"Over the
24-month period, 28 children were enrolled and had multiple
opportunities to interact with the robots.";Effect with CA on xyz (could have been any CA);"This paper explores the impact of robots on
the verbal communication skills of secondary aged students
with moderate to severe intellectual disabilities and autism";robot;"This paper explores the impact of robots on
the verbal communication skills of secondary aged students
with moderate to severe intellectual disabilities and autism";verbal communication skills;"This paper explores the impact of robots on
the verbal communication skills of secondary aged students
with moderate to severe intellectual disabilities and autism";;;;ns;;ns;;ns;;
239;2018;conference;AAMAS;robot/agent;conference;USA;North America;A social robot system for modeling children's word pronunciation;Autonomous educational social robots can be used to help promote literacy skills in young children. Such robots, which emulate the emotive, perceptual, and empathic abilities of human teachers, are capable of replicating some of the benefits of one-on-one tutoring from human teachers, in part by leveraging individual student's behavior and task performance data to infer sophisticated models of their knowledge. These student models are then used to provide personalized educational experiences by, for example, determining the optimal sequencing of curricular material. In this paper we introduce an integrated system for autonomously analyzing and assessing children's speech and pronunciation in the context of an interactive word game between a social robot and a child. We present a novel game environment and its computational formulation, an integrated pipeline for capturing and analyzing children's speech in real-time, and an autonomous robot that models children's word pronunciation via Gaussian Process Regression (GPR), augmented with an Active Learning protocol that informs the robot's behavior. We show that the system is capable of autonomously assessing children's pronunciation ability, with ground truth determined by a post-experiment evaluation by human raters. We also compare phoneme- and word-level GPR models and discuss trade-offs of each approach in modeling children's pronunciation. Finally, we describe and analyze a pipeline for automatic analysis of children's speech and pronunciation, including an evaluation of SpeechAce as a tool for future development of autonomous, speech-based language tutors. © 2018 International Foundation for Autonomous Agents and Multiagent Systems (www.ifaamas.org). All rights reserved.;https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054767511&partnerID=40&md5=95360a691bf7eda4b2a57ed9db9c5b2b;239.Spaulding.pdf;Yes;ns;"For this experiment, we used a commercially
available social robot, though any ROS-compatible robot
platform can be used, as the agent’s gameplay behaviors are abstracted
through a software interface.";Robot;"For this experiment, we used a commercially
available social robot, though any ROS-compatible robot
platform can be used, as the agent’s gameplay behaviors are abstracted
through a software interface.";Fully autonomous;"We have evaluated this system with an agent model that supports
efficient assessment of the student’s knowledge via active learning
and phoneme- and word-based GPR models of student vocabulary
learned from children’s speech, and have shown that the system
can effectively assess children’s pronunciation ability from speech
captured during autonomous, interactive gameplay.";ns;;ns;;ns;"The robot analyzes the collected speech from each child’s ingame
responses to construct two different models of word pronunciation
ability: a PHONEME-GPR model, which models the child’s
average pronunciation score of each phoneme in the standard ARPAbet
list of English phonemes as a normal random variable, and a
WORD-GPR model, which models the average pronunciation score
of each word in the Test Curriculum as a normal random variable.
Note that while a PHONEME-GPR model can be transformed into a
WORD-GPR model (by averaging the posterior prediction mean of
each phoneme in a word), the structure and training of each model
is different.";limited phrases;limited;ns;"Pictures and letters of
words from a Test Curriculum (e.g, animals or common household
items, see Sec. 3.3) appear in the center of the tablet, and when
a picture/word pair appears, the robot and child ""race"" to tap their
button and ""ring in"", giving the player who rings in first a chance
to say the word out loud and receive a point if the pronunciation
is deemed ""correct"".";Education;"Autonomous educational social robots can be used to help promote
literacy skills in young children";peer;equal;"Thus,
the interaction is framed as a competitive game between a peer-like
robot (an expressive robotic ‘character’ with a child-like voice and
expressive sound effects, see Sec: 3.1) and the child.";lab;lab;"After arriving at our lab, children were
asked to draw a picture of a robot as a ‘warm-up’ exercise while
the parent filled out a survey and consent form.";evaluate and model child's speech;"In this paper, we present an autonomous robotic tutoring system
designed to evaluate and model children’s productive vocabulary
by recording and analyzing their spoken responses";ns;ns;ns;ns;;ns;;15;Nov/20;"We recruited 15 children from the local area to participate in a
system evaluation interaction in three phases";Evaluating the CA;"We have evaluated this system with an agent model that supports
efficient assessment of the student’s knowledge via active learning
and phoneme- and word-based GPR models of student vocabulary
learned from children’s speech, and have shown that the system
can effectively assess children’s pronunciation ability from speech
captured during autonomous, interactive gameplay.";System;"We have evaluated this system with an agent model that supports
efficient assessment of the student’s knowledge via active learning
and phoneme- and word-based GPR models of student vocabulary
learned from children’s speech, and have shown that the system
can effectively assess children’s pronunciation ability from speech
captured during autonomous, interactive gameplay.";children's pronunciation;"We have evaluated this system with an agent model that supports
efficient assessment of the student’s knowledge via active learning
and phoneme- and word-based GPR models of student vocabulary
learned from children’s speech, and have shown that the system
can effectively assess children’s pronunciation ability from speech
captured during autonomous, interactive gameplay.";;;;ns;;English;"As part of our model evaluation, we also
compare the results of SpeechAce analysis to the ratings of native
English speakers collected via Amazon Mechanical Turk (see Sec.
4.1), to characterize SpeechAce’s utility as a tool for further research
on autonomous assessment of student word pronunciation.";English;;
240;2019;in proceedings;HRI;robot/agent;conference;USA;North America;Pronunciation-Based Child-Robot Game Interactions to Promote Literacy Skills;In this paper we present additional results from a prior study of speech-based games to promote early literacy skills through child-robot interaction [6]. The additional data and results support our original conclusion, that pronunciation analysis software can be an effective enabler of speech child-robot interactions. We also include a comparison of other pronunciation services, an updated version of the SpeechAce API and a new technology from Soapbox Labs. We reflect on some lessons learned and introduce a redesigned version of the game interaction called 'RhymeRacer' based on the results and observations from both data collections.;https://ieeexplore.ieee.org/abstract/document/8673296;240.Spaulding.pdf;Yes;Jibo;"Children sit
across from a robot (for the experiment, we used a modified
version of the Jibo robot)";Robot;"Children sit
across from a robot (for the experiment, we used a modified
version of the Jibo robot)";ns;;ns;;ns;;Partially;"Here, we present the combined results
from both data collections, including comparisons with a
new service called Soapbox Labs, which offers pronunciation
technology specifically designed for children’s speech";limited phrases;limited;ns;"Children were told that they were racing
the robot to tap the buzzer on their side of the screen to ”ring
in” and then pronounce the word in the center.";Education;"to promote early literacy
skills through child-robot interaction";tutor;higher;"personalized literacy
tutoring interventions";School;school;"we conducted
a follow-up study with 13 additional participants from pre-K
and Kindergarten classes at a local public school";promote literacy skills;"to promote early literacy
skills through child-robot interaction";3,4,5,6;broad;4;broad;"we conducted
a follow-up study with 13 additional participants from pre-K
and Kindergarten classes at a local public school";ns;;13;Nov/20;"we conducted
a follow-up study with 13 additional participants from pre-K
and Kindergarten classes at a local public school";Evaluating the CA;"The
additional data supports our previous findings: pronunciation
tools can be an effective technology to enable speech-based
child-robot interactions.";System;"The
additional data supports our previous findings: pronunciation
tools can be an effective technology to enable speech-based
child-robot interactions.";children's pronunciation;"The
additional data supports our previous findings: pronunciation
tools can be an effective technology to enable speech-based
child-robot interactions.";;;;ns;;ns;;ns;;
242;2020;conference;IDC;CCI;conference;Italy;Europe;Whom would you like to talk with: Exploring conversational agents for children's linguistic assessment;The dramatic increment of communication impairments among children increases the demand for intensive, highly accessible and low-cost interventions as well as new assessment and therapeutic tools. Our research aims at exploring the use of Conversational Agents (CAs) to support linguistic assessment and training among children with language impairment. One of the open research issues in this arena concerns the identification of the most appropriate form of embodiment of the CA for children to interact with. To this end, we evaluated the linguistic performance of 14 neuro-typical children and 3 children with language impairment comparing different CAs - physical object and virtual character - with traditional human interaction. Based on our analysis, we identify insights for the design of CA: the physicality does influence the performance of linguistic tasks for children with linguistic impairment. In addition, children seem to show a preference for the physical CA and perceived it as smarter than the virtual one. © 2020 ACM.;https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087403598&doi=10.1145%2f3392063.3394421&partnerID=40&md5=67d1aa30eb809857efab6814563e86b5;242.Spitale.pdf;Yes;;"An animal-like (a lamb), not a pet, CA was chosen to avoid any
bias due to the children’s affection towards pets. In both toy and
avatar conditions, the CAs were able just to move their mouths
(lip-sync).";Virtual Agent;"An animal-like (a lamb), not a pet, CA was chosen to avoid any
bias due to the children’s affection towards pets. In both toy and
avatar conditions, the CAs were able just to move their mouths
(lip-sync).";Wizarded;"In this paper, we use the term CA as a broader concept
which also includes the Wizard-of-Oz approach.";ns;;ns;;Wizarded;;free speech;free speech;;"The only difference was that the children
were not asked to repeat a story they listened, but they had to
produce their own story and tell it to the conversational agent/human
counterpart.";Healthcare;"Our research aims at exploring the use of Conversational Agents
(CAs) to support linguistic assessment and training among children
with language impairment.";tutor;higher;"Our research aims at exploring the use of Conversational Agents
(CAs) to support linguistic assessment and training among children
with language impairment.";lab;lab;The experiment was performed in a quiet room;support linguistic assessment;"Our research aims at exploring the use of Conversational Agents
(CAs) to support linguistic assessment and training among children
with language impairment.";6,7,8;07/Sep;3;narrow;"Fourteen NT children (half female) were recruited from the local
summer campus. Participants ranged from 7 to 8 years (mean =
7.7, SD = 0.4) and they composed our control group. Regarding
the target group, the recruitment process included ten participants
from a local speech-therapy center, but then some children were
excluded from the final clinical group because their parents did not
return signed consents, while for other children it was impossible to
administer the protocol because they were too tired, too distracted
or too severely impaired. Eventually, three children (one female)
with language impairment (age range 6-8, mean = 6.6, SD = 1.1)
constituted our target group.";language impairment;"Fourteen NT children (half female) were recruited from the local
summer campus. Participants ranged from 7 to 8 years (mean =
7.7, SD = 0.4) and they composed our control group. Regarding
the target group, the recruitment process included ten participants
from a local speech-therapy center, but then some children were
excluded from the final clinical group because their parents did not
return signed consents, while for other children it was impossible to
administer the protocol because they were too tired, too distracted
or too severely impaired. Eventually, three children (one female)
with language impairment (age range 6-8, mean = 6.6, SD = 1.1)
constituted our target group.";17;Nov/20;"Fourteen NT children (half female) were recruited from the local
summer campus. Participants ranged from 7 to 8 years (mean =
7.7, SD = 0.4) and they composed our control group. Regarding
the target group, the recruitment process included ten participants
from a local speech-therapy center, but then some children were
excluded from the final clinical group because their parents did not
return signed consents, while for other children it was impossible to
administer the protocol because they were too tired, too distracted
or too severely impaired. Eventually, three children (one female)
with language impairment (age range 6-8, mean = 6.6, SD = 1.1)
constituted our target group.";Effect of CA on xyz;"In this paper, we examined how the
embodiment of CAs affects the performance of the children during
linguistic tasks and how they perceive the Cas";embodiment;"In this paper, we examined how the
embodiment of CAs affects the performance of the children during
linguistic tasks and how they perceive the Cas";linguistic skills;"In this paper, we examined how the
embodiment of CAs affects the performance of the children during
linguistic tasks and how they perceive the Cas";;;;ns;;Italian;"All children in the present study were monolingual Italian-speaking
children";Other;;
246;2020;in proceedings;HRI;robot/agent;conference;Germany;Europe;She Talks to Me as If She Were Alive: Assessing the Social Reactions and Perceptions of Children toward Voice Assistants and Their Appraisal of the Appropriateness of These Reactions;It has long been observed that people treat technical entities like computers as social actors. Numerous studies have been conducted on media equation theory, which describes users' social reactions toward devices to be unconscious. These users consider this inappropriate when thought about consciously. However, both long-term trends as well as the reactions of children have been neglected - although children might indeed differ in their reactions, as they are especially prone to the tendency to humanize and anthropomorphize. This study examined the social behavior of 20 children aged six to twelve toward a voice assistant during a five-week field study. In the course of three sessions, interviews were conducted to assess the children's tendency to socially react to the devices and to what extent they perceive such reactions as appropriate. Additionally, the tendency to anthropomorphize was assessed together with qualitative data to gain insight into cues that would trigger anthropomorphization. Results indicate that age and duration have an effect on the tendency to anthropomorphize. However, age and duration have only a partial effect on social reactions and the evaluation of their appropriateness. Furthermore, natural speech appeared to be a rather strong cue in triggering tendencies to anthropomorphize in the children.;https://doi.org/10.1145/3383652.3423906;246.Strathmann.pdf;Yes;Echo Dot, Home Mini;"For this purpose, 18 German households each received either an
Amazon Echo Dot or a Google Home Mini.";Smart Speaker;"For this purpose, 18 German households each received either an
Amazon Echo Dot or a Google Home Mini.";Fully autonomous;"The hypotheses were examined in a five-week long-term field study.
For this purpose, 18 German households each received either an
Amazon Echo Dot or a Google Home Mini";ns;;ns;;ns;;free speech;free speech;;"where they could ask Alexa or Google Assistant whatever they
wanted.";Other;"Therefore, a list of eight requests was prepared for
the children to ask the voice assistant in the course of an interaction.
These were supposed to elicit appreciation from Alexa/Google
Assistant (Table 1). To make the interaction more diverse, the
requests were partially changed for the second and third sessions.
Since the devices were not programmed beforehand, the requests
were chosen because of the suitability of the answers given.";assistant;lower;"This study examined the social behavior of 20
children aged six to twelve toward a voice assistant during a five
week
field study.";Home;home;"The hypotheses were examined in a five-week long-term field study.
For this purpose, 18 German households each received either an
Amazon Echo Dot or a Google Home Mini";"Therefore, a list of eight requests was prepared for
the children to ask the voice assistant in the course of an interaction.
These were supposed to elicit appreciation from Alexa/Google
Assistant (Table 1). To make the interaction more diverse, the
requests were partially changed for the second and third sessions.
Since the devices were not programmed beforehand, the requests
were chosen because of the suitability of the answers given.";answer questions;6,7,8,9,10,11,12;broad;7;wide;"This study examined the social behavior of 20
children aged six to twelve toward a voice assistant during a five
week
field study.";ns;;20;Nov/20;"This study examined the social behavior of 20
children aged six to twelve toward a voice assistant during a five
week
field study.";Effect with CA on xyz (could have been any CA);"This study examined the social behavior of 20
children aged six to twelve toward a voice assistant during a fiveweek
field study.";time, CA;"This study examined the social behavior of 20
children aged six to twelve toward a voice assistant during a five
week
field study.";social behavior;"This study examined the social behavior of 20
children aged six to twelve toward a voice assistant during a five
week
field study.";;;;ns;;German;"The hypotheses were examined in a five-week long-term field study.
For this purpose, 18 German households each received either an
Amazon Echo Dot or a Google Home Mini";Other;;
250;2017;in proceedings;HAI;robot/agent;conference;Japan;East Asia;Effects of a Listener Robot with Children in Storytelling;This paper investigates the effects of a listener robot that joins a storytelling situation for children as a side-participant. For this purpose, we develop a storytelling-robot system that consists of both reader and listener robots. Our semi-autonomous system involves a human operator who makes correct responses and provides easily understandable answers to the children's questions during storytelling. We develop a gaze model for the natural and autonomous gaze behaviors of a reader robot by considering multiple listeners and a storytelling object (a display that shows images). We conducted an experiment with 16 children to investigate whether they preferred storytelling with/without the listener robot and the changes of speech activities during the storytelling. Children preferred storytelling with the listener robot to storytelling without it. Their speech activities decreased when the listener robot was involved in the storytelling.;https://doi.org/10.1145/3125739.3125750;250.Tamura.pdf;Yes;Sota;Figure 3 shows Sota, an interactive humanoid robot characterized by its humanlike physical expressions;Robot;Figure 3 shows Sota, an interactive humanoid robot characterized by its humanlike physical expressions;Wizarded;For this study, an operator assumed the speech recognition and behavior selector functions using a tele-operation system (Wizard of Oz [23]) and followed pre-determined rules.;ns;;ns;;Wizarded;;free speech;free speech;;If the children asked the reader robot a question, it answers with prepared explanation behaviors based on speech recognition results from the operator;Entertainment;This paper investigates the effects of a listener robot that joins a storytelling situation for children as a side-participant;peer;equal;This paper investigates the effects of a listener robot that joins a storytelling situation for children as a side-participant;lab;lab;"Figure 5 shows the experiment environment. The room is approximately 40 m2. Toys, books, and chairs are available for the participants to represent a realistic playroom environment; already this environment was used for experiments in child-robot interaction research fields";listener;This paper investigates the effects of a listener robot that joins a storytelling situation for children as a side-participant;3,4,5;young;3;narrow;Sixteen children (eight girls and eight boys from three to five years old, average age 4.3, S.D 0.86) and their parents (15 mothers and 1 father) participated in the experiment.;ns;;16;Nov/20;Sixteen children (eight girls and eight boys from three to five years old, average age 4.3, S.D 0.86) and their parents (15 mothers and 1 father) participated in the experiment.;Effect with CA on xyz (could have been any CA);To investigate the effects of the listener robot on the preferences and the activities of children, we conducted a within-participant experiment where our robot system reads stories to children.;CA;We predict that the presence of a listener robot will provide positive storytelling effects for children;positive storytelling effects;We predict that the presence of a listener robot will provide positive storytelling effects for children;;;;ns;;Japanese;Nekko umare no kobito tachi in Japanese);Other;;
256;2020;article;IVA;robot/agent;conference;Vietnam;Southeast Asia;The influential role of robot in second language classes based on artificial intelligence;In the fast development of technology, robot has played many vital roles in different fields of life. Robot is also used in education as a teaching assistant in arts, science and language course. In Viet Nam, the secondary schools or high schools spend a lot of money on hiring native speakers for teaching English classes. Currently, in most Asian countries where English is taught as a foreign language, therefore many various instructional methods are being used for teaching English at school as well as English Language Centers. As a result of this paper approach to English teaching robots, the use of robots like as foreign language instructors has been referred to as a robotic revolution in Industrial Revolution 4.0. In this study, we will describe applications of several robots have been conducting the English teaching instructors tutoring some extra-class's programs in Viet Nam. The results shown the potentials of individualized interaction in language education are substantial. These performances give the positively contributions to improve of motivations of children studying English as well as other languages. This paper will show the effective tools for language education of robot. From experiments in reality, using robot being an instructor in English class has proven to enhance the language teaching for children effectively. The children, who learn English with an instructor like as robot have shown better learning results in speaking, as well as more confidence and much motivation in studying. © 2020 by the authors.;https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089805802&doi=10.18178%2fijmerr.9.9.1306-1311&partnerID=40&md5=49c83f51e02174e2db51af83e46b681c;256.TruongThinh.pdf;Yes;MiABot;The main objective of the current research was the service robot named MiABot for use in the English learning classes.;Robot;The main objective of the current research was the service robot named MiABot for use in the English learning classes.;Fully autonomous;MiABot was pre-programmed based on AI for each teaching session to be teacher in teaching a particular syllabus and was operated autonomously during the teaching sessions.;ns;;ns;;ns;;free speech;free speech;;Specially, when students talk in a trained topic which practiced many times with robot, students can speak really fluently, raise many own ideas and use a lot of difficult vocabularies;Education;As a result of this paper approach to English teaching robots, the use of robots like as foreign language instructors has been referred to as a robotic revolution in Industrial Revolution 4.0;tutor;higher;As a result of this paper approach to English teaching robots, the use of robots like as foreign language instructors has been referred to as a robotic revolution in Industrial Revolution 4.0;School;school;Classroom is set up with a round table, chairs, speakers, microphones and a robot;foreign language instructor;As a result of this paper approach to English teaching robots, the use of robots like as foreign language instructors has been referred to as a robotic revolution in Industrial Revolution 4.0;ns;ns;ns;ns;;ns;;ns;ns;;Effect with CA on xyz (could have been any CA);From experiments in reality, using robot being an instructor in English class has proven to enhance the language teaching for children effectively.;robot;The children, who learn English with an instructor like as robot have shown better learning results in speaking, as well as more confidence and much motivation in studying. _;learning, confidence, motivation;The children, who learn English with an instructor like as robot have shown better learning results in speaking, as well as more confidence and much motivation in studying. _;;;;ns;;English;"As a result of this paper approach to
English teaching robots, the use of robots like as foreign
language instructors has been referred to as a robotic
revolution in Industrial Revolution 4.0";English;English quite bad, hard to read;
258;2017;conference;HAI;robot/agent;conference;Brazil, Japan;Combination;Wizard of Oz vs autonomous: Children's perception changes according to robot's operation condition;The presence of robots in human lifestyle is no longer a distant reality, as robots are being employed in several fields, including educational purposes. However, most of the research in educational robotics does not use autonomous social behavior, but rather techniques like Wizard of Oz (WoZ). This paper presents the very first test in a school environment of a robotic architecture to control an autonomous system for educational interactions, evaluated from user's perspective when compared to a teleoperated situation. The architecture aims to manage three main communication robot resources - speech, vision and gesture - in an autonomous way and provide an interaction as acceptable as when someone controls the robot. The experiment was performed randomly assigning 82 students aged between 7 and 11 to interact with a NAO robot in two conditions of robot operation: Autonomous and teleoperated. The results suggest that there is no significant difference between the conditions in user's enjoyment and system time response, but they decreased their perception regarding robot's intelligence after knowing about the teleoperation. © 2017 IEEE.;https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045857040&doi=10.1109%2fROMAN.2017.8172374&partnerID=40&md5=7ab95f8de1ff566113f2090e1d26fa70;258.Tozadore.pdf;Yes;NAO;"The experiment was performed randomly assigning 82
students aged between 7 and 11 to interact with a NAO robot
in two conditions of robot operation: autonomous and teleoperated.";Robot;"The experiment was performed randomly assigning 82
students aged between 7 and 11 to interact with a NAO robot
in two conditions of robot operation: autonomous and teleoperated.";Fully autonomous;"The experiment was performed randomly assigning 82
students aged between 7 and 11 to interact with a NAO robot
in two conditions of robot operation: autonomous and teleoperated.";ns;;No;"For the voice
synthesis we use NAO’s default voice, as it was well accepted
in other tests.";No;"We used Google Speech Recognition, which is an API that
sends a wave file format to its server and takes back the
corresponding string.";free speech;free speech;;"The architecture described in
Section III decided the interaction flow based on the students’
verbal answers and on the visual system classification";Education;"We chose the content according to their teaching plan for
mathematics, with the help of their teachers and educational
experts, which means that all participants should already
have knowledge of the geometric figures and the concepts
of Faces and Edges.";tutor;higher;"The approaches proceeded following
the adopted pedagogical model: Present a concept, define
it and take practical exercise as challenge.";School;school;"The participants were 82 students (37 males, 46 females)
aged between 7 to 11 (M=9.36, SD=1.24) from the elementary
school ”Professor Paulino Botelho” in the city of S˜ao
Carlos - SP, Brazil, where we applied the activity";challenge students;"The approaches proceeded following
the adopted pedagogical model: Present a concept, define
it and take practical exercise as challenge.";7,8,9,10,11;broad;5;broad;"The participants were 82 students (37 males, 46 females)
aged between 7 to 11 (M=9.36, SD=1.24) from the elementary
school ”Professor Paulino Botelho” in the city of S˜ao
Carlos - SP, Brazil, where we applied the activity";ns;;82;81-90;"The participants were 82 students (37 males, 46 females)
aged between 7 to 11 (M=9.36, SD=1.24) from the elementary
school ”Professor Paulino Botelho” in the city of S˜ao
Carlos - SP, Brazil, where we applied the activity";Effect with CA on xyz (could have been any CA);"In this study, we investigated whether there was significant
difference in users’ experience in different system operation
conditions.";operating condition;"In this study, we investigated whether there was significant
difference in users’ experience in different system operation
conditions.";user experience;"In this study, we investigated whether there was significant
difference in users’ experience in different system operation
conditions.";;;;ns;;ns;;ns;;
262;2018;conference;ICRA;robot/agent;conference;Greece;Europe;Multi3: Multi-sensory perception system for multi-modal child interaction with multiple robots;Child-robot interaction is an interdisciplinary research area that has been attracting growing interest, primarily focusing on edutainment applications. A crucial factor to the successful deployment and wide adoption of such applications remains the robust perception of the child's multimodal actions, when interacting with the robot in a natural and untethered fashion. Since robotic sensory and perception capabilities are platform-dependent and most often rather limited, we propose a multiple Kinect-based system to perceive the child-robot interaction scene that is robot-independent and suitable for indoors interaction scenarios. The audio-visual input from the Kinect sensors is fed into speech, gesture, and action recognition modules, appropriately developed in this paper to address the challenging nature of child-robot interaction. For this purpose, data from multiple children are collected and used for module training or adaptation. Further, information from the multiple sensors is fused to enhance module performance. The perception system is integrated in a modular multi-robot architecture demonstrating its flexibility and scalability with different robotic platforms. The whole system, called Multi3, is evaluated, both objectively at the module level and subjectively in its entirety, under appropriate child-robot interaction scenarios containing several carefully designed games between children and robots. © 2018 IEEE.;https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063145395&doi=10.1109%2fICRA.2018.8461210&partnerID=40&md5=858acd545e526ff9788fdbdfe74ed58e;262.Tsiami.pdf;Yes;Multi3;"Aiming to evaluate the use case scenario, we tested the
Multi3 system real-time";Robot;"Aiming to evaluate the use case scenario, we tested the
Multi3 system real-time";Fully autonomous;"The multi-modal, multi-sensory robot perception system,
which is depicted in Fig. 2, has been designed and
trained/adapted specifically for children. The upper part of
the perception system refers to action and gesture recognition,
while the lower part to distant speech recognition.
Results from the perception modules are forwarded to the
multi-robot architecture described in Section III.";Yes;"The development of a multi-modal (action, gesture,
speech), multi-sensory (multiple Kinects) robot perception
system for child-robot interaction, the Multi3
system, presented in detail in Section II.";ns;;Yes;"Concerning audio data, children voice characteristics differ
from adults, e.g., the pitch of the voice. Although several
human action and speech databases exist with adult data, they
are not suitable for training/adapting a perception system
dedicated to children. At the same time, children data are
hard and time-consuming to obtain.
Therefore, for each game we performed an extensive data
collection featuring 28 children participants. During data collection,
each child performed sequentially all seven gestures
mentioned in the “Show me the gesture” game, expressed the
six feelings from the “Express the feeling” task, mimed the
twelve tasks of the “Pantomime” game, and also performed
random movements for the background models. In addition,
each child uttered 40 out of 120 phrases inspired by and
adapted to the use case scenario. These consist of gamedependent
utterances like pantomime answers with variations
in pronunciation, e.g. “Are you dancing?” and “I think you
are dancing”, and some general purpose utterances, e.g.
“yes”,“no”.";limited phrases;limited;40;"Concerning audio data, children voice characteristics differ
from adults, e.g., the pitch of the voice. Although several
human action and speech databases exist with adult data, they
are not suitable for training/adapting a perception system
dedicated to children. At the same time, children data are
hard and time-consuming to obtain.
Therefore, for each game we performed an extensive data
collection featuring 28 children participants. During data col
lection,
each child performed sequentially all seven gestures
mentioned in the “Show me the gesture” game, expressed the
six feelings from the “Express the feeling” task, mimed the
twelve tasks of the “Pantomime” game, and also performed
random movements for the background models. In addition,
each child uttered 40 out of 120 phrases inspired by and
adapted to the use case scenario. These consist of game
dependent
utterances like pantomime answers with variations
in pronunciation, e.g. “Are you dancing?” and “I think you
are dancing”, and some general purpose utterances, e.g.
“yes”,“no”.";Entertainment;"As mentioned earlier, three games have been designed to
prompt children to interact with robots using not only their
voice, but also their facial expressions and body movements
(see also Fig. 6). Thus, the kids perceive that they can play
with robots without many limitations and act as they do while
playing with their peers.";peer;equal;"As mentioned earlier, three games have been designed to
prompt children to interact with robots using not only their
voice, but also their facial expressions and body movements
(see also Fig. 6). Thus, the kids perceive that they can play
with robots without many limitations and act as they do while
playing with their peers.";lab;lab;"The interaction took place in an appropriately
designed friendly environment, decorated to remind a child’s
room.";play;"As mentioned earlier, three games have been designed to
prompt children to interact with robots using not only their
voice, but also their facial expressions and body movements
(see also Fig. 6). Thus, the kids perceive that they can play
with robots without many limitations and act as they do while
playing with their peers.";6,7,8,9,10;broad;5;broad;"For this purpose, 28 children (18
male, 10 female) from six to ten years old (average: eight
years old), were invited to interact with the system.";ns;;28;21-30;"For this purpose, 28 children (18
male, 10 female) from six to ten years old (average: eight
years old), were invited to interact with the system.";Evaluating the CA;"Aiming to evaluate the use case scenario, we tested the
Multi3 system real-time";system;"System evaluation has been carried out using
children data collected according to the proposed use case,
with objective and subjective evaluation results confirming
the success of our system both in terms of performance and
user acceptability.";user acceptability;"System evaluation has been carried out using
children data collected according to the proposed use case,
with objective and subjective evaluation results confirming
the success of our system both in terms of performance and
user acceptability.";;;;ns;;Greek;The employed language is Greek;Other;;
266;2014;in proceedings;HRI;robot/agent;conference;USA;North America;Spatial and Other Social Engagement Cues in a Child-Robot Interaction: Effects of a Sidekick;In this study, we explored the impact of a co-located sidekick on child-robot interaction. We examined child behaviors while interacting with an expressive furniture robot and his robot lamp sidekick. The results showed that the presence of a sidekick did not alter child proximity, but did increase attention to spoken elements of the interaction. This suggests the addition of a co-located sidekick has potential to increase engagement but may not alter subtle physical interactions associated with personal space and group spatial arrangements. The findings also reinforce existing research by the community on proxemics and anthropomorphism.;https://doi.org/10.1145/2559636.2559684;266.Vazquez.pdf;Yes;Chester;"Chester is a child-sized, mobile chionier robot, with wood
casing, actuated drawers, and a hidden 1D laser measure-
ment system (Figure 1).";Robot;"Chester is a child-sized, mobile chionier robot, with wood
casing, actuated drawers, and a hidden 1D laser measure-
ment system (Figure 1).";Wizarded;a Wizard of Oz fashion.;ns;;ns;;Wizarded;;free speech;free speech;;"1. Acknowledgment. The participants were acknowledged.
Blink and Chester looked towards the end of the hall,
and realized that the children were coming. As par-
ticipants approached, Chester and Blink verbally indi-
cated that they were checking if they had the photos.
2. Greeting. Chester greeted the participants, and intro-
duced Blink.
3. How are you? Chester asked the participants how they
were doing and if they liked being at our research fa-
cility.
4. Remember. Chester asked the children if they remem-
bered the pictures they took during the earlier game.
Chester told them that the photos were in its drawers.
5. Stuck. Participants experienced the rising action part
of the story: Chester realized its drawers were \stuck""
and, after conferring with Blink, said that they may
need oil.
6. Bump. Chester indicted that he thought that bumping
into a wall was a good way of xing the problem, but
Blink dissuaded Chester to prevent him from damaging
the wall.
7. Spin. Chester asked participants to step back and spun
around in an attempt to unstick the drawers, but was
unsuccessful.
8. Shaking. Chester shook, following Blink's advice, and
nally got the drawers unstuck.
9. Opened drawers. Chester told the participants to\come
grab your pictures"", and
(a) Participants grabbed the pictures, or
(b) If participants did not want to grab the pictures,
then the experimenter removed the pictures and
gave them to the participants.
10. Visit again?
Chester asked the participants if they
liked the pictures, if they would come to visit again,
and if they had to leave.
11. Goodbye. Chester and Blink said goodbye to the chil-
dren, retreated to a safe location, and closed their eyes.";Entertainment;"1. Acknowledgment. The participants were acknowledged.
Blink and Chester looked towards the end of the hall,
and realized that the children were coming. As par-
ticipants approached, Chester and Blink verbally indi-
cated that they were checking if they had the photos.
2. Greeting. Chester greeted the participants, and intro-
duced Blink.
3. How are you? Chester asked the participants how they
were doing and if they liked being at our research fa-
cility.
4. Remember. Chester asked the children if they remem-
bered the pictures they took during the earlier game.
Chester told them that the photos were in its drawers.
5. Stuck. Participants experienced the rising action part
of the story: Chester realized its drawers were \stuck""
and, after conferring with Blink, said that they may
need oil.
6. Bump. Chester indicted that he thought that bumping
into a wall was a good way of xing the problem, but
Blink dissuaded Chester to prevent him from damaging
the wall.
7. Spin. Chester asked participants to step back and spun
around in an attempt to unstick the drawers, but was
unsuccessful.
8. Shaking. Chester shook, following Blink's advice, and
nally got the drawers unstuck.
9. Opened drawers. Chester told the participants to\come
grab your pictures"", and
(a) Participants grabbed the pictures, or
(b) If participants did not want to grab the pictures,
then the experimenter removed the pictures and
gave them to the participants.
10. Visit again? Chester asked the participants if they
liked the pictures, if they would come to visit again,
and if they had to leave.
11. Goodbye. Chester and Blink said goodbye to the chil-
dren, retreated to a safe location, and closed their eyes.";unclear;unclear;"1. Acknowledgment. The participants were acknowledged.
Blink and Chester looked towards the end of the hall,
and realized that the children were coming. As par-
ticipants approached, Chester and Blink verbally indi-
cated that they were checking if they had the photos.
2. Greeting. Chester greeted the participants, and intro-
duced Blink.
3. How are you? Chester asked the participants how they
were doing and if they liked being at our research fa-
cility.
4. Remember. Chester asked the children if they remem-
bered the pictures they took during the earlier game.
Chester told them that the photos were in its drawers.
5. Stuck. Participants experienced the rising action part
of the story: Chester realized its drawers were \stuck""
and, after conferring with Blink, said that they may
need oil.
6. Bump. Chester indicted that he thought that bumping
into a wall was a good way of xing the problem, but
Blink dissuaded Chester to prevent him from damaging
the wall.
7. Spin. Chester asked participants to step back and spun
around in an attempt to unstick the drawers, but was
unsuccessful.
8. Shaking. Chester shook, following Blink's advice, and
nally got the drawers unstuck.
9. Opened drawers. Chester told the participants to\come
grab your pictures"", and
(a) Participants grabbed the pictures, or
(b) If participants did not want to grab the pictures,
then the experimenter removed the pictures and
gave them to the participants.
10. Visit again? Chester asked the participants if they
liked the pictures, if they would come to visit again,
and if they had to leave.
11. Goodbye. Chester and Blink said goodbye to the chil-
dren, retreated to a safe location, and closed their eyes.";lab;lab;"The physical space where the interaction occurred is de-
picted in Figure 2. The robot wizard was in the same room
as the participants due to safety concerns, since this was
our rst experiment with the platform.";give pictures;"1. Acknowledgment. The participants were acknowledged.
Blink and Chester looked towards the end of the hall,
and realized that the children were coming. As par-
ticipants approached, Chester and Blink verbally indi-
cated that they were checking if they had the photos.
2. Greeting. Chester greeted the participants, and intro-
duced Blink.
3. How are you? Chester asked the participants how they
were doing and if they liked being at our research fa-
cility.
4. Remember. Chester asked the children if they remem-
bered the pictures they took during the earlier game.
Chester told them that the photos were in its drawers.
5. Stuck. Participants experienced the rising action part
of the story: Chester realized its drawers were \stuck""
and, after conferring with Blink, said that they may
need oil.
6. Bump. Chester indicted that he thought that bumping
into a wall was a good way of xing the problem, but
Blink dissuaded Chester to prevent him from damaging
the wall.
7. Spin. Chester asked participants to step back and spun
around in an attempt to unstick the drawers, but was
unsuccessful.
8. Shaking. Chester shook, following Blink's advice, and
nally got the drawers unstuck.
9. Opened drawers. Chester told the participants to\come
grab your pictures"", and
(a) Participants grabbed the pictures, or
(b) If participants did not want to grab the pictures,
then the experimenter removed the pictures and
gave them to the participants.
10. Visit again? Chester asked the participants if they
liked the pictures, if they would come to visit again,
and if they had to leave.
11. Goodbye. Chester and Blink said goodbye to the chil-
dren, retreated to a safe location, and closed their eyes.";4,5,6,7,8,9,10;broad;7;wide;"Twenty groups of 3 or 4 children interacted with Chester
and Blink. Participants were 4 to 10 years old, some were
siblings, and were accompanied by at least one adult.";ns;;70;61-70;"Twenty groups of 3 or 4 children interacted with Chester
and Blink. Participants were 4 to 10 years old, some were
siblings, and were accompanied by at least one adult.";Effect with CA on xyz (could have been any CA);"In this study, we explored the impact of a co-located sidekick
on child-robot interaction.";sidekick;"We examined child behaviors
while interacting with an expressive furniture robot and his
robot lamp sidekick.";behavior;"We examined child behaviors
while interacting with an expressive furniture robot and his
robot lamp sidekick.";;;;ns;;ns;;ns;;
267;2011;in proceedings;HRI;robot/agent;conference;USA;North America;DOMER: A Wizard of Oz Interface for Using Interactive Robots to Scaffold Social Skills for Children with Autism Spectrum Disorders;"This report describes the development of a prototypical Wizard of Oz, graphical user interface to wirelessly control a small, humanoid robot (Aldebaran Nao) during a therapy session for children with Autism Spectrum Disorders (ASD). The Dynamically Operated Manually Executed Robot interface (DOMER) enables an operator to initiate pre-developed behavior sequences for the robot as well as access the text-to-speech capability of the robot in real-time interactions between children with ASD and their therapist. Preliminary results from a pilot study suggest that the interface enables the operator to control the robot with sufficient fidelity such that the robot can provide positive feedback, practice social dialogue, and play the game, ""Simon Says"" in a convincing and engaging manner.";https://doi.org/10.1145/1957656.1957770;267.Villano.pdf;Yes;NAO;"This report describes the development of a prototypical Wizard of
Oz, graphical user interface to wirelessly control a small,
humanoid robot (Aldebaran Nao) during a therapy session for
children with Autism Spectrum Disorders (ASD).";Robot;"This report describes the development of a prototypical Wizard of
Oz, graphical user interface to wirelessly control a small,
humanoid robot (Aldebaran Nao) during a therapy session for
children with Autism Spectrum Disorders (ASD).";Wizarded;"This report describes the development of a prototypical Wizard of
Oz, graphical user interface to wirelessly control a small,
humanoid robot (Aldebaran Nao) during a therapy session for
children with Autism Spectrum Disorders (ASD).";ns;;ns;;Wizarded;;free speech;free speech;;"We developed
therapy
scripts
relevant
to
the
child’s
ABA
goals
of
initiating/responding to greetings, answering questions about
feelings, and answering/asking recall questions.";Healthcare;"This report describes the development of a prototypical Wizard of
Oz, graphical user interface to wirelessly control a small,
humanoid robot (Aldebaran Nao) during a therapy session for
children with Autism Spectrum Disorders (ASD).";tutor;higher;"We developed
therapy scripts relevant to the child’s ABA goals of
initiating/responding to greetings, answering questions about
feelings, and answering/asking recall questions.";therapy;therapy;"During the
session, the child would alternate between interacting with either
the therapist or the robot. The therapy session was monitored by
an operator through a one-way mirror and audio/video feeds";respond to child;"We developed
therapy scripts relevant to the child’s ABA goals of
initiating/responding to greetings, answering questions about
feelings, and answering/asking recall questions.";9;07/Sep;1;narrow;"The child in the pilot study
was a nine-year-old male with ASD whose IQ and language skills
were measured at 3 standard deviations below the mean for his
age.";autistic;"The child in the pilot study
was a nine-year-old male with ASD whose IQ and language skills
were measured at 3 standard deviations below the mean for his
age.";1;01/Oct;"The child in the pilot study
was a nine-year-old male with ASD whose IQ and language skills
were measured at 3 standard deviations below the mean for his
age.";Evaluating the CA;"The purpose of our pilot study was to determine the feasibility of
using our interface to control an interactive robot during an ABAbased
social-communication therapy";interface;"Preliminary results from a pilot
study suggest that the interface enables the operator to control the
robot with sufficient fidelity such that the robot can provide
positive feedback, practice social dialogue, and play the game,
“Simon Says” in a convincing and engaging manner.";elicit behavior, practice skills;"Preliminary results from a pilot
study suggest that the interface enables the operator to control the
robot with sufficient fidelity such that the robot can provide
positive feedback, practice social dialogue, and play the game,
“Simon Says” in a convincing and engaging manner.";;;;ns;;ns;;ns;;
271;2014;article;HRI;robot/agent;conference;UK;Europe;A Pilot Study with a Novel Setup for Collaborative Play of the Humanoid Robot KASPAR with Children with Autism;This article describes a pilot study in which a novel experimental setup, involving an autonomous humanoid robot, KASPAR, participating in a collaborative, dyadic video game, was implemented and tested with children with autism, all of whom had impairments in playing socially and communicating with others. The children alternated between playing the collaborative video game with a neurotypical adult and playing the same game with the humanoid robot, being exposed to each condition twice. The equipment and experimental setup were designed to observe whether the children would engage in more collaborative behaviours while playing the video game and interacting with the adult than performing the same activities with the humanoid robot. The article describes the development of the experimental setup and its first evaluation in a small-scale exploratory pilot study. The purpose of the study was to gain experience with the operational limits of the robot as well as the dyadic video game, to determine what changes should be made to the systems, and to gain experience with analyzing the data from this study in order to conduct a more extensive evaluation in the future. Based on our observations of the childrens' experiences in playing the cooperative game, we determined that while the children enjoyed both playing the game and interacting with the robot, the game should be made simpler to play as well as more explicitly collaborative in its mechanics. Also, the robot should be more explicit in its speech as well as more structured in its interactions. Results show that the children found the activity to be more entertaining, appeared more engaged in playing, and displayed better collaborative behaviours with their partners (For the purposes of this article, 'partner' refers to the human/robotic agent which interacts with the children with autism. We are not using the term's other meanings that refer to specific relationships or emotional involvement between two individuals.) in the second sessions of playing with human adults than during their first sessions. One way of explaining these findings is that the children's intermediary play session with the humanoid robot impacted their subsequent play session with the human adult. However, another longer and more thorough study would have to be conducted in order to better re-interpret these findings. Furthermore, although the children with autism were more interested in and entertained by the robotic partner, the children showed more examples of collaborative play and cooperation while playing with the human adult. © 2013 The Author(s).;https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896306077&doi=10.1007%2fs12369-013-0195-x&partnerID=40&md5=dce600a31aafc44171eaa2c92e48d11e;271.Wainer.pdf;Yes;KASPAR;"This article describes a pilot study in which a
novel experimental setup, involving an autonomous humanoid
robot, KASPAR, participating in a collaborative,
dyadic video game, was implemented and tested with children
with autism, all of whom had impairments in playing
socially and communicating with others.";Robot;"This article describes a pilot study in which a
novel experimental setup, involving an autonomous humanoid
robot, KASPAR, participating in a collaborative,
dyadic video game, was implemented and tested with children
with autism, all of whom had impairments in playing
socially and communicating with others.";Fully autonomous;"In contrast with
previous HRI studies that involved children with autism, our
robot behaved autonomously instead of being controlled in a
“Wizard of Oz” fashion, in the sense that it was not remotely
controlled by a hidden human operator";No;"Designed to interact with people in HRI studies by
performing simple gestures, displaying basic facial expressions,
and, in our setup, using speech and low-level socially
communicative behaviours such as joint attention and pointing,
KASPAR is equipped with two 4 degree-of-freedom
(DOF) arms as well as an 8 DOF head capable of panning,
tilting, blinking and moving its eyes, and displaying a range
of smiles and frowns [14]";Partially;"However, in order to make the voice
sound slightly more childish, we raised the pitch on all of
the speech samples by 21 % using Audacity, a free software
package used for the mixing and editing of sound and music
files.";Other;"However, using a speech recognition system as a means of
communication was ruled out for two reasons: firstly, the
extensive amount of training that the system would have to
undergo to learn each child’s pronunciation and intonation
of each word was likely to be so uninteresting for most of
the children as to dissuade them from participating in our
study; secondly, some of the children’s limited communicative
abilities would make it difficult for any speech recognition
system to consistently and correctly interpret their
speech.";not relevant;not relevant;;"As such, we programmed KASPAR to instead respond
to the buttons that the children pressed on their Wiimotes
(this information was also received directly from the
video game via a Yarp connection), since all of the children
were theoretically capable of communicating in this manner
and the easily-identifiable nature of the button’s signal
would guarantee that it would always be reliably and correctly
interpreted by the robot.";Healthcare;"This article describes a pilot study in which a
novel experimental setup, involving an autonomous hu
manoid
robot, KASPAR, participating in a collaborative,
dyadic video game, was implemented and tested with chil
dren
with autism, all of whom had impairments in playing
socially and communicating with others.";peer;equal;"This article describes a pilot study in which a
novel experimental setup, involving an autonomous hu
manoid
robot, KASPAR, participating in a collaborative,
dyadic video game, was implemented and tested with chil
dren
with autism, all of whom had impairments in playing
socially and communicating with others.";ns;ns;;play video game;"This article describes a pilot study in which a
novel experimental setup, involving an autonomous hu
manoid
robot, KASPAR, participating in a collaborative,
dyadic video game, was implemented and tested with chil
dren
with autism, all of whom had impairments in playing
socially and communicating with others.";6,7,8;07/Sep;3;narrow;Table 1 Descriptions of the children participating in this study;autistic;"Six children with autism participated in this preliminary
study from a local school for children with special needs";6;01/Oct;"Six children with autism participated in this preliminary
study from a local school for children with special needs";Evaluating the CA;"The purpose of the study was to gain
experience with the operational limits of the robot as well as
the dyadic video game, to determine what changes should be
made to the systems, and to gain experience with analyzing
the data from this study in order to conduct a more extensive
evaluation in the future.";system;"The children alternated
between playing the collaborative video game with a
neurotypical adult and playing the same game with the humanoid
robot, being exposed to each condition twice.";experience;"Results show that the children found the activity to be
more entertaining, appeared more engaged in playing, and
displayed better collaborative behaviours with their partners";;;;ns;;English;"Acapela text-to-speech generator using the
male English voice of “Graham”,";English;;
273;2011;article;HRI;robot/agent;conference;USA;North America;My Science Tutor: A conversational multimedia virtual tutor for elementary school science;This article describes My Science Tutor (MyST), an intelligent tutoring system designed to improve science learning by students in 3rd, 4th, and 5th grades (7 to 11 years old) through conversational dialogs with a virtual science tutor. In our study, individual students engage in spoken dialogs with the virtual tutor Marni during 15 to 20 minute sessions following classroom science investigations to discuss and extend concepts embedded in the investigations. The spoken dialogs in MyST are designed to scaffold learning by presenting open-ended questions accompanied by illustrations or animations related to the classroom investigations and the science concepts being learned. The focus of the interactions is to elicit self-expression from students. To this end, Marni applies some of the principles of Questioning the Author, a proven approach to classroom conversations, to challenge students to think about and integrate new concepts with prior knowledge to construct enriched mental models that can be used to explain and predict scientific phenomena. In this article, we describe how spoken dialogs using Automatic Speech Recognition (ASR) and natural language processing were developed to stimulate students' thinking, reasoning and self explanations. We describe the MyST system architecture and Wizard of Oz procedure that was used to collect data from tutorial sessions with elementary school students. Using data collected with the procedure, we present evaluations of the ASR and semantic parsing components. A formal evaluation of learning gains resulting from system use is currently being conducted. This paper presents survey results of teachers' and children's impressions of MyST. © 2011 ACM.;https://www.scopus.com/inward/record.uri?eid=2-s2.0-80052007190&doi=10.1145%2f1998384.1998392&partnerID=40&md5=17015f91bba8cd7310ca73bb304435c5;273.Ward.pdf;Yes;Marni;"In our study, individual students engage in spoken dialogs with the virtual tutor Marni
during 15 to 20 minute sessions following classroom science investigations to discuss and extend concepts
embedded in the investigations.";Virtual Agent;"In our study, individual students engage in spoken dialogs with the virtual tutor Marni
during 15 to 20 minute sessions following classroom science investigations to discuss and extend concepts
embedded in the investigations.";Wizarded;"We describe the
MyST system architecture and Wizard of Oz procedure that was used to collect data from tutorial sessions
with elementary school students.";Yes;"Since 2007, our research team has been involved in an intensive effort to develop
an intelligent tutoring system, My Science Tutor (MyST), intended to improve science
learning by 3rd, 4th, and 5th grade children through natural spoken dialogs with Marni,
a virtual science tutor.";ns;;Wizarded;;free speech;free speech;;"When ready to speak, the student holds down the space bar. As the student speaks, the
audio data is sent to the speech recognition system. When the space bar is released,
the single best scoring word string is sent to the parser, which returns a set of semantic
parses. The set of parses is sent to the dialogue manager, which selects a single best
parse given the current context, integrates the new information into the context and
generates an action sequence given the new context. The actions are executed and the
system again waits for a student response.";Education;"In our study, individual students engage in spoken dialogs with the virtual tutor Marni
during 15 to 20 minute sessions following classroom science investigations to discuss and extend concepts
embedded in the investigations.";tutor;higher;"In our study, individual students engage in spoken dialogs with the virtual tutor Marni
during 15 to 20 minute sessions following classroom science investigations to discuss and extend concepts
embedded in the investigations.";School;school;"In our study, individual students engage in spoken dialogs with the virtual tutor Marni
during 15 to 20 minute sessions following classroom science investigations to discuss and extend concepts
embedded in the investigations.";help learn science concepts;"to help struggling students learn the science concepts
encountered in classroom science instruction.";ns;ns;ns;ns;;ns;;167;>100;"A written survey was given to 167 students who used MyST in five elementary schools
during the 2009–2010 school year.";Evaluating the CA;"The survey included ten questions that asked for ratings of student experience and
impressions of the program and its usability";system;"The survey included ten questions that asked for ratings of student experience and
impressions of the program and its usability";usability;"The survey included ten questions that asked for ratings of student experience and
impressions of the program and its usability";;;;ns;;ns;;ns;;
275;2016;conference;RO-MAN;robot/agent;conference;USA;North America;Effects of framing a robot as a social agent or as a machine on children's social behavior;The presentation or framing of a situation - such as how something or someone is introduced - can influence people's subsequent behavior. In this paper, we describe a study in which we manipulated how a robot was introduced, framing it as either a social agent or as a machine-like being. We asked whether framing the robot in these ways would influence young children's social behavior while playing a ten-minute game with the robot. We coded children's behavior during the robot interaction, including their speech, gaze, and various courteous, prosocial actions. We found several subtle differences in children's gaze behavior between conditions that may reflect children's perceptions of the robot's status as more, or less, of a social actor. In addition, more parents of children in the Social condition reported that their children acted less shy and more talkative with the robot that parents of children in the Machine condition. This study gives us insight into how the interaction context can influence how children think about and respond to social robots. © 2016 IEEE.;https://www.scopus.com/inward/record.uri?eid=2-s2.0-85002968698&doi=10.1109%2fROMAN.2016.7745193&partnerID=40&md5=310849c1edbf2c1e33453f6425d9e4e4;275.KoryWestlund.pdf;Yes;Tega;"We used the Android phone-based robot Tega (Figure 1),
a fluffy “squash and stretch” robot developed by the Personal
Robots Group at the MIT Media Lab";Robot;"We used the Android phone-based robot Tega (Figure 1),
a fluffy “squash and stretch” robot developed by the Personal
Robots Group at the MIT Media Lab";Wizarded;"The robot
was teleoperated by Experimenter 1, primarily to deal with
language understanding.";Yes;"Tega, which was
designed for interactions with young children.";Partially;"The robot’s speech was recorded by a female
adult and the pitch was shifted higher so it would sound more
child-like.";Wizarded;;free speech;free speech;;"The goal was to see how
many spontaneous questions, comments, and laughs the child
produced,";Entertainment;"Following the framing manipulation, Experimenter 2 left
the robot area and the robot interaction began. Experimenter
1 teleoperated the robot (more details below). The robot
introduced itself and asked the child about their favorite color
and what they liked to do for fun. Then the robot led the child
in a second session of the APT.
After this task, children played a sorting game with the
robot. The game involved sorting a set of objects by color,
size, and shape. The robot began by sorting by one attribute
(“Can you put all the blue shapes in the same pile for
me?”), then invited the child to sort by a different attribute
(Figure 1). The robot and child each got three turns.";unclear;unclear;"Following the framing manipulation, Experimenter 2 left
the robot area and the robot interaction began. Experimenter
1 teleoperated the robot (more details below). The robot
introduced itself and asked the child about their favorite color
and what they liked to do for fun. Then the robot led the child
in a second session of the APT.
After this task, children played a sorting game with the
robot. The game involved sorting a set of objects by color,
size, and shape. The robot began by sorting by one attribute
(“Can you put all the blue shapes in the same pile for
me?”), then invited the child to sort by a different attribute
(Figure 1). The robot and child each got three turns.";lab;lab;"Next, the child was led to the robot interaction area, where
Experimenter 2 performed the framing manipulation";play game;"Following the framing manipulation, Experimenter 2 left
the robot area and the robot interaction began. Experimenter
1 teleoperated the robot (more details below). The robot
introduced itself and asked the child about their favorite color
and what they liked to do for fun. Then the robot led the child
in a second session of the APT.
After this task, children played a sorting game with the
robot. The game involved sorting a set of objects by color,
size, and shape. The robot began by sorting by one attribute
(“Can you put all the blue shapes in the same pile for
me?”), then invited the child to sort by a different attribute
(Figure 1). The robot and child each got three turns.";3,4,5,6,7;broad;5;broad;"Twenty-two children aged 3–7 years (M = 5:04, SD =
1:23, min = 3:12, max = 7:42) were recruited";TD;"All
children except one were typically-developing. One girl had
a sensory processing disorder and she did not complete the
robot interaction, so she was excluded from the behavior
analysis.";22;21-30;"Twenty-two children aged 3–7 years (M = 5:04, SD =
1:23, min = 3:12, max = 7:42) were recruited";Effect with CA on xyz (could have been any CA);"In this paper, we describe a study
in which we manipulated how a robot was introduced, framing
it as either a social agent or as a machine-like being. We
asked whether framing the robot in these ways would influence
young children’s social behavior while playing a ten-minute
game with the robot.";introduction;"In this paper, we describe a study
in which we manipulated how a robot was introduced, framing
it as either a social agent or as a machine-like being. We
asked whether framing the robot in these ways would influence
young children’s social behavior while playing a ten-minute
game with the robot.";social behavior;"In this paper, we describe a study
in which we manipulated how a robot was introduced, framing
it as either a social agent or as a machine-like being. We
asked whether framing the robot in these ways would influence
young children’s social behavior while playing a ten-minute
game with the robot.";;;;ns;;ns;;ns;;
276;2018;in proceedings;IDC;CCI;conference;USA;North America;"""My Doll Says It's Ok"": A Study of Children's Conformity to a Talking Doll";Today's children are growing up with smart toys, Internet-connected devices that use artificial intelligence to drive interactive play. In a prior research study, we found that children ages 4--10 perceive these toys as worthy of trust [5]. This leads us to inquire if children in this age range could be directly influenced by these devices. In this work, we used a conformity test and a disobedience task to study how children are influenced by a talking doll. We found that the doll could influence children to change their judgments about moral transgressions, however it was unsuccessful in persuading children to disobey an instruction. Finally, we analyzed children's perceptions of the smart toy and discusses implications of this work for future child-agent interaction.;https://doi.org/10.1145/3202185.3210788;276.Williams.pdf;Yes;Cayla;"My Friend Cayla (Figure 2) is a speech-enabled
doll with the appearance and voice of a female toddler that
uses a companion mobile-phone app to entertain children
with stories and games.";Robot;"My Friend Cayla (Figure 2) is a speech-enabled
doll with the appearance and voice of a female toddler that
uses a companion mobile-phone app to entertain children
with stories and games.";Wizarded;"We teleoperated Cayla with a mobile-phone
app that sent audio through the toy’s internal speaker.";Yes;"My Friend Cayla (Figure 2) is a speech-enabled
doll with the appearance and voice of a female toddler that
uses a companion mobile-phone app to entertain children
with stories and games.";Yes;"My Friend Cayla (Figure 2) is a speech-enabled
doll with the appearance and voice of a female toddler that
uses a companion mobile-phone app to entertain children
with stories and games.";Wizarded;;ns;ns;;;Other;;unclear;unclear;;lab;lab;"Top: child completing
conformity test with Cayla doll.
Bottom: child waiting with box
during disobedience task.";convince;"we investigated the ability of a
talking doll to directly influence children on a conformity test
and a disobedience task.";4,5,6,7,8,9,10;broad;7;wide;"Our sample consisted of 40 children (40%
female) who ranged in age from 4 to 10 years old (M=6.60,
SD=2.05).";ns;;40;31-40;"Our sample consisted of 40 children (40%
female) who ranged in age from 4 to 10 years old (M=6.60,
SD=2.05).";Effect with CA on xyz (could have been any CA);"we investigated the ability of a
talking doll to directly influence children on a conformity test
and a disobedience task.";system;"we investigated the ability of a
talking doll to directly influence children on a conformity test
and a disobedience task.";conformity;"we investigated the ability of a
talking doll to directly influence children on a conformity test
and a disobedience task.";;;;ns;;ns;;ns;;
278;2015;in proceedings;OzCHI;hci;conference;Australia;Oceania;Children's Expectations and Strategies in Interacting with a Wizard of Oz Robot;This paper presents an analysis of children's interactions with an early prototype of a robot that is being designed for deployment in early learning centres. 23 children aged 2-6 interacted with the prototype, consisting of a pair of tablets embedded in a flat and vaguely humanoid form. We used a Wizard of Oz (WoZ) technique to control a synthesized voice that delivered predefined statements and questions, and a tablet mounted as a head that displayed animated eyes. The children's interactions with the robot and with the adult experimenter were video recorded and analysed in order to identify some of the children's expectations of the robot's behaviour and capabilities, and to observe their strategies for interacting with a speaking and minimally animated artificial agent. We found a surprising breadth in children's reactions, expectations and strategies (as evidenced by their behaviour) and a noteworthy tolerance for the robot's occasionally awkward behaviour.;https://doi.org/10.1145/2838739.2838793;278.Worthy.pdf;Yes;ns;;Robot;"The first version of the prototype robot developed for user
testing was a 110 cm wooden robot with two arms by its
side (see Figure 1).";Wizarded;"We used a Wizard of Oz (WoZ) technique to control a
synthesized voice that delivered predefined statements
and questions, and a tablet mounted as a head that
displayed animated eyes";ns;;ns;;Wizarded;;free speech;free speech;;"the detailed verbal and non-verbal
strategies attempted by children and employed at
sequentially relevant conversational points";Education;"This paper presents an analysis of children’s interactions
with an early prototype of a robot that is being designed
for deployment in early learning centres";tutor;higher;"The first robot design in the project is
focused on the development of a friendly robot for
interacting with children aged 2-6 years. One of the aims
of the project is to compare children’s learning with
tablet-based tasks, with and without a social robot";public space;public space;"We tested this version of the robot in a laboratory
environment and at an open public space.";teach;"The first robot design in the project is
focused on the development of a friendly robot for
interacting with children aged 2-6 years. One of the aims
of the project is to compare children’s learning with
tablet-based tasks, with and without a social robot";2,3,4,5,6;broad;5;broad;"23 children aged
2-6 interacted with the prototype, consisting of a pair of
tablets embedded in a flat and vaguely humanoid form.";ns;;23;21-30;"23 children aged
2-6 interacted with the prototype, consisting of a pair of
tablets embedded in a flat and vaguely humanoid form.";Evaluating the CA;"This paper presents an analysis of children’s interactions
with an early prototype of a robot that is being designed
for deployment in early learning centres";system;"In this paper we have described a range of young
children’s reactions to, expectations of, and strategies for
engaging a robot in interaction.";expectations, strategies;"In this paper we have described a range of young
children’s reactions to, expectations of, and strategies for
engaging a robot in interaction.";;;;ns;;ns;;ns;;
280;2020;conference;IDC;CCI;conference;USA;North America;"""elinor Is talking to me on the screen!"" integrating conversational agents into children's television programming";"Science-oriented television and video programming can be an important source of science learning for young children. However, the educational benefits of television have long been limited by children not being able to interact with the content in a contingent way. This project leverages an intelligent conversational agent -an on-screen character capable of verbal interaction-to add social contingency into children's experience watching science videos. This conversational agent has been developed in an iterative process and embedded in a new PBS KIDS science show ""Elinor Wonders Why."" This Late Breaking Work presents the design of the conversational agent and reports findings from a field study that has proven feasibility of this approach. We also discuss our planned future work to examine the agent's effectiveness in enhancing children's engagement and learning. © 2020 Owner/Author.";https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090234961&doi=10.1145%2f3334480.3383000&partnerID=40&md5=6f29618304ea8f4828a79320b3542d0d;280.Xu.pdf;Yes;Elinor;"We have developed an initial version of the interactive video
with a CA based on one episode of Elinor Wonders Why,
""Learning from Nature,"" where Elinor and her friends explore
how hook-and-loop tapes work";Virtual Agent;"We have developed an initial version of the interactive video
with a CA based on one episode of Elinor Wonders Why,
""Learning from Nature,"" where Elinor and her friends explore
how hook-and-loop tapes work";Fully autonomous;Conversation Design;Yes;"The Show Elinor Wonders Why
Elinor Wonders Why is intended to inspire young children’s
wonder and curiosity about nature and teach them the basic
scientific skills to answer their own questions. The stories
in this show center around Elinor, a curious and observant
bunny rabbit. In each episode, Elinor models for
children her amazing powers of observation, her willingness
to experiment, and her positive attitude.";ns;;Yes;"Using Google’s Dialogflow engine, the agent underlying Elinor
learns to understand children’s responses both from
the pre-trained language models already built into this engine
as well as training phrases that we provide, which are
sample phrases of what children may say to respond to a
particular conversational prompt";free speech;free speech;;"Using Google’s Dialogflow engine, the agent underlying Eli
nor
learns to understand children’s responses both from
the pre-trained language models already built into this en
gine
as well as training phrases that we provide, which are
sample phrases of what children may say to respond to a
particular conversational prompt";Edutainment;"The Show Elinor Wonders Why
Elinor Wonders Why is intended to inspire young children’s
wonder and curiosity about nature and teach them the ba
sic
scientific skills to answer their own questions. The sto
ries
in this show center around Elinor, a curious and ob
servant
bunny rabbit. In each episode, Elinor models for
children her amazing powers of observation, her willingness
to experiment, and her positive attitude.";companion;equal;"The Show Elinor Wonders Why
Elinor Wonders Why is intended to inspire young children’s
wonder and curiosity about nature and teach them the ba
sic
scientific skills to answer their own questions. The sto
ries
in this show center around Elinor, a curious and ob
servant
bunny rabbit. In each episode, Elinor models for
children her amazing powers of observation, her willingness
to experiment, and her positive attitude.";ns;ns;;induce curiosity;"The Show Elinor Wonders Why
Elinor Wonders Why is intended to inspire young children’s
wonder and curiosity about nature and teach them the ba
sic
scientific skills to answer their own questions. The sto
ries
in this show center around Elinor, a curious and ob
servant
bunny rabbit. In each episode, Elinor models for
children her amazing powers of observation, her willingness
to experiment, and her positive attitude.";3,4,5,6;broad;4;broad;"We recruited five children (three girls and two boys) aged
3 to 6 to participate in the field-testing of the video (Figure
4).";ns;;5;01/Oct;"We recruited five children (three girls and two boys) aged
3 to 6 to participate in the field-testing of the video (Figure
4).";Evaluating the CA;"This Late Breaking Work presents the design
of the conversational agent and reports findings from
a field study that has proven feasibility of this approach.";system;"This Late Breaking Work presents the design
of the conversational agent and reports findings from
a field study that has proven feasibility of this approach.";feasibility;"This Late Breaking Work presents the design
of the conversational agent and reports findings from
a field study that has proven feasibility of this approach.";;;;ns;;English;"Three of these children spoke a language other than
English at home but they all attended preschools and were
thus fluent in English.";English;;
281;2020;in proceedings;OzCHI;hci;conference;USA;North America;What Are You Talking To?: Understanding Children's Perceptions of Conversational Agents;Conversational agents (CAs) available in smart phones or smart speakers play an increasingly important role in young children's technological landscapes and life worlds. While a handful of studies have documented children's natural interactions with CAs, little is known about children's perceptions of CAs. To fill this gap, we examined three- to six-year-olds' perceptions of CAs' animate/artifact domain membership and properties, as well as their justifications for these perceptions. We found that children sometimes take a more nuanced position and spontaneously attribute both artifact and animate properties to CAs or view them as neither artifacts nor animate objects. This study extends current research on children's perceptions of intelligent artifacts by adding CAs as a new genre of study and provides some underlying knowledge that may guide the development of CAs to support young children's cognitive and social development.;https://doi.org/10.1145/3313831.3376416;281.Xu.pdf;Yes;Google Home Mini;"Each child’s interaction entailed three sessions with a Google
Home Mini device and lasted approximately 40 minutes in
total.";Smart Speaker;"Each child’s interaction entailed three sessions with a Google
Home Mini device and lasted approximately 40 minutes in
total.";Fully autonomous;"In the first session, the CA asked children their age, favorite
color, and a simple animal question (i.e., which animal has
a really long neck?). The CA was programmed to repeat
children’s responses.";ns;;ns;;ns;;free speech;free speech;;"children were encouraged to freely talk
to the CA and ask the CA any questions they would like.";Entertainment;"At the beginning
of each session, the experimenter introduced the interaction
task as a game";storyteller;higher;"In the second session, the CA read a ten-minute fantasy story
and asked the children 10 story-related open-ended questions
throughout";School;school;"Each child met individually with a trained experimenter in a
designated quiet area at the child’s school.";interactive storytelling;"In the second session, the CA read a ten-minute fantasy story
and asked the children 10 story-related open-ended questions
throughout";3,4,5,6;broad;4;broad;"Our participants consisted of 28 typically developing children
between the ages of 3 and 6 recruited from preschools and
afterschool programs in a university community";TD;"Our participants consisted of 28 typically developing children
between the ages of 3 and 6 recruited from preschools and
afterschool programs in a university community";28;21-30;"Our participants consisted of 28 typically developing children
between the ages of 3 and 6 recruited from preschools and
afterschool programs in a university community";Effect with CA on xyz (could have been any CA);"we examined three- to six-year-olds’ perceptions
of CAs’ domain membership and properties, as well as their
justifications for these perceptions";system;"we examined three- to six-year-olds’ perceptions
of CAs’ domain membership and properties, as well as their
justifications for these perceptions";perceptions;"we examined three- to six-year-olds’ perceptions
of CAs’ domain membership and properties, as well as their
justifications for these perceptions";;;;ns;;English;"Nineteen children (68%)
spoke only English at home, and the rest of them were bilingual
or spoke English as a second language, but all children
possessed sufficient oral English proficiency for daily conversation.";English;;
282;2018;conference;IDC;CCI;conference;USA;North America;Children asking questions: Speech interface reformulations and personification preferences;The pervasive availability of voice assistants may support children in finding answers to informational queries by re-moving the literacy requirements of text search (e.g., typing, spelling). However, most such systems are not de-signed for the specific needs and preferences of children and may struggle with understanding the intent of their questions. In our investigation, we observed 87 children and 27 adults interacting with three Wizard-of-Oz speech inter-faces to arrive at answers to questions that required reformulation. We found that many children and some adults required help to reach an effective question reformulation. We report the common types of reformulations (both effective and ineffective ones). We also compared three versions of speech interfaces with different approaches to referring to itself (personification) and to the participant (naming personalization). We found that children preferred personified interfaces, but naming personalization did not affect preference. We connect our findings to implications for design of speech systems for families. © 2018 Association for Computing Machinery.;https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051544939&doi=10.1145%2f3202185.3202207&partnerID=40&md5=72b1b2fd21497d70065dac398bbfcb61;282.Yarosh.pdf;Yes;Fraga, Swali;"Voice Search System – the non-personified and non-personalized system never referred to itself in first per-son, gave only task-related responses, and did not men-tion the participant’s name, e.g.: “Welcome to the voice search system. Please, say your question.”
• Fraga2 – the personified and non-personalized system referred to itself in first person, gave some responses that were not task-related, but did not mention the par-ticipant’s name, e.g.: “Hello, I am Fraga. Do you have a question for me?”
• Swali2 – the personified and personalized system re-ferred to itself in first person, gave some responses that were not task-related, and periodically referred to the participant’s name and age, e.g.: “Hello Jake, I am Swali. I see you are 6 years old. That makes you 5.5 years older than me. Do you have a question for me?”";Voice Assistant;"Voice Search System – the non-personified and non-personalized system never referred to itself in first per-son, gave only task-related responses, and did not men-tion the participant’s name, e.g.: “Welcome to the voice search system. Please, say your question.”
• Fraga2 – the personified and non-personalized system referred to itself in first person, gave some responses that were not task-related, but did not mention the par-ticipant’s name, e.g.: “Hello, I am Fraga. Do you have a question for me?”
• Swali2 – the personified and personalized system re-ferred to itself in first person, gave some responses that were not task-related, and periodically referred to the participant’s name and age, e.g.: “Hello Jake, I am Swali. I see you are 6 years old. That makes you 5.5 years older than me. Do you have a question for me?”";Wizarded;In our investigation, we observed 87 children and 27 adults interacting with three Wizard-of-Oz speech inter-faces to arrive at answers to questions that required refor-mulation.;No;Each study station housed three variations of speech inter-faces, each represented as a different plastic housing and an AISBR 3W wired speaker (see Figure 1) to allow partici-pants to more easily distinguish between and refer to each interface. All participants were audio recorded using a Blue Yeti USB microphone, which contains a tri-capsule array to support field recording of human voices at a 48kHz sample rate and 16bit bit rate. All of these peripherals were con-nected to a laptop running the Wizard-of-Oz’s control soft-ware and Audacity recording software.;ns;;Wizarded;;free speech;free speech;;To focus our investigation on the role of question reformu-lations (rather than recognition factors), we chose to use a Wizard-of-Oz (WoZ) technique to simulate a voice assis-tant.;Education;"In our investigation, we observed 87 children and
27 adults interacting with three Wizard-of-Oz speech inter-
faces to arrive at answers to questions that required refor-
mulation.";assistant;lower;The questions were presented to participants on a sheet of paper and they were asked to write a response once they arrived at an answer with the voice systems;lab;lab;The study took place in the research outreach building at the Minnesota (MN) State Fair.;collaborate on an answer;The questions were presented to participants on a sheet of paper and they were asked to write a response once they arrived at an answer with the voice systems;5,6,7,8,9,10,11,12;broad;8;wide;We recruited child participants (ages 5–12),;ns;;87;81-90;In our investigation, we observed 87 children and 27 adults interacting with three Wizard-of-Oz speech inter-faces to arrive at answers to questions that required refor-mulation.;Effect with CA on xyz (could have been any CA);Our work address-es this gap by observing 87 children asking questions of three variations of Wizard-of-Oz speech interfaces. We found that children struggled with reformulating questions, with most of them requiring hints to complete the task.;personalization;Our work address-es this gap by observing 87 children asking questions of three variations of Wizard-of-Oz speech interfaces. We found that children struggled with reformulating questions, with most of them requiring hints to complete the task.;question handling;Our work address-es this gap by observing 87 children asking questions of three variations of Wizard-of-Oz speech interfaces. We found that children struggled with reformulating questions, with most of them requiring hints to complete the task.;;;;ns;;English;participants (adults and children) were either native English speakers or agreed with the statement “I am comfortable speaking English.”;English;;
290;2019;conference;CHI;hci;conference;Kazakhstan;Central Asia;Robot-Assisted Therapy for Children with Delayed Speech Development: a Pilot Study;This paper presents a study that aims to investigate the effects of Robot-Assisted Therapy (RAT) on children who have a form of verbal development retardation such as Delayed Speech Development (DSD). To this end, we developed a number of applications for a humanoid robot NAO with the aim to engage children during RAT sessions. We conducted an evaluation of these applications with DSD children who interacted with the robot on a few occasions. Our findings demonstrate the utility of such applications for the therapy of DSD children which was both engaging and entertaining. Similar approach could be utilized for the therapy of children with Autism Spectrum Disorder and Attention Deficit Hyperactivity Disorder. © 2019 IEEE.;https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078834845&doi=10.1109%2fRO-MAN46459.2019.8956257&partnerID=40&md5=0116d4a5b43cc599264bd90bc6dc061c;290.Zhanatkyzy.pdf;Yes;NAO;"To this end, we developed
a number of applications for a humanoid robot NAO with the
aim to engage children during RAT sessions.";Robot;"To this end, we developed
a number of applications for a humanoid robot NAO with the
aim to engage children during RAT sessions.";ns;;ns;;ns;;ns;;ns;ns;;;Healthcare;"This paper presents a study that aims to investigate
the effects of Robot-Assisted Therapy (RAT) on children
who have a form of verbal development retardation such as
Delayed Speech Development (DSD)";unclear;unclear;;therapy;therapy;"The study was conducted at the Republican Children’s
Rehabilitation Center in the capital of Kazakhstan, with four
children who are diagnosed with DSD";encourage communication;"The robot-assisted therapy was conducted among the children
with DSD. The therapy was aimed to explore whether
robot applications could be effective to encourage speech
and communication with the robot.";4,5,6;young;3;narrow;"The age range of
children varied between 4 and 6 years old and all of them
were males.";delayed speech development;"This paper presents a study that aims to investigate
the effects of Robot-Assisted Therapy (RAT) on children
who have a form of verbal development retardation such as
Delayed Speech Development (DSD)";4;01/Oct;"The study was conducted at the Republican Children’s
Rehabilitation Center in the capital of Kazakhstan, with four
children who are diagnosed with DSD";Effect with CA on xyz (could have been any CA);"The robot-assisted therapy was conducted among the children
with DSD. The therapy was aimed to explore whether
robot applications could be effective to encourage speech
and communication with the robot.";system;"The robot-assisted therapy was conducted among the chil
dren
with DSD. The therapy was aimed to explore whether
robot applications could be effective to encourage speech
and communication with the robot.";communication;"The robot-assisted therapy was conducted among the chil
dren
with DSD. The therapy was aimed to explore whether
robot applications could be effective to encourage speech
and communication with the robot.";;;;ns;;Kazakh, Russian;Kazakh and Russian;Other;;
291;2020;in proceedings;HRI;robot/agent;conference;Kazakhstan;Central Asia;Quantitative Results of Robot-Assisted Therapy for Children with Autism, ADHD and Delayed Speech Development;This paper presents an ongoing work that aims to provide a quantitative analysis of a large clinical study that was conducted with 21 children aged 4-8 years old. Children were diagnosed with various forms of Autism Spectrum Disorder (ASD), Attention Deficit Hyperactivity Disorder (ADHD) or Delayed Speech Development (DSD) with autistic traits. Each child participated in four to six Robot-Assisted Therapy (RPT) sessions that lasted for fifteen minutes each. We manually video-coded the videos from the sessions to find behavioral patterns, engagement and valence scores, and we are now in the process of statistical data analysis to understand whether children's exposure to a robot had a significant effect over time.;https://doi.org/10.1145/3371382.3378254;291.Zhanatkyzy.pdf;Yes;NAO;implemented on a humanoid NAO robot.;Robot;implemented on a humanoid NAO robot.;ns;;ns;;ns;;ns;;ns;ns;;;Healthcare;"Each child participated in four to six
Robot-Assisted Therapy (RPT) sessions that lasted for fifteen minutes
each.";unclear;unclear;;therapy;therapy;"Each child participated in four to six
Robot-Assisted Therapy (RPT) sessions that lasted for fifteen minutes
each.";interact;"During each session children were offered to interact with a
robot through robot applications such as “Follow Me”, “Touch Me”,
“Dance with Me”, ""Transport"", ""Animals"", ""Emotions"", ""Storytelling"",
and others";4,5,6,7,8;broad;5;broad;"This paper presents an ongoing work that aims to provide a quantitative
analysis of a large clinical study that was conducted with
21 children aged 4-8 years old.";autistic, ADHD, DSD;"Children were diagnosed with various
forms of Autism Spectrum Disorder (ASD), Attention Deficit
Hyperactivity Disorder (ADHD) or Delayed Speech Development
(DSD) with autistic traits.";21;21-30;"This paper presents an ongoing work that aims to provide a quantitative
analysis of a large clinical study that was conducted with
21 children aged 4-8 years old.";Effect with CA on xyz (could have been any CA);"This paper presents an ongoing work that aims to provide a quantitative
analysis of a large clinical study that was conducted with
21 children aged 4-8 years old. Children were diagnosed with various
forms of Autism Spectrum Disorder (ASD), Attention Deficit
Hyperactivity Disorder (ADHD) or Delayed Speech Development
(DSD) with autistic traits. Each child participated in four to six
Robot-Assisted Therapy (RPT) sessions that lasted for fifteen minutes
each. We manually video-coded the videos from the sessions
to find behavioral patterns, engagement and valence scores";system;"We manually video-coded the videos from the sessions
to find behavioral patterns, engagement and valence scores,";behavioral patterns, engagement;"We manually video-coded the videos from the sessions
to find behavioral patterns, engagement and valence scores,";;;;ns;;ns;;ns;;
168;2010;conference;RO-MAN;robot/agent;conference;Netherlands;Europe;Using child-robot interaction to investigate the user acceptance of constrained and artificial languages;The possibility of improving speech recognition accuracy within human computer and robot interaction by having users interact in a constrained natural language or an artificial language has been explored. However, what has not been evaluated yet is the user acceptance of such forms of interaction. In this paper we discuss two separate but similar studies which were aimed at assessing the usability of constrained and artificial languages in contrast to natural languages. The interaction context was implemented in a game played between children and the iCat robot. We subjectively measured various variables related to gaming experience and interacting in the new languages. Our results reveal that there were no significant differences in the user experience across the two interaction mediums in comparison to natural languages. © 2010 IEEE.;https://www.scopus.com/inward/record.uri?eid=2-s2.0-78649825177&doi=10.1109%2fROMAN.2010.5598731&partnerID=40&md5=f7652aed6110d0d12202b5a36438307f;168.Mubin.pdf;Yes;iCat;"The interaction context was implemented in a game
played between children and the iCat robot.";Robot;"The interaction context was implemented in a game
played between children and the iCat robot.";Wizarded;"We used a Wizard of Oz method to
simulate both the verbal and non verbal behavior of the
iCat.";No;"We used the iCat robot [15] as the gaming partner of the
children. The iCat robot is a cat like robot by Philips that
has the ability to communicate to users via both verbal and
non-verbal means such as by exhibiting facial expressions.";No;"Dutch text to speech engine was also employed
in order to elicit the responses of the iCat.";Wizarded;;limited phrases;limited;15;"With these aspects in mind we composed a
constrained language consisting of fifteen permissible
commands (see Table 1). When designing the constrained
language, we also kept the difficulties in mind that speech
recognizers would experience. For instance, the commands
were designed such that they would have the least confusion
amongst themselves. Moreover we used as few words as
possible with which every sentence could be fully
informative. Prior to interacting with the iCat, the children
were given three minutes to study the constrained language
so that they could recall and use it while playing the game.";Entertainment;"To win a particular game, the child was required to
guess every number correctly. The child played seven
rounds of this game and was encouraged to discuss every
guess with the iCat.";companion;equal;"We used the iCat robot [15] as the gaming partner of the
children.";lab;lab;"The children were seated on a bench, which was placed
in front of a table with a computer screen on it. As shown in
figure 2, children sat beside the iCat.";discuss and interact;to learn new languages to interact with it.;9,10,11,12;broad;4;broad;"92 children took part in this study, that were between 9
and 12 years old. From them, 52 took part in the natural
language condition (Dutch); the other 40 conversed with the
iCat in constrained Dutch. We balanced gender in both
conditions.";ns;;92;91-100;"92 children took part in this study, that were between 9
and 12 years old. From them, 52 took part in the natural
language condition (Dutch); the other 40 conversed with the
iCat in constrained Dutch. We balanced gender in both
conditions.";Effect with CA on xyz (could have been any CA);"the primary goal of this research was to
evaluate whether children are comfortable with using
constrained or artificial languages in comparison to natural
languages and whether they are willing to invest some effort
in learning such languages.";language;"In this paper we have presented an evaluation of how
children interact with a robot using constrained and
artificial languages. We compared these two languages with
natural languages (the native language of participants) and
showed that the children as users were comfortable in
communicating with a robot using both languages.";likability, cognitive demand, game engagement, game endurability;"In this paper we have presented an evaluation of how
children interact with a robot using constrained and
artificial languages. We compared these two languages with
natural languages (the native language of participants) and
showed that the children as users were comfortable in
communicating with a robot using both languages.";;;;ns;;Dutch;"The first study was carried out in Tilburg, the
Netherlands, where children either collaborated in their
native language Dutch or in a restricted set of Dutch
utterances that was suitable for the communication purpose
(i.e. collaborating in a game).";Other;CaseI;
187;2017;conference;ICRA;robot/agent;conference;USA, Italy;Combination;Backchannel opportunity prediction for social robot listeners;This paper investigates how a robot that can produce contingent listener response, i.e., backchannel, can deeply engage children as a storyteller. We propose a backchannel opportunity prediction (BOP) model trained from a dataset of children's dyad storytelling and listening activities. Using this dataset, we gain better understanding of what speaker cues children can decode to find backchannel timing, and what type of nonverbal behaviors they produce to indicate engagement status as a listener. Applying our BOP model, we conducted two studies, within- and between-subjects, using our social robot platform, Tega. Behavioral and self-reported analyses from the two studies consistently suggest that children are more engaged with a contingent backchanneling robot listener. Children perceived the contingent robot as more attentive and more interested in their story compared to a non-contingent robot. We find that children significantly gaze more at the contingent robot while storytelling and speak more with higher energy to a contingent robot. © 2017 IEEE.;https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027981344&doi=10.1109%2fICRA.2017.7989266&partnerID=40&md5=2f358373ccadfc87fdd7c07df88a1740;187.Park.pdf;Yes;Tega;"Tega is a social robot platform developed to support longterm
deployment in homes and schools (Fig. 2).";Robot;"Tega is a social robot platform developed to support longterm
deployment in homes and schools (Fig. 2).";Fully autonomous;"We presented a method to developing a robot listener
that autonomously and actively responds to child speaker’s
prosodic cues.";Yes;"Tega is a social robot platform developed to support longterm
deployment in homes and schools (Fig. 2).";Yes;"For each storytelling episode, the nonverbal behaviors of
both the listener and storyteller were manually coded using
a video-annotation software by four coders (Fig. 4). Three
additional coders were recruited to simulate themselves being
a listener and mark the moments when they wanted to BC.
After this simulation, coders reviewed the audio snippets
surrounding these moments to further categorize the type
of speaker cues perceived (pitch, energy, pause, filled pause,
long utterance, clause ending, other).
From the analysis of this dataset, the most observed
backchannel responses were gaze at partner, lean toward,
brow raise, smile, nod, and short utterances. Our robot
listener implements these nonverbal behaviors to signal its
engagement to the child speaker";ns;;free speech;free speech;;"When the participant told the robot(s) that
he/she is here to tell them stories, the Tega robot(s)
woke up and started backchanneling to the child’s
speech. When the child indicated an end of a story,
the robot(s) went back to sleep, and the experimenter
conducted a post survey.";Entertainment;"When the participant told the robot(s) that
he/she is here to tell them stories, the Tega robot(s)
woke up and started backchanneling to the child’s
speech. When the child indicated an end of a story,
the robot(s) went back to sleep, and the experimenter
conducted a post survey.";listener;lower;"When the participant told the robot(s) that
he/she is here to tell them stories, the Tega robot(s)
woke up and started backchanneling to the child’s
speech. When the child indicated an end of a story,
the robot(s) went back to sleep, and the experimenter
conducted a post survey.";ns;ns;;listening;"When the participant told the robot(s) that
he/she is here to tell them stories, the Tega robot(s)
woke up and started backchanneling to the child’s
speech. When the child indicated an end of a story,
the robot(s) went back to sleep, and the experimenter
conducted a post survey.";4,5,6,7,8;broad;5;broad;"In the within-subjects study, 23 children (age M = 6.13,
SD = 1.36; 43.5% female) between the age of 4–8 years old
were recruited through a local parents’ mailinglist";ns;;23;21-30;"In the within-subjects study, 23 children (age M = 6.13,
SD = 1.36; 43.5% female) between the age of 4–8 years old
were recruited through a local parents’ mailinglist";Effect with CA on xyz (could have been any CA);"In the within-subjects
study, children told stories to both contingent and non-contingent BC Tega
robots at the same time";contingency;"In the within-subjects
study, children told stories to both contingent and non-contingent BC Tega
robots at the same time";interest, likability;"Afterwards in the within-subjects study,
children were asked about the level of interest each
robot showed toward their storyUsing a 5-point smileyometer Likert scale,
likeability of the robots (how much did you like Tega?)
and enjoyability of the storytelling task (how much did
you like telling stories to Tega?) were first asked in
both studies. Afterwards in the within-subjects study,
children were asked about the level of interest each
robot showed toward their story";;;;ns;;ns;;ns;within-study;
301;2021;conference;IDC;CCI;conference;USA;North America;“Alexa, What is That Sound?” A Video Analysis of Child-Agent Communication From Two Amazon Alexa Games;The rapid adoption of smart home speakers into households has enabled young children to verbally communicate with voice assistants (VAs). VAs, such as Amazon Alexa, allow children to use their voice to play various game-based activities and offer opportunities to understand children's communication during child-agent communication. This qualitative study describes findings from video analysis of young children between three and six years old who engaged in voice-based gameplay with two commercial Alexa Skills, Animal Sounds and Animal Game. We report findings about verbal behaviors and communication breakdowns and repairs between children and Alexa, and discuss key considerations for designing voice games for children.;https://dl.acm.org/doi/10.1145/3459990.3465195;301.Du;Yes;Alexa;"To investigate how children verbally interact with Alexa in the
context of games, we recruited a total of 16 children who met the
following criteria: (1) be between ages three years zero month and
six years 11 months old; (2) have no history of physical, cognitive,
or communication disorders in speech, language, and voice; and
(3) have prior experience to comfortably use VAs on smartphones
and/or smart home speakers.";Voice Assistant;"To investigate how children verbally interact with Alexa in the
context of games, we recruited a total of 16 children who met the
following criteria: (1) be between ages three years zero month and
six years 11 months old; (2) have no history of physical, cognitive,
or communication disorders in speech, language, and voice; and
(3) have prior experience to comfortably use VAs on smartphones
and/or smart home speakers.";Fully autonomous;"Children interacted with the two Alexa Skills via the Alexa app
on an iPhone 6 mobile device. In order to start the game, children
were given instruction to verbally enact invocations: “Alexa, play
Animal Sounds” and “Alexa, play Animal Game.”";No;"Children interacted with the two Alexa Skills via the Alexa app
on an iPhone 6 mobile device. In order to start the game, children
were given instruction to verbally enact invocations: “Alexa, play
Animal Sounds” and “Alexa, play Animal Game.”";Yes;"Through examining children’s verbal behaviors, children’s
communication repair strategies, Alexa’s response accuracy, and
Alexa’s response error breakdowns, our data indicated that not
only did children experience communication breakdowns and em
ploy
repair strategies, Alexa game designers and developers also
actively created fallback mechanisms to prevent such breakdowns
to support children’s communication during the gameplay expe
rience.";Yes;"Through examining children’s verbal behaviors, children’s
communication repair strategies, Alexa’s response accuracy, and
Alexa’s response error breakdowns, our data indicated that not
only did children experience communication breakdowns and em
ploy
repair strategies, Alexa game designers and developers also
actively created fallback mechanisms to prevent such breakdowns
to support children’s communication during the gameplay expe
rience.";free speech;free speech;;"While interacting
with the game Animal Sounds, children utilized different
verbal expressions to name animals based on animal sounds provided
by Alexa. Specifically, children demonstrated multiple verbal
behaviors, including saying Alexa commands (ACs), commenting
about the game, and clarifying previous responses";Entertainment;"To investigate how children verbally interact with Alexa in the
context of games, we recruited a total of 16 children who met the
following criteria: (1) be between ages three years zero month and
six years 11 months old; (2) have no history of physical, cognitive,
or communication disorders in speech, language, and voice; and
(3) have prior experience to comfortably use VAs on smartphones
and/or smart home speakers.";assistant;lower;"In this study, we collected in-home videos and conducted qualitative
analysis to understand how children communicate with
commercially available technologies like Amazon Alexa using two
games.";home;home;"The study occurred
at children’s homes with at least one adult caregiver present and
completed questionnaires for their demographic information";game player;"In this study, we collected in-home videos and conducted qualitative
analysis to understand how children communicate with
commercially available technologies like Amazon Alexa using two
games.";3,4,5,6;broad;4;broad;"To investigate how children verbally interact with Alexa in the
context of games, we recruited a total of 16 children who met the
following criteria: (1) be between ages three years zero month and
six years 11 months old; (2) have no history of physical, cognitive,
or communication disorders in speech, language, and voice; and
(3) have prior experience to comfortably use VAs on smartphones
and/or smart home speakers.";TD;"To investigate how children verbally interact with Alexa in the
context of games, we recruited a total of 16 children who met the
following criteria: (1) be between ages three years zero month and
six years 11 months old; (2) have no history of physical, cognitive,
or communication disorders in speech, language, and voice; and
(3) have prior experience to comfortably use VAs on smartphones
and/or smart home speakers.";16;Nov/20;"To investigate how children verbally interact with Alexa in the
context of games, we recruited a total of 16 children who met the
following criteria: (1) be between ages three years zero month and
six years 11 months old; (2) have no history of physical, cognitive,
or communication disorders in speech, language, and voice; and
(3) have prior experience to comfortably use VAs on smartphones
and/or smart home speakers.";Effect of CA on xyz;"In this study, we collected in-home videos and conducted qualitative
analysis to understand how children communicate with
commercially available technologies like Amazon Alexa using two
games.";Alexa's games;"Through examining children’s verbal behaviors, children’s
communication repair strategies, Alexa’s response accuracy, and
Alexa’s response error breakdowns, our data indicated that not
only did children experience communication breakdowns and employ
repair strategies, Alexa game designers and developers also
actively created fallback mechanisms to prevent such breakdowns
to support children’s communication during the gameplay experience.";verbal behaviors, repair strategies, response accuracy, error breakdowns;"Through examining children’s verbal behaviors, children’s
communication repair strategies, Alexa’s response accuracy, and
Alexa’s response error breakdowns, our data indicated that not
only did children experience communication breakdowns and employ
repair strategies, Alexa game designers and developers also
actively created fallback mechanisms to prevent such breakdowns
to support children’s communication during the gameplay experience.";;;;ns;;ns;;ns;;
304;2023;article;Computers in Human Behavior;hci;journal;Germany, Switzerland;Europe;How children speak with their voice assistant Sila depends on what they think about her;Children encounter artificial intelligence, e.g., voice assistants, at an increasingly younger age. However, we know far too little about children's communicative behavior with a voice-only interaction partner and whether and how their assumptions about the interaction partner predict how they communicate. To address these aspects, we ran a treasure hunt game with 50 5-6-year-olds who were guided through the hunt by a voice-only interaction partner “Sila” – acted by a human and connected via loudspeakers. Crucially, Sila was introduced either as a human or a voice assistant. We employed easy communication trials in which communication went smoothly and challenging communication trials that contained staged misunderstandings. Results showed that at the group level, children differed in the way they adjusted their speech across experimental phases – both in quantity (fewer grounding turns and fewer syllables in speech directed to voice assistants) and quality (reduced increase in f0 and intensity in speech directed to voice assistants). Notably, the communicative behavior was predicted by children's individual parasocial relationship with their voice-only interaction partner. We discuss the relevance of our findings for developmental science at the interface with communication science and phonetics, and the direct practical implications we can infer for speech technology. © 2023 Elsevier Ltd;https://www.sciencedirect.com/science/article/pii/S0747563223000444;304.Gampe;Yes;Sila;"50 5-6-year-olds who were guided through the hunt by a voice-only
interaction partner “Sila”";Voice Assistant;"50 5-6-year-olds who were guided through the hunt by a voice-only
interaction partner “Sila”";Wizarded;acted by a human and connected via loudspeakers;ns;;No;acted by a human and connected via loudspeakers;Wizarded;;limited phrases;limited;ns;"Each trial followed the same
pattern. Each time the child named the picture on the key, Sila uttered
the following components, always in the same order: She named the
object on the child’s key (key-check), gave information about the object (key-description), and mentioned which cardboard box had to be found
next and asked if the child could find it (next-box). Once the child put
the key in the cash box, she asked what object was on the key this time
(key-object). There was only one form of additional communication that
did not fit this pattern: some children answered the next-box question (e.
g., by repeating that they have found the key) and wanted Sila to give
them permission. In this case, Sila said “When you have found it, you
may open it” (permission), see Table A4 in the Appendix for an overview
of the information on the key descriptions.";Entertainment;"The experiment was set up as a treasure hunt, i.e., a naturalistic
setting. The treasure hunt task with Sila was an adapted version of the
treasure hunt task developed by Aeschlimann et al. (2020).";assistant;lower;"50 5-6-year-olds who were guided through the hunt by a voice-only
interaction partner “Sila”";ns;ns;;guide;"50 5-6-year-olds who were guided through the hunt by a voice-only
interaction partner “Sila”";5,6;young;2;narrow;"We included 50 5-6-year-old Swiss German-speaking children, 25 in
each of the two conditions (human vs. voice assistants);";ns;;50;41-50;"We included 50 5-6-year-old Swiss German-speaking children, 25 in
each of the two conditions (human vs. voice assistants);";Effect with CA on xyz (could have been any CA);"we know far too little about how children
communicate with AI-based tools. In particular, how do they talk to
digital interaction partners, and do they differentiate between humans
and AI-based interaction partners in digital communication? Also, do
children’s assumptions about their digital communication partners influence
how they interact with them?";voice agent or human;"RQ1: Do children speak differently to humans and voice assistants in
digital voice-only communication?
• RQ2: Do children’s assumptions about the interaction partner predict
the way they communicate in digital voice-only
communication?";effort to communicate, speaking style;"RQ1: Do children speak differently to humans and voice assistants in
digital voice-only communication?
• RQ2: Do children’s assumptions about the interaction partner predict
the way they communicate in digital voice-only
communication?";;;;;;German;"Swiss German-speaking children,We included 50 5-6-year-old Swiss German-speaking children, 25 in
each of the two conditions (human vs. voice assistants);";Other;not really speech tech since voice is real-life person through loudspeakers;
307;2022;article;International Journal of Child-Computer Interaction;CCI;journal;USA;North America;Examining voice assistants in the context of children’s speech;An estimated 3.25 billion voice assistants (VAs) are in homes around the world, but these devices are not always able to recognize and respond to children's speech. To inform the design of VAs that support kids, we report on a lab study where 28 5- to 10-year-old participants interacted with a commercial VA to: (1) attempt to execute common VA-supported requests (such as setting an alarm), (2) recite a set of such scripts verbatim, and (3) engage in unstructured conversation. We find that devices only respond appropriately to the content of children's speech half of the time. Frequency of appropriate responses increased with children's age and as their discourse became more standardized. Based on themes in participants’ speech, we identify design opportunities in child-VA interaction, such as exploring a topic or responding to a conversational bid. In addition to our empirical findings, we contribute a structured corpus of children's speech. © 2022 Elsevier B.V.;https://www.sciencedirect.com/science/article/pii/S2212868922000587;307.Kim;Yes;Amazon Echo;"engaging in free-form
conversation with a specific VA (the Amazon Echo)";Smart Speaker;"engaging in free-form
conversation with a specific VA (the Amazon Echo)";Fully autonomous;;No;"We selected
the Echo as a representative example of a current (at the time
of this study) VA, and one in which there is no accompanying
screen-based interaction, which would likely require (and allow)
different interaction design decision";ns;;ns;;free speech;free speech;;"engaging in free-form
conversation with a specific VA (the Amazon Echo)";Generic;"we report on a lab study where 28 5- to 10-year-old participants interacted with a
commercial VA to: (1) attempt to execute common VA-supported requests (such as setting an alarm),
(2) recite a set of such scripts verbatim, and (3) engage in unstructured conversation.";assistant;lower;"we report on a lab study where 28 5- to 10-year-old participants interacted with a
commercial VA to: (1) attempt to execute common VA-supported requests (such as setting an alarm),
(2) recite a set of such scripts verbatim, and (3) engage in unstructured conversation.";lab;lab;"we report on a lab study where 28 5- to 10-year-old participants interacted with a
commercial VA to: (1) attempt to execute common VA-supported requests (such as setting an alarm),
(2) recite a set of such scripts verbatim, and (3) engage in unstructured conversation.";assist;"we report on a lab study where 28 5- to 10-year-old participants interacted with a
commercial VA to: (1) attempt to execute common VA-supported requests (such as setting an alarm),
(2) recite a set of such scripts verbatim, and (3) engage in unstructured conversation.";5,6,7,8,9,10;broad;6;wide;"we report on a lab study where 28 5- to 10-year-old participants interacted with a
commercial VA to: (1) attempt to execute common VA-supported requests (such as setting an alarm),
(2) recite a set of such scripts verbatim, and (3) engage in unstructured conversation.";ns;;28;21-30;"we report on a lab study where 28 5- to 10-year-old participants interacted with a
commercial VA to: (1) attempt to execute common VA-supported requests (such as setting an alarm),
(2) recite a set of such scripts verbatim, and (3) engage in unstructured conversation.";Effect with CA on xyz (could have been any CA);"To inform the design of VAs that
support kids";System;"We saw a number of themes in the ways children constructed
VA-directed utterances and in the ways the VA responded to this
input. Consistent with prior work (Lovato et al., 2019), we found
that children’s conversational speech was transcribed accurately
more than 84% of the time.";speech patterns, errors;"We saw a number of themes in the ways children constructed
VA-directed utterances and in the ways the VA responded to this
input. Consistent with prior work (Lovato et al., 2019), we found
that children’s conversational speech was transcribed accurately
more than 84% of the time.";;;;ns;;ns;;ns;;
309;2022;article;Frontiers in Robotics and AI;robot/agent;journal;Netherlands;Europe;Getting acquainted: First steps for child-robot relationship formation;In this article we discuss two studies of children getting acquainted with an autonomous socially assistive robot. The success of the first encounter is key for a sustainable long-term supportive relationship. We provide four validated behavior design elements that enable the robot to robustly get acquainted with the child. The first are five conversational patterns that allow children to comfortably self-disclose to the robot. The second is a reciprocation strategy that enables the robot to adequately respond to the children’s self-disclosures. The third is a ‘how to talk to me’ tutorial. The fourth is a personality profile for the robot that creates more rapport and comfort between the child and the robot. The designs were validated with two user studies ( <italic>N</italic> <sub>1</sub> = 30, <italic>N</italic> <sub>2</sub> = 75, 8–11 years. o. children). The results furthermore showed similarities between how children form relationships with people and how children form relationships with robots. Most importantly, self-disclosure, and specifically how intimate the self-disclosures are, is an important predictor for the success of child-robot relationship formation. Speech recognition errors reduces the intimacy and feeling similar to the robot increases the intimacy of self-disclosures.;https://www.frontiersin.org/articles/10.3389/frobt.2022.853665/full;309.Ligthart;Yes;NAO;"An orange V4 Nao robot was used with its default speech
recognition software.";Robot;"An orange V4 Nao robot was used with its default speech
recognition software.";ns;;ns;;ns;;No;"An orange V4 Nao robot was used with its default speech
recognition software.";free speech;free speech;;"For each topic
the robot would, using conversational patterns, ask the
participant questions (see Section 3.3 for an example),
typically about their favorite sports, leisure activity, etc. After
the conversation the robot would initiate the second activity
and after that say goodbye.";Generic;"In this article we discuss two studies of children getting acquainted with an
autonomous socially assistive robot.";companion;equal;"We are, for example, developing a social robot companion (a
Nao robot we call Hero) for pediatric oncology patients";school;school;"The experiment took place in two rooms at the school
familiar to the participants.";companion;"We are, for example, developing a social robot companion (a
Nao robot we call Hero) for pediatric oncology patients";8,9,10,11;broad;4;broad;"75 children, between 8 and 11 years old, of two Dutch
primary schools (school A and B) completed the experiment";ns;;75;71-80;"75 children, between 8 and 11 years old, of two Dutch
primary schools (school A and B) completed the experiment";Effect with CA on xyz (could have been any CA);"We used a 2 _ 2 between-subjects study design to research the
effect of (mis)matching the arousal behavioral profile of the robot with the extraversion level of the children on self-disclosure
(RQ2.2) and relationship formation (RQ2.3).";arousal behavior, extraversion;"The two
independent variables are the extraversion of the child
(introvert versus extrovert) and the behavior profile of the
robot (high or low arousal).";self-disclosure, positive affect change;"The three dependent variables are
the amount and the intimacy of self-disclosure and positive affect
change.";;;;ns;;Dutch;"75 children, between 8 and 11 years old, of two Dutch
primary schools (school A and B) completed the experiment";Other;2 user studies, annotations on 2nd study with most children;
311;2023;article;Early Childhood Research Quarterly;misc;journal;Australia;Oceania;Preschool children's engagement with a social robot compared to a human instructor;Young children are exposed to new digital technologies, such as social robots. Limited research exists on how children engage with social robots and their role in the preschool classroom. This study observed 35 English-speaking children (M age = 4.60 years) participating in two tasks (Simon Says and iPad drawing) under the guidance of a social robot or human instructor. Children's engagement was measured across behavioural, emotional, and verbal domains. Results showed higher behavioural and positive emotional engagement with a human instructor than a social robot instructor across both tasks. Children uttered more words with the human instructor than the social robot instructor for the iPad drawing task, although there were no differences for the Simon Says task. Children's engagement and utterances were positively correlated during the social robot and human instructor conditions. The findings suggest that child engagement was overall higher with a human than with a social robot and such differences may be important for teachers to consider when using social robots in the preschool classroom. © 2023 The Author(s);https://www.sciencedirect.com/science/article/pii/S0885200623000959;311.Neumann;Yes;NAO;"The NAO humanoid social robot (version NO5) was designed to
interact with children, and this robot has a pre-programmed voice and
25 degrees of freedom to perform a range of physical actions, including
arm and leg movements.";Robot;"The NAO humanoid social robot (version NO5) was designed to
interact with children, and this robot has a pre-programmed voice and
25 degrees of freedom to perform a range of physical actions, including
arm and leg movements.";Fully autonomous;"The robot’s
physical movements, gestures, and speech were coded using the software Choregraphe Suite (2.1).";Yes;"The NAO humanoid social robot (version NO5) was designed to
interact with children, and this robot has a pre-programmed voice and
25 degrees of freedom to perform a range of physical actions, including
arm and leg movements.";ns;;ns;;free speech;free speech;;"Children’s behavioural
engagement (child responses required for a direct question or
request to participate in an action or task), utterances, and emotional
engagement were coded for the Simon Says and iPad Drawing phases
and used for statistical analyses. Behaviours were measured at the onset
of the segment of speech by the instructor and ended at the next
segment.";Education;"Simon Says and drawing tasks are common preschool activities
for young children (Fridin, 2014; Neumann, 2018).";instructor;higher;"Each
child participated in the social robot instructor and human instructor
conditions in a counterbalanced order to avoid sequence effects";preschool;school;"Each session was conducted
in a quiet room at the child’s preschool, and the child sat on the
floor facing the social robot or human instructor.";instructing;"Each session was conducted
in a quiet room at the child’s preschool, and the child sat on the
floor facing the social robot or human instructor.";3,4,5;young;3;narrow;"The final sample consisted of 35 children (male = 20, female = 15) aged
between 3 and 5 years (M = 4.60; SD = 0.55).";ns;;35;31-40;"The final sample consisted of 35 children (male = 20, female = 15) aged
between 3 and 5 years (M = 4.60; SD = 0.55).";Effect with CA on xyz (could have been any CA);"The present
study investigated children’s engagement with a social robot through a
comparison with a human instructor during two preschool tasks (Simon
Says and iPad drawing).";system;"The present
study investigated children’s engagement with a social robot through a
comparison with a human instructor during two preschool tasks (Simon
Says and iPad drawing).";engagement;"The present
study investigated children’s engagement with a social robot through a
comparison with a human instructor during two preschool tasks (Simon
Says and iPad drawing).";;;;ns;;English;"""The present
study investigated children’s engagement with a social robot through a
comparison with a human instructor during two preschool tasks (Simon
Says and iPad drawing).""";English;;
312;2022;article;European Early Childhood Education Research Journal;misc;journal;Australia;Oceania;Young children’s interactions with a social robot during a drawing task;"Social robots communicate through human-like behaviours and are being used in preschool settings. However, the ways in which young children interact with social robots and how these impact upon early educational experiences are not fully known. This observational study explored preschoolers’ (Mean age = 4.58 years; N = 40) interactions with a social robot called NAO during a drawing activity, in South East Queensland, Australia. Most children (83%) engaged in the drawing activity and 60% talked to the social robot. A smaller proportion (10%) of children did not talk to the social robot or attempt the drawing task. Overall, the social robot successfully engaged and instructed preschool children during the drawing task however, not all children interacted with the social robot. It is recommended that a thoughtful approach to introducing social robots to preschools is needed to meet the individual and personal learning needs of young children. © 2022 EECERA.";https://www.tandfonline.com/doi/full/10.1080/1350293X.2022.2116653;312.Neumann;Yes;NAO;"The NAO humanoid social robot (version NO5) has been designed to interact with children
(Tolksdorf et al. 2021b).";Robot;"The NAO humanoid social robot (version NO5) has been designed to interact with children
(Tolksdorf et al. 2021b).";Fully autonomous;"The robot’s physical movements,
gestures, and speech were programmed and coded using Choregraphe Suite";Yes;"The robot’s physical movements,
gestures, and speech were programmed and coded using Choregraphe Suite";ns;;ns;;free speech;free speech;;"To analyse child talk, the total number of utterances spoken by the child (Crystal 2008;
Neumann and Neumann 2016) was measured. The number of words (e.g. dog) or filler
words (e.g. umm) a child produced when talking with the social robot during the interactions
was counted (Wood et al. 2013)";Education;Drawing is a popular activity for children in their preschool classrooms (OECD 2021).;instructor;higher;"This task involved the child listening to the social robot’s instructions and drawing a
picture of a dog on an iPad then writing their name next to their drawing.";preschool;school;"In a quiet room at the child’s preschool each child was invited
to sit on the floor in front of the social robot and participate in the drawing activity
with the robot.";instructing;"the instructions and questions programmed in the present study
were the same for each child";3,4,5;young;3;narrow;"Participants were 40 typically developing English-speaking children aged 3–5 years old,
with a Mean age of 4.58 years (SD = .55)";ns;;40;31-40;"Participants were 40 typically developing English-speaking children aged 3–5 years old,
with a Mean age of 4.58 years (SD = .55)";Effect with CA on xyz (could have been any CA);"This was an exploratory study that examined preschool children’s interactions with a
social robot during a drawing task";system;"Some children talked with the social robot but did not attempt
the drawing task and other children talked with the robot and completed the drawing
task";response;"Some children talked with the social robot but did not attempt
the drawing task and other children talked with the robot and completed the drawing
task";;;;ns;;English;"Participants were 40 typically developing English-speaking children aged 3–5 years old,
with a Mean age of 4.58 years (SD = .55)";English;;
313;2021;article;Human Behavior and Emerging Technologies;hci;journal;Germany;Europe;“Alexa, let me ask you something different” Children's adaptive information search with voice assistants;In this study, we investigated how children interact with voice assistants, particularly focusing on what kinds of questions they ask and how they react to the responses obtained. We recorded 3- to 10-year-old children's (N = 43) spontaneous interactions with Amazon Alexa, and analyzed the questions they asked, as well as how they adjusted their information search based on the responses received. Our results confirm previous work in showing that children's questions are mostly information-seeking, yet the type of questions children ask also depends on their age and familiarity with voice assistants. For example, children who are younger and less familiar with voice assistants are more likely to ask questions about themselves and their environment (e.g., “What is my sister's name?”). We also show for the first time that, even though all children are sensitive to the relevance and accuracy of voice assistants' responses to a certain extent, older children are more likely to change the topic and type of the questions asked upon receiving irrelevant or uninformative responses. This study shows that, with age and familiarity, children become more sensitive to the behavior, informativeness, and constraints of artificial agents, growing into adaptive and sophisticated technology users. © 2021 The Authors. Human Behavior and Emerging Technologies published by Wiley Periodicals LLC.;https://onlinelibrary.wiley.com/doi/10.1002/hbe2.270;313.Oranc;Yes;Amazon Alexa;"We recorded 3- to 10-year-old children's (N = 43) spontaneous interactions
with Amazon Alexa, and analyzed the questions they asked, as well as how they
adjusted their information search based on the responses received.";Smart Speaker;"We recorded 3- to 10-year-old children's (N = 43) spontaneous interactions
with Amazon Alexa, and analyzed the questions they asked, as well as how they
adjusted their information search based on the responses received.";Fully autonomous;"We recorded 3- to 10-year-old children's (N = 43) spontaneous interactions
with Amazon Alexa, and analyzed the questions they asked, as well as how they
adjusted their information search based on the responses received.";ns;;ns;;ns;;free speech;free speech;;"The experiment consisted of two phases. In the Q&A phase, the
experimenter introduced Alexa to the child as “Alexa here is a voice
assistant. You can ask her questions and she will answer” and asked
“Alexa, can you hear me?” to demonstrate how it worked. The experimenter
then asked the child if they would like to talk to Alexa, and
invited them to ask questions if the answer was positive. If the child
hesitated to initiate the conversation, the experimenter encouraged
them by saying “Why don't you just try and say something?”";Generic;"In the new digital era, voice assistants (VAs) are becoming ubiquitous
in children's daily lives. Parents consult voice assistants such as Apple
Siri and Amazon Alexa on topics such as weather conditions for a family
trip, or directions to a birthday party.";assistant;lower;"To our knowledge, this is the first study examining the adaptiveness
of children's inquiry behavior with VAs. That is, it is the first study to
explore whether and how children adapt the type of questions asked to
the answers obtained by the virtual conversational partner";zoo;public space;The study was conducted in a designated testing area at the zoo.;answering questions;" The experiment consisted of two phases. In the Q&A phase, the
experimenter introduced Alexa to the child as “Alexa here is a voice
assistant. You can ask her questions and she will answer” and asked
“Alexa, can you hear me?” to demonstrate how it worked. The experimenter
then asked the child if they would like to talk to Alexa, and
invited them to ask questions if the answer was positive. If the child
hesitated to initiate the conversation, the experimenter encouraged
them by saying “Why don't you just try and say something?”";3,4,5,6,7,8,9,10;broad;8;wide;"""We recorded 3- to 10-year-old children's (N = 43) spontaneous interactions
with Amazon Alexa, and analyzed the questions they asked, as well as how they
adjusted their information search based on the responses received.""";ns;;43;41-50;"We recorded 3- to 10-year-old children's (N = 43) spontaneous interactions
with Amazon Alexa, and analyzed the questions they asked, as well as how they
adjusted their information search based on the responses received.";Effect with CA on xyz (could have been any CA);"In this study, we investigated how children interact with voice assistants, particularly
focusing on what kinds of questions they ask and how they react to the responses
obtained.";answers;"This study examined the adaptiveness of children's inquiry behavior
with VAs, exploring whether and how 3- to 10-year-old children adapt
the type of questions they ask of Amazon Alexa based on the answers
previously obtained.";response, adaptiveness, question type;"This study examined the adaptiveness of children's inquiry behavior
with VAs, exploring whether and how 3- to 10-year-old children adapt
the type of questions they ask of Amazon Alexa based on the answers
previously obtained.";;;;ns;;German;"Fifty-four German-speaking children between the ages of 3 and
10 were recruited at the Berlin Zoo in Germany";Other;;
314;2023;conference;HRI;robot/agent;conference;Netherlands;Europe;Robot-Supported Information Search;Searching via speech with a robot can be used to better support children in expressing their information needs. We report on an exploratory study where children (N=35) worked on search tasks with two robots using different interaction styles. One system posed closed, yes/no questions and was more system-driven while the other system used open-ended questions and was more user-driven. We studied children's preferences and experiences of these interaction styles using questionnaires and semi-structured interviews. We found no overall strong preference between the interaction styles. However, some children reported task-dependent preferences. We further report on children's interpretation and reasoning around interaction styles for robots supporting information search.;https://dl.acm.org/doi/10.1145/3568294.3580128;314.Sharma;Yes;Furhat;"Children were presented with two different search tasks and the
Furhat robot2 with two different interaction styles";Robot;"Children were presented with two different search tasks and the
Furhat robot2 with two different interaction styles";Fully autonomous;"The Furhat robot would take input by listening to
the user, convert the speech to text, run one of the two different
interaction styles, generate a response based on the algorithm of
that interaction style, and then convert that response back from
text to speech for the user to hear.";ns;;ns;;ns;;free speech;free speech;;"The researcher explained to the participants
that the goal of the session was to talk to the robot to complete
a search task that would be given to them. The participant was
informed that the robot was not “very smart” and so they should try
to be as clear and expressive as possible when talking and asking
questions to the robot.";Edutainment;"Searching via speech with a robot can be used to better support
children in expressing their information needs";assistant;lower;"The researcher explained to the participants
that the goal of the session was to talk to the robot to complete
a search task that would be given to them. The participant was
informed that the robot was not “very smart” and so they should try
to be as clear and expressive as possible when talking and asking
questions to the robot.";school;school;"A within-subject design study, in which children aged 9–12 years
old conducted a search task with the Furhat robot, was carried out
at two different schools.";answering questions;"The researcher explained to the participants
that the goal of the session was to talk to the robot to complete
a search task that would be given to them. The participant was
informed that the robot was not “very smart” and so they should try
to be as clear and expressive as possible when talking and asking
questions to the robot.";9,10,11,12;broad;4;broad;"A total of 35 participants (23 boys and 12 girls), aged between 9 and
12 years, were recruited.";ns;;35;31-40;"A total of 35 participants (23 boys and 12 girls), aged between 9 and
12 years, were recruited.";Effect with CA on xyz (could have been any CA);"In this exploratory study into robot-supported information search
for children, we compared a more system-driven interaction style
that asks closed-ended questions to a more user-responsive ELIZAstyle
conversational interaction, and found that participants had no
strong preference for one interaction style over the other.";interaction style;"In this exploratory study into robot-supported information search
for children, we compared a more system-driven interaction style
that asks closed-ended questions to a more user-responsive ELIZAstyle
conversational interaction, and found that participants had no
strong preference for one interaction style over the other.";enjoyment, preference;"In this exploratory study into robot-supported information search
for children, we compared a more system-driven interaction style
that asks closed-ended questions to a more user-responsive ELIZAstyle
conversational interaction, and found that participants had no
strong preference for one interaction style over the other.";;;;ns;;English;"Two participants,
whose first language was not English, said that they would prefer
speaking to the robot over typing as they were not fluent in English
yet.";English;;
316;2023;article;Int. J. of Social Robotics;robot/agent;journal;Italy, USA;Combination;Using Socially Assistive Robots in Speech-Language Therapy for Children with Language Impairments;"Socially assistive robots (SARs) have been shown to be promising therapy tools for children with primary or co-occurring language impairments (e.g., developmental language disorder and autism spectrum disorder), but only a few studies have explored the use of SARs in speech-language therapies. This work sought to address the following research goals: (1) explore the potential of using SAR for training linguistic skills of children with language impairments, targeting specific aspects of language and measuring their linguistic improvements in speech-language therapy; (2) explore children’s facial cues during SAR-supoported speech-language therapy; and (3) collect therapist perspectives on using SARs in speech-language therapy after having experienced it. Toward these goals, we conducted an 8-week between-subjects study involving 20 children with language impairments and 6 speech-language therapists who conducted the SAR-supported therapy. Children were randomly assigned to either a physical SAR or a virtual SAR condition; both provided the same language impairment therapy. We collected linguistic activity scores, video recordings, therapist questionnaires, and group interview data. The study results show that: (i) the study participants’ overall linguistic skills improved significantly in both conditions; (ii) participants who were engaged with the physical SAR (measured based on gaze direction and head position) were more likely to demonstrate linguistic skill improvements and had a significantly higher numbers of speech occurrences in the child-robot-therapist triads with the physical SAR; (iii) therapists reported skepticism about SAR efficacy in this context but believed that SAR could be beneficial for keeping children engaged, motivated, and positive during speech-language therapy. © 2023, The Author(s), under exclusive licence to Springer Nature B.V.";https://link.springer.com/article/10.1007/s12369-023-01028-7;316.Spitale;Yes;LuxAI QT;"We provided the therapists with a LuxAI QT robot [56], two
tablets, a laptop, and a camera.";Robot;"We provided the therapists with a LuxAI QT robot [56], two
tablets, a laptop, and a camera.";Fully autonomous;"The robot/agent was able to transcribe
a participant’s speech using real-time speech-to-text,
then used natural language processing to check for correctness and then determined if to move ahead with the training
or request that the participant try again.";ns;;ns;;ns;;free speech;free speech;;"Ten activities were developed for robot/agent-supported
training of linguistic comprehension and production skills.
The role of the robot/agent was to present the training activities,
i.e., to show pictures and describe the scene for each
activity. Psycholinguistics expert members of the research
group recommended the use of the popular storytelling format
because linguistic context (as in storytelling) boosts
language skill learning and syntactic comprehension [42].
Therefore, participants were presented with a series stories;
for comprehension, a story composed of several sentences
and a picture to match to a heard sentence, and for production,
a sentence from story to repeat verbatim.";Healthcare;"Socially assistive robots (SARs) have been shown to be promising therapy tools for children with primary or co-occurring
language impairments (e.g., developmental language disorder and autism spectrum disorder), but only a few studies have
explored the use of SARs in speech-language therapies.";trainer;higher;"This work sought to address the following research goals: (1) explore
the potential of using SAR for training linguistic skills of children with language impairments, targeting specific aspects of
language and measuring their linguistic improvements in speech-language therapy; (2) explore children’s facial cues during
SAR-supoported speech-language therapy; and (3) collect therapist perspectives on using SARs in speech-language therapy
after having experienced it.";therapy;therapy;"The assessment was performed in the
participants’ usual therapy room, at a table, with a therapist
who sat next to them and assisted them as necessary without
intervening.";training;"This work sought to address the following research goals: (1) explore
the potential of using SAR for training linguistic skills of children with language impairments, targeting specific aspects of
language and measuring their linguistic improvements in speech-language therapy; (2) explore children’s facial cues during
SAR-supoported speech-language therapy; and (3) collect therapist perspectives on using SARs in speech-language therapy
after having experienced it.";6,7,8,9,10,11;broad;6;broad;"This resulted in 20
children being included in the study, 14 males and 6 females,
aged 6–11 (M=8.2, SD=1.36); 11 were diagnosed with
ASD-LI and 9 with DLD.";Language disorder, ASD;"diagnosed with developmental language disorder (DLD)
or autism spectrum disorder and co-occurring language
impairment (ASD-LI);";20;Nov/20;"This resulted in 20
children being included in the study, 14 males and 6 females,
aged 6–11 (M=8.2, SD=1.36); 11 were diagnosed with
ASD-LI and 9 with DLD.";Effect with CA on xyz (could have been any CA);"This work sought to address the following research goals: (1) explore
the potential of using SAR for training linguistic skills of children with language impairments, targeting specific aspects of
language and measuring their linguistic improvements in speech-language therapy; (2) explore children’s facial cues during
SAR-supoported speech-language therapy; and (3) collect therapist perspectives on using SARs in speech-language therapy
after having experienced it.";system;"The results of the study show that participants significantly
improved their linguistic skills involving clitic pronouns
when trained with both SAR and V-SAR. However, the participants
did not showany significant improvement in passive
clauses.";linguistic skills;"The results of the study show that participants significantly
improved their linguistic skills involving clitic pronouns
when trained with both SAR and V-SAR. However, the participants
did not showany significant improvement in passive
clauses.";;;;ns;;Italian;"monolingual (Italian);";Other;;
318;2021;conference;IDC;CCI;conference;USA;North America;Designing Emotionally Expressive Social Commentary to Facilitate Child-Robot Interaction;Emotion expression in human-robot interaction has been widely explored, however little is known about how such expressions should be coupled with feelings and opinions expressed by a social robot. We explored how 12 children experienced emotionally expressive social commentaries from a reading companion robot across five interaction styles that differed in their non-verbal emotional expressiveness and opinionated conversational styles (neutral, divergent, or convergent opinions). We found that, while the robot’s opinions and non-verbal emotion expressions affected children’s experiences with the robot, the speech content of the commentaries was the more prominent factor in their experience. Additionally, children differed in their perceptions of social commentary: while some expressed a sense of connection-making with the robot’s self-disclosure commentaries, others felt distracted by them or felt like the robot was off-topic. We recommend designers pay particular attention to the robot’s speech content and consider children’s individual differences in designing emotional and opinionated speech.;https://dl.acm.org/doi/10.1145/3459990.3460714;318.White;Yes;Misty II;"We used the Misty II2 platform in our work
which is a small semi-humanoid robot with a 4-inch LCD for its
face, allowing for highly customizable facial expressions and displaying
other cues. The robot has a multi-color LED chest-light
that we utilized to support emotion expression. The robot has a
three-degree-of-freedom head and two single-degree-of freedom
arms and moves using a motor-driven tread system.";Robot;"We used the Misty II2 platform in our work
which is a small semi-humanoid robot with a 4-inch LCD for its
face, allowing for highly customizable facial expressions and displaying
other cues. The robot has a multi-color LED chest-light
that we utilized to support emotion expression. The robot has a
three-degree-of-freedom head and two single-degree-of freedom
arms and moves using a motor-driven tread system.";Wizarded;"We conducted the study in a Wizard of Oz (WoZ)
format which allowed us to adjust the appropriate timing of the
robot’s responses and determine which type of response to give
(i.e., convergent or divergent) based on the child’s answer. All commentaries
expressed by the robot were pre-defined and the WoZ
operator initiated the interaction only when a child showed an
AprilTag to the robot and determined whether to have the robot
express a convergent or divergent response.";ns;;ns;;Wizarded;;free speech;free speech;;"We conducted the study in a Wizard of Oz (WoZ)
format which allowed us to adjust the appropriate timing of the
robot’s responses and determine which type of response to give
(i.e., convergent or divergent) based on the child’s answer. All commentaries
expressed by the robot were pre-defined and the WoZ
operator initiated the interaction only when a child showed an
AprilTag to the robot and determined whether to have the robot
express a convergent or divergent response.";Edutainment;"We explored how 12 children experienced emotionally expressive
social commentaries from a reading companion robot across five
interaction styles that differed in their non-verbal emotional expressiveness
and opinionated conversational styles (neutral, divergent,
or convergent opinions).";companion;equal;"We explored how 12 children experienced emotionally expressive
social commentaries from a reading companion robot across five
interaction styles that differed in their non-verbal emotional expressiveness
and opinionated conversational styles (neutral, divergent,
or convergent opinions).";online;online;"The study was conducted remotely via the video conferencing software
Zoom.";reading companion;"We explored how 12 children experienced emotionally expressive
social commentaries from a reading companion robot across five
interaction styles that differed in their non-verbal emotional expressiveness
and opinionated conversational styles (neutral, divergent,
or convergent opinions).";10,11,12;10/Dec;3;narrow;"We recruited 12 children (5 female, 7 male) between the age range
of 10–12 (M = 10.5, SD = 0.4) through organizational mailing
lists which included university faculty and staff.";ns;;12;Nov/20;"We recruited 12 children (5 female, 7 male) between the age range
of 10–12 (M = 10.5, SD = 0.4) through organizational mailing
lists which included university faculty and staff.";Effect with CA on xyz (could have been any CA);"We explored how 12 children experienced emotionally expressive
social commentaries from a reading companion robot across five
interaction styles that differed in their non-verbal emotional expressiveness
and opinionated conversational styles (neutral, divergent,
or convergent opinions).";interaction style;"We explored how 12 children experienced emotionally expressive
social commentaries from a reading companion robot across five
interaction styles that differed in their non-verbal emotional expressiveness
and opinionated conversational styles (neutral, divergent,
or convergent opinions).";experience;"We explored how 12 children experienced emotionally expressive
social commentaries from a reading companion robot across five
interaction styles that differed in their non-verbal emotional expressiveness
and opinionated conversational styles (neutral, divergent,
or convergent opinions).";;;;ns;;English;"Families were recruited through institutional mailing lists (including
university staff and employee) targeting families that have a
child aged between 10–12 that speaks fluent English";English;;
320;2022;conference;RO-MAN;robot/agent;conference;Switzerland, Germany;Europe;How a Social Robot’s Vocalization Affects Children’s Speech, Learning, and Interaction;A wider incorporation of robots into classrooms is hampered by current technological limitations on full autonomy in social robots. Automated speech recognition, for example, a key enabler for vocal communication, is still unable to perform with sufficient accuracy. Past studies have shown that humans adjust their speech patterns to accommodate less skilled interlocutors. If such a response holds in human-robot interactions as well, we may be able to exploit it to lessen the burden on social robots and enable rich, autonomous vocal communication. In this paper we explore whether a robot’s speaking ability could have an impact on children’s speech patterns, learning, and engagement by designing an interaction where a child and a robot collaborate on a Tower of Hanoi puzzle. Sixteen children aged 7-14 completed this collaborative task partnered with a social robot that communicated with either high verbal (full sentences), low verbal (short phrases or single words), or nonverbal (sound-based utterances) vocalization. While we found no significant impact on children’s speech patterns or learning due to the robot’s method of vocalization, children in the non-verbal condition had a significantly lower perception of the robot’s intelligence along with higher rates of providing feedback and more instances of undoing its moves. This suggests that a link may exist between a robot’s perceived speaking ability and children’s confidence in that robot’s overall intelligence and capability in a collaborative task, as well as their empathy towards a peer they perceive as less skilled in the task.;https://ieeexplore.ieee.org/document/9900811;320.Wright;Yes;Reachy;"Reachy is
equipped with two cameras in its head, a microphone and
speaker in its torso, and arms with 7 degrees of freedom.
Reachy’s torso and arms are roughly the size of a preteen,
which makes it well suited for child-robot interaction studies.
Reachy’s overall form-factor is not overly humanoid; the
antennas and asymmetrical eyes in particular give Reachy
a distinctly “robotic” appearance.";Robot;"Reachy is
equipped with two cameras in its head, a microphone and
speaker in its torso, and arms with 7 degrees of freedom.
Reachy’s torso and arms are roughly the size of a preteen,
which makes it well suited for child-robot interaction studies.
Reachy’s overall form-factor is not overly humanoid; the
antennas and asymmetrical eyes in particular give Reachy
a distinctly “robotic” appearance.";Fully autonomous;"The listening node was
implemented using the speech recognition library3 and building
on [28], and operates over four-second intervals. Upon
successful speech recognition, the transcript is parsed for
keywords indicating a change of turn, agreement, dissent,
and requests for help, some of which are used to trigger the
reactions listed in Table II. Speech that doesn’t align to a
keyword is classified as an “interruption” and handled by
randomly picking one among a set of generic responses to
maintain the illusion of back and forth communication.";Yes;"Reachy is
equipped with two cameras in its head, a microphone and
speaker in its torso, and arms with 7 degrees of freedom.
Reachy’s torso and arms are roughly the size of a preteen,
which makes it well suited for child-robot interaction studies.
Reachy’s overall form-factor is not overly humanoid; the
antennas and asymmetrical eyes in particular give Reachy
a distinctly “robotic” appearance.";No;"The speaking node either uses text-to-speech (TTS) from
the pyttsx3 library (for the high and low verbal conditions) or
directly plays .wav audio files (for the non-verbal condition).
The TTS output is run through a diode ring synthesizer which
adds a weak vocoder effect to Reachy’s voice to make it more
“robotic” [29].";Yes;"The listening node was
implemented using the speech recognition library3 and building
on [28], and operates over four-second intervals. Upon
successful speech recognition, the transcript is parsed for
keywords indicating a change of turn, agreement, dissent,
and requests for help, some of which are used to trigger the
reactions listed in Table II. Speech that doesn’t align to a
keyword is classified as an “interruption” and handled by
randomly picking one among a set of generic responses to
maintain the illusion of back and forth communication.";free speech;free speech;;"The listening node was
implemented using the speech recognition library3 and building
on [28], and operates over four-second intervals. Upon
successful speech recognition, the transcript is parsed for
keywords indicating a change of turn, agreement, dissent,
and requests for help, some of which are used to trigger the
reactions listed in Table II. Speech that doesn’t align to a
keyword is classified as an “interruption” and handled by
randomly picking one among a set of generic responses to
maintain the illusion of back and forth communication.";Edutainment;"We chose the Tower of Hanoi puzzle for this study as it
is a logical thinking task that challenges children’s working
memory and can be easily adapted to a collaborative turntaking
task [25]. In the Tower of Hanoi puzzle, there are three
posts and a set of rings of different sizes, as shown in Figure
1. At the beginning of the puzzle these rings are stacked in
order of size on one of the posts in a pyramid, with the largest
ring on the bottom and the smallest on top. The goal is to
rebuild the pyramid onto another post, maintaining the size
order, with two rules: only one ring may be moved at a time
and a larger ring can never be placed on a smaller one.";peer;equal;"We expect that the robot’s level of speech complexity
would impact not only the children’s speech patterns in
response to the robot, but also their learning and their
engagement with the robot as a peer in the interaction";lab;lab;"Experimental setup from the child’s viewpoint, showing
Reachy behind the Tower of Hanoi puzzle, mid-solution. The child
sits across from Reachy where they can both reach the game to play
collaboratively. Sessions were recorded using the webcam mounted
to the right of Reachy’s head in the image. The researcher was
seated behind the monitor with their face and upper body blocked
from the child’s direct view.";collaboration;"a child and a
robot collaborate on a Tower of Hanoi puzzle.";7,8,9,10,11,12,13,14;broad;8;wide;"This experiment was conducted over two weeks in individual
sessions with 16 children aged 7-14";ns;;16;Nov/20;"This experiment was conducted over two weeks in individual
sessions with 16 children aged 7-14";Effect with CA on xyz (could have been any CA);"In
this paper we explore whether a robot’s speaking ability could
have an impact on children’s speech patterns, learning, and
engagement";level of speech complexity;"We expect that the robot’s level of speech complexity
would impact not only the children’s speech patterns in
response to the robot, but also their learning and their
engagement with the robot as a peer in the interaction";speech patterns, learning, engagement;"We expect that the robot’s level of speech complexity
would impact not only the children’s speech patterns in
response to the robot, but also their learning and their
engagement with the robot as a peer in the interaction";;;;ns;;ns;;ns;;
321;2023;conference;IDC;CCI;conference;USA;North America;“Rosita Reads With My Family”: Developing A Bilingual Conversational Agent to Support Parent-Child Shared Reading;Bilingual children have unique needs for school readiness as they navigate between two languages and cultures. A supportive home language environment, where children are frequently exposed to language through conversation and reading, can positively impact their language development and prepare them for school. However, current conversational agents and e-books designed for children do not typically take into account the cultural and linguistic needs of bilingual children and do not involve parents. This project presents the development of a bilingual conversational agent and accompanying e-book, designed to support parent-child interactions and promote language development for Latinx Spanish-English bilingual children. Results from a user study indicate that the bilingual agent effectively engages children verbally and encourages parental involvement in reading processes. The study also provides design insights for creating conversational agents for bilingual children.;https://dl.acm.org/doi/10.1145/3585088.3589354;321.Xu;Yes;Rosita;"We designed our bilingual conversational agent, Rosita, along with
the e-book in which she is embedded, with the specific goal of promoting
Latinx bilingual children’s school readiness. The three key
design features of “Rosita Reads With My Family”, as detailed below,
were based on the research literature highlighted in the previous
section. First, we incorporated cultural practices deemed relevant
to our target participants. The story we chose for developing the
e-book is centered on Latinx food culture [23] and familismo [15].
In the story, Rosita’s abuela visits from Mexico and guides Rosita
and her friend Elmo through the process of making guacamole
and salsa. Rosita, her abuela, and Elmo write down the recipes, go
grocery shopping, make the dishes together, and share the food
with their friends. Second, we aimed to support bilingual reading
and conversation. In addition to having a Mexican accent, the conversational
agent empowering Rosita is capable of comprehending
and carrying out dialogue in English, Spanish, and a mix of two.
Families are also allowed to choose either Spanish or English as
their preferred language for the printed text and narrated audio.
Third, we aimed to promote parent-child joint engagement. Rosita
facilitates the co-reading experience by asking two questions after
each page. These question pairs are specifically designed to encourage
children to reflect on and express their understanding of the
story while also encouraging dialogue between the parent and the
child.";Virtual Agent;"We designed our bilingual conversational agent, Rosita, along with
the e-book in which she is embedded, with the specific goal of promoting
Latinx bilingual children’s school readiness. The three key
design features of “Rosita Reads With My Family”, as detailed below,
were based on the research literature highlighted in the previous
section. First, we incorporated cultural practices deemed relevant
to our target participants. The story we chose for developing the
e-book is centered on Latinx food culture [23] and familismo [15].
In the story, Rosita’s abuela visits from Mexico and guides Rosita
and her friend Elmo through the process of making guacamole
and salsa. Rosita, her abuela, and Elmo write down the recipes, go
grocery shopping, make the dishes together, and share the food
with their friends. Second, we aimed to support bilingual reading
and conversation. In addition to having a Mexican accent, the conversational
agent empowering Rosita is capable of comprehending
and carrying out dialogue in English, Spanish, and a mix of two.
Families are also allowed to choose either Spanish or English as
their preferred language for the printed text and narrated audio.
Third, we aimed to promote parent-child joint engagement. Rosita
facilitates the co-reading experience by asking two questions after
each page. These question pairs are specifically designed to encourage
children to reflect on and express their understanding of the
story while also encouraging dialogue between the parent and the
child.";Fully autonomous;Natural language understanding process in “Rosita-child dialogue”;Yes;"We designed our bilingual conversational agent, Rosita, along with
the e-book in which she is embedded, with the specific goal of promoting
Latinx bilingual children’s school readiness. The three key
design features of “Rosita Reads With My Family”, as detailed below,
were based on the research literature highlighted in the previous
section. First, we incorporated cultural practices deemed relevant
to our target participants. The story we chose for developing the
e-book is centered on Latinx food culture [23] and familismo [15].
In the story, Rosita’s abuela visits from Mexico and guides Rosita
and her friend Elmo through the process of making guacamole
and salsa. Rosita, her abuela, and Elmo write down the recipes, go
grocery shopping, make the dishes together, and share the food
with their friends. Second, we aimed to support bilingual reading
and conversation. In addition to having a Mexican accent, the conversational
agent empowering Rosita is capable of comprehending
and carrying out dialogue in English, Spanish, and a mix of two.
Families are also allowed to choose either Spanish or English as
their preferred language for the printed text and narrated audio.
Third, we aimed to promote parent-child joint engagement. Rosita
facilitates the co-reading experience by asking two questions after
each page. These question pairs are specifically designed to encourage
children to reflect on and express their understanding of the
story while also encouraging dialogue between the parent and the
child.";Yes;"Procedure for Developing Rosita’s Conversation Script. The
script for Rosita’s speech output was developed through an iterative
process. We worked with 20 Latinx children aged three to six years
old who were bilingual in English and Spanish. Each child listened
to the story read by an experimenter and was asked both types of
questions. These sessions were recorded and transcribed to analyze
whether the questions were sufficiently clear and answerable and to
make minor revisions as needed. We removed questions for which
the majority of children did not have much to say, as well as any
questions that seemed to confuse the children.";Yes;"For each child question, Rosita provides automatic responsive feedback
based on the child’s answer. Using Google’s speech-to-text
and DialogFlow API, Rosita analyzes these answers by performing
end-to-end processing to first transcribe speech into text and then
classify the transcription by semantic intent in a process known as
natural language understanding (NLU).
The NLU process, shown in Figure 2, was based on Google’s
generic pre-trained language model and fine-tuned with utterances
specific to the conversational moments in our e-books. This finetuning
allows the agent to more precisely and accurately extract
semantic intent from children’s responses. Given that children
could respond to any particular question in a variety of ways, the
agent was trained to associate more than one semantic intent with
each conversational opportunity. For example, children’s possible
answers to the question “Where can Abuela and I get all the ingredients?”
might include keywords such as “market,” “fridge,” “farm,”
or “outside.” These semantic intents in our NLU model were created
based on predicted responses formulated by the research team and
on actual utterances recorded during a field test with ten children.
We also included a fallback intent that is triggered when a child’s
utterance cannot be classified into any of the predefined intents
or when the child does not respond to the question at all. When a
fallback intent is triggered, the agent scaffolds the conversation by
rephrasing the original question using more accessible language
(e.g., changing from an open-ended question to a multiple-choice
question, “Should Abuela go to the market or go to the farm?”). If
the child’s response to the scaffolded question triggers the fallback
intent again, the agent then provides neutral feedback and explains
the correct answer to the child (e.g., “We are going to the market!
A market is a place where we can buy avocados, tomatoes, lemons,
and black beans – what we need for making guacamole!”). This
fallback system is important for extending the conversation while
preventing communication breakdowns and frustration.";free speech;free speech;;"For each child question, Rosita provides automatic responsive feedback
based on the child’s answer. Using Google’s speech-to-text
and DialogFlow API, Rosita analyzes these answers by performing
end-to-end processing to first transcribe speech into text and then
classify the transcription by semantic intent in a process known as
natural language understanding (NLU).
The NLU process, shown in Figure 2, was based on Google’s
generic pre-trained language model and fine-tuned with utterances
specific to the conversational moments in our e-books. This finetuning
allows the agent to more precisely and accurately extract
semantic intent from children’s responses. Given that children
could respond to any particular question in a variety of ways, the
agent was trained to associate more than one semantic intent with
each conversational opportunity. For example, children’s possible
answers to the question “Where can Abuela and I get all the ingredients?”
might include keywords such as “market,” “fridge,” “farm,”
or “outside.” These semantic intents in our NLU model were created
based on predicted responses formulated by the research team and
on actual utterances recorded during a field test with ten children.
We also included a fallback intent that is triggered when a child’s
utterance cannot be classified into any of the predefined intents
or when the child does not respond to the question at all. When a
fallback intent is triggered, the agent scaffolds the conversation by
rephrasing the original question using more accessible language
(e.g., changing from an open-ended question to a multiple-choice
question, “Should Abuela go to the market or go to the farm?”). If
the child’s response to the scaffolded question triggers the fallback
intent again, the agent then provides neutral feedback and explains
the correct answer to the child (e.g., “We are going to the market!
A market is a place where we can buy avocados, tomatoes, lemons,
and black beans – what we need for making guacamole!”). This
fallback system is important for extending the conversation while
preventing communication breakdowns and frustration.";Edutainment;"We designed our bilingual conversational agent, Rosita, along with
the e-book in which she is embedded, with the specific goal of promoting
Latinx bilingual children’s school readiness.";facilitator;lower;"The first question Rosita asks after each page is directed to the
child (“child question”) and focuses on the narrative of the story, aiming to facilitate children’s comprehension.";community center;public space;"The user study sessions took place at a community center in the
neighborhood from where the participants were recruited";promoting school readiness;"We designed our bilingual conversational agent, Rosita, along with
the e-book in which she is embedded, with the specific goal of promoting
Latinx bilingual children’s school readiness.";3,4,5,6;broad;4;broad;Table 1: Demographics of user study participants.;ns;;18;Nov/20;Table 1: Demographics of user study participants.;Evaluating the CA;"This project involved the implementation of a conversational agent
to encourage and support shared reading practices between parents
and children.";system;"We found that children actively responded to Rosita’s
questions, and Rosita’s family questions sparked discussions between
parents and children about their daily life as it related to the
story. Interviews with the parents revealed their appreciation for
the way our conversational";response;"We found that children actively responded to Rosita’s
questions, and Rosita’s family questions sparked discussions between
parents and children about their daily life as it related to the
story. Interviews with the parents revealed their appreciation for
the way our conversational";;;;ns;;English, Spanish;"To accommodate bilingual Hispanic families, we created NLU
modules capable of processing both English and Spanish responses.
First, we deployed a multilingual detection function that enabled
the agent to detect whether an utterance was in Spanish or English,
and then process this utterance using the appropriate NLU
module. If a child or parent responds using both languages within
a single utterance, the agent processes the utterance through both
the English and Spanish NLU modules simultaneously";Other;;
322;2023;conference;CHI;hci;conference;China, Sweden;Combination;MathKingdom: Teaching Children Mathematical Language Through Speaking at Home via a Voice-Guided Game;The amount and quality of mathematical language in the family are positively associated with promoting children’s mathematical abilities. However, mathematical language in many families is poor. Through need-finding investigation, we developed MathKingdom, a voice-agent-based game that helps children aged 4–7 learn and use rich, accurate mathematical language (e.g., mathematical expressions related to measurement, sequence, patterns). The game has four flows, in which users can wake up, transform, decorate, and perform as their avatars, as well as practice basic mathematical vocabulary, mathematical single sentences, coherent mathematical statements, and free expression. We refined the system design through wizard-of-oz testing and then evaluated it with 18 families. The results showed that MathKingdom effectively engaged children, enhanced their mathematical language skills and mathematical abilities, and encouraged parent-child conversations about math.;https://dl.acm.org/doi/10.1145/3544548.3581043;322.Xu;Yes;MathKingdom;Through need-fnding investigation, we developed MathKingdom, a voice-agent-based game that helps children aged 4–7 learn and use rich, accurate mathematical language (e.g., mathematical ex-pressions related to measurement, sequence, patterns).;Virtual Agent;Through need-fnding investigation, we developed MathKingdom, a voice-agent-based game that helps children aged 4–7 learn and use rich, accurate mathematical language (e.g., mathematical ex-pressions related to measurement, sequence, patterns).;Fully autonomous;We developed the game in Unity, using the MMPose for pose estimation, the Baidu AI Cloud speech-to-text library2 for capturing children’s speech, and the Tencent Cloud text-to-speech library for speech synthesis of our voice agent. We chose Tencent TTS because it has a cartoon-like voice, which is more user-friendly to children. The pose estimation algorithm catches the data of body key points, which is sent to Unity through UDP and mapped to the geometry-composed avatar in real-time.;Yes;Based on previous research and our fndings, we propose four main design goals: 1) Make children speak math. Our system uses voice agents to guide children in speaking mathematical language. 2) Promote parent-child dialogue in mathematical language. 3) Mastery learning of mathematical language. Our system covers diferent mathematical concepts and guides children to organize rational, coherent mathematical sentences and use more accurate mathematical language. 4) Sustained engagement. Our game tries to increase their interest and engagement by employing embodied interaction. Specifcally, the main character in the game is an avatar of the child and can map the child’s body movements.;Yes;We chose Tencent TTS because it has a cartoon-like voice, which is more user-friendly to children.;No;the Baidu AI Cloud speech-to-text library2 for capturing children’s speech;free speech;free speech;;"for speech synthesis of our voice agent. We chose Tencent TTS
because it has a cartoon-like voice, which is more user-friendly to
children. The pose estimation algorithm catches the data of body
key points, which is sent to Unity through UDP and mapped to the
geometry-composed avatar in real-time.";Education;Through need-fnding investigation, we developed MathKingdom, a voice-agent-based game that helps children aged 4–7 learn and use rich, accurate mathematical language (e.g., mathematical ex-pressions related to measurement, sequence, patterns).;navigator;higher;We used a voice agent to navigate all the fows of the game, introducing it as a member of the mathematical world.;home;home;Because this study focused on mathematical language activities in the family, we conducted the short-term evaluation in the participants’ homes.;navigate through the game;We used a voice agent to navigate all the fows of the game, introducing it as a member of the mathematical world.;4,5,6,7;broad;4;broad;"We recruited 21 pairs of participants for this user study through the authors’ social media accounts. Each pair consisted of a parent and a child aged 4–7, and all these participants were native Mandarin speakers. Three pairs of participants did not complete the study and were therefore not included in the analysis, leaving 18 valid data sets(M = 6.22, SD = 0.97; 7 female, 11 male). We labeled these 18 children as numbers C1 to C18.";ns;;18;Nov/20;"We recruited 21 pairs of participants for this user study through the authors’ social media accounts. Each pair consisted of a parent and a child aged 4–7, and all these participants were native Mandarin speakers. Three pairs of participants did not complete the study and were therefore not included in the analysis, leaving 18 valid data sets(M = 6.22, SD = 0.97; 7 female, 11 male). We labeled these 18 children as numbers C1 to C18.";Evaluating the CA;The results of our evaluation demonstrated that MathKingdom could efectively engage children to actively interact with the voice agent during play. Our quantitative analysis revealed that children’s mathematical language skills and mathematical abilities improved through the game. In addition, we observed that MathKingdom promoted mathematical dialogues between parents and children.;system;The results of our evaluation demonstrated that MathKingdom could efectively engage children to actively interact with the voice agent during play. Our quantitative analysis revealed that children’s mathematical language skills and mathematical abilities improved through the game. In addition, we observed that MathKingdom promoted mathematical dialogues between parents and children.;math skills, language skills, dialogue;The results of our evaluation demonstrated that MathKingdom could efectively engage children to actively interact with the voice agent during play. Our quantitative analysis revealed that children’s mathematical language skills and mathematical abilities improved through the game. In addition, we observed that MathKingdom promoted mathematical dialogues between parents and children.;;;;ns;;Mandarin;"We recruited 21 pairs of participants for this user study through the authors’ social media accounts. Each pair consisted of a parent and a child aged 4–7, and all these participants were native Mandarin speakers. Three pairs of participants did not complete the study and were therefore not included in the analysis, leaving 18 valid data sets(M = 6.22, SD = 0.97; 7 female, 11 male). We labeled these 18 children as numbers C1 to C18.";Other;;